{"cells":[{"cell_type":"markdown","id":"c3c5dcf1-fd7f-498b-a627-f95b3d1f7a28","metadata":{"id":"c3c5dcf1-fd7f-498b-a627-f95b3d1f7a28"},"source":["# Initialization"]},{"cell_type":"markdown","id":"39d290d1-8777-435f-a5e2-123d0c6e436b","metadata":{"id":"39d290d1-8777-435f-a5e2-123d0c6e436b"},"source":["## Library installation (only first time and in a local setup)"]},{"cell_type":"code","execution_count":19,"id":"e924a47d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Files removed: 0\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: No matching packages\n"]}],"source":["!pip cache purge"]},{"cell_type":"code","execution_count":20,"id":"62cb41aa","metadata":{},"outputs":[],"source":["!pip install pygments >=2.4.1"]},{"cell_type":"code","execution_count":21,"id":"e40eceac-2dd3-4eba-bfbc-8b2a2cfb7545","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4068,"status":"ok","timestamp":1725029902026,"user":{"displayName":"Aldo Barca","userId":"06199393374380275251"},"user_tz":-120},"id":"e40eceac-2dd3-4eba-bfbc-8b2a2cfb7545","outputId":"4533585b-7c88-46e9-f147-70a50582d947"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: transformers in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.41.1)\n","Requirement already satisfied: filelock in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.23.1)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2.32.2)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n","Requirement already satisfied: colorama in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":22,"id":"063fd768-2fe9-4315-b472-23d52a5c921b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7746,"status":"ok","timestamp":1725029909771,"user":{"displayName":"Aldo Barca","userId":"06199393374380275251"},"user_tz":-120},"id":"063fd768-2fe9-4315-b472-23d52a5c921b","outputId":"78abe255-a482-46a4-aec1-8d5a75eafb5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: datasets in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.20.0)\n","Requirement already satisfied: filelock in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (2.32.2)\n","Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (4.66.4)\n","Requirement already satisfied: xxhash in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n","Requirement already satisfied: aiohttp in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.23.1)\n","Requirement already satisfied: packaging in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n","Requirement already satisfied: colorama in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":23,"id":"5fce462e-0156-4751-aeba-df63a6cf37c6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3052,"status":"ok","timestamp":1725029912821,"user":{"displayName":"Aldo Barca","userId":"06199393374380275251"},"user_tz":-120},"id":"5fce462e-0156-4751-aeba-df63a6cf37c6","outputId":"f3600ae3-c635-446a-a8b8-762bc99ce7c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: librosa in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.10.2.post1)\n","Requirement already satisfied: audioread>=2.1.9 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (1.26.4)\n","Requirement already satisfied: scipy>=1.2.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (1.13.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (1.5.0)\n","Requirement already satisfied: joblib>=0.14 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (5.1.1)\n","Requirement already satisfied: numba>=0.51.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (0.59.1)\n","Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (0.12.1)\n","Requirement already satisfied: pooch>=1.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (1.8.1)\n","Requirement already satisfied: soxr>=0.3.2 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (4.11.0)\n","Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from librosa) (1.0.8)\n","Requirement already satisfied: packaging in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from lazy-loader>=0.1->librosa) (24.0)\n","Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from numba>=0.51.0->librosa) (0.42.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pooch>=1.1->librosa) (4.2.2)\n","Requirement already satisfied: requests>=2.19.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pooch>=1.1->librosa) (2.32.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n","Requirement already satisfied: cffi>=1.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n","Requirement already satisfied: pycparser in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\n"]}],"source":["!pip install librosa"]},{"cell_type":"code","execution_count":24,"id":"c3fff90d-213e-48ab-9eb4-34bb2f5bf9fa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2028,"status":"ok","timestamp":1725029914847,"user":{"displayName":"Aldo Barca","userId":"06199393374380275251"},"user_tz":-120},"id":"c3fff90d-213e-48ab-9eb4-34bb2f5bf9fa","outputId":"81b35aeb-15a7-4b48-f0fb-05dddb8b9e84"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: soundfile in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.12.1)\n","Requirement already satisfied: cffi>=1.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from soundfile) (1.16.0)\n","Requirement already satisfied: pycparser in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from cffi>=1.0->soundfile) (2.22)\n"]}],"source":["!pip install soundfile"]},{"cell_type":"code","execution_count":25,"id":"d5a170de","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4274,"status":"ok","timestamp":1725029919119,"user":{"displayName":"Aldo Barca","userId":"06199393374380275251"},"user_tz":-120},"id":"d5a170de","outputId":"4a00f705-22b5-45da-b374-cb553f75ff2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: ipywidgets in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (8.1.5)\n","Requirement already satisfied: comm>=0.1.3 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipywidgets) (0.2.2)\n","Requirement already satisfied: ipython>=6.1.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipywidgets) (8.25.0)\n","Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipywidgets) (5.14.3)\n","Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipywidgets) (4.0.13)\n","Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipywidgets) (3.0.13)\n","Requirement already satisfied: decorator in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n","Requirement already satisfied: jedi>=0.16 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n","Requirement already satisfied: matplotlib-inline in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n","Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n","Requirement already satisfied: pygments>=2.4.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n","Requirement already satisfied: stack-data in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n","Requirement already satisfied: colorama in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n","Requirement already satisfied: wcwidth in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n","Requirement already satisfied: executing>=1.2.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n","Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n","Requirement already satisfied: pure-eval in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"]}],"source":["!pip install --upgrade ipywidgets"]},{"cell_type":"code","execution_count":26,"id":"ce828cc7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2633,"status":"ok","timestamp":1725029921750,"user":{"displayName":"Aldo Barca","userId":"06199393374380275251"},"user_tz":-120},"id":"ce828cc7","outputId":"56ff31b0-e2c3-4cfc-d06b-bc795ad97466"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: packaging in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (24.0)\n","Requirement already satisfied: ninja in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.11.1.1)\n"]}],"source":["!pip install packaging ninja"]},{"cell_type":"code","execution_count":27,"id":"e884067e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6918,"status":"ok","timestamp":1725029928666,"user":{"displayName":"Aldo Barca","userId":"06199393374380275251"},"user_tz":-120},"id":"e884067e","outputId":"b011acbb-0a41-43eb-a056-b8f6588b47ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: flash-attn in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.6.3)\n","Requirement already satisfied: torch in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from flash-attn) (2.2.1+cu121)\n","Requirement already satisfied: einops in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from flash-attn) (0.8.0)\n","Requirement already satisfied: filelock in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->flash-attn) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->flash-attn) (4.11.0)\n","Requirement already satisfied: sympy in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->flash-attn) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->flash-attn) (3.3)\n","Requirement already satisfied: jinja2 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->flash-attn) (3.1.4)\n","Requirement already satisfied: fsspec in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->flash-attn) (2024.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch->flash-attn) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy->torch->flash-attn) (1.3.0)\n"]}],"source":["!pip install flash-attn --no-build-isolation"]},{"cell_type":"code","execution_count":28,"id":"2744b399","metadata":{"id":"2744b399","outputId":"face2585-48db-488b-9f2b-495d9e10da25"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: imblearn in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.0)\n","Requirement already satisfied: imbalanced-learn in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from imblearn) (0.12.3)\n","Requirement already satisfied: numpy>=1.17.3 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from imbalanced-learn->imblearn) (1.26.4)\n","Requirement already satisfied: scipy>=1.5.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from imbalanced-learn->imblearn) (1.13.1)\n","Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from imbalanced-learn->imblearn) (1.5.0)\n","Requirement already satisfied: joblib>=1.1.1 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from imbalanced-learn->imblearn) (3.5.0)\n"]}],"source":["!pip install imblearn"]},{"cell_type":"code","execution_count":29,"id":"8813382f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: mir_eval in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.7)\n","Requirement already satisfied: numpy>=1.7.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mir_eval) (1.26.4)\n","Requirement already satisfied: scipy>=1.0.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mir_eval) (1.13.1)\n","Requirement already satisfied: future in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mir_eval) (1.0.0)\n","Requirement already satisfied: six in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mir_eval) (1.16.0)\n"]}],"source":["!pip install mir_eval"]},{"cell_type":"code","execution_count":30,"id":"11d4b391","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: pygments in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.18.0)\n"]}],"source":["!pip install --upgrade pygments "]},{"cell_type":"code","execution_count":31,"id":"a2fd1078","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: metrics in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.3.3)\n","Collecting Pygments==2.2.0 (from metrics)\n","  Downloading Pygments-2.2.0-py2.py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: pathspec==0.5.5 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from metrics) (0.5.5)\n","Requirement already satisfied: pathlib2>=2.3.0 in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from metrics) (2.3.7.post1)\n","Requirement already satisfied: six in c:\\users\\aldob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pathlib2>=2.3.0->metrics) (1.16.0)\n","Downloading Pygments-2.2.0-py2.py3-none-any.whl (841 kB)\n","   ---------------------------------------- 0.0/841.7 kB ? eta -:--:--\n","   ---------------------------------------- 841.7/841.7 kB 6.2 MB/s eta 0:00:00\n","Installing collected packages: Pygments\n","  Attempting uninstall: Pygments\n","    Found existing installation: Pygments 2.18.0\n","    Uninstalling Pygments-2.18.0:\n","      Successfully uninstalled Pygments-2.18.0\n","Successfully installed Pygments-2.2.0\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 8.25.0 requires pygments>=2.4.0, but you have pygments 2.2.0 which is incompatible.\n","nbconvert 7.16.4 requires pygments>=2.4.1, but you have pygments 2.2.0 which is incompatible.\n"]}],"source":["!pip install metrics"]},{"cell_type":"code","execution_count":32,"id":"64bc0b99","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: pip in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0\\lib\\site-packages (24.2)\n"]}],"source":["!pip install --upgrade pip"]},{"cell_type":"markdown","id":"45523688-2323-47c6-a4c3-74441a7d68b0","metadata":{"id":"45523688-2323-47c6-a4c3-74441a7d68b0"},"source":["## Import libraries"]},{"cell_type":"code","execution_count":33,"id":"ab574a30-36f7-46f2-8b5c-df8de3bc784f","metadata":{"id":"ab574a30-36f7-46f2-8b5c-df8de3bc784f"},"outputs":[],"source":["from transformers import AutoFeatureExtractor, ASTForAudioClassification, ASTModel,Wav2Vec2Model,Wav2Vec2Config,Wav2Vec2Tokenizer,Wav2Vec2Processor\n","from datasets import load_dataset\n","import os\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","import torch\n","from functools import reduce\n","from operator import mul\n","import math\n","import torch.nn as nn\n","import librosa\n","from torch.utils.data import Dataset\n","import random\n","from tqdm.notebook import tqdm\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import json\n","from scipy.signal import butter, lfilter\n","from scipy import stats\n","from collections import Counter\n","import torchaudio\n","import glob\n","from sklearn.metrics import f1_score,precision_score, recall_score,confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.cuda.amp import autocast\n","from collections import defaultdict\n","from imblearn.over_sampling import SMOTE\n","import mir_eval\n","import scipy\n","import pandas as pd\n","import argparse\n","import os\n","import json\n","import numpy as np\n","import csv\n","import metrics\n","from datetime import datetime\n","import copy\n","from scipy import stats\n","from tqdm import tqdm"]},{"cell_type":"markdown","id":"u_GsoAfvObd7","metadata":{"id":"u_GsoAfvObd7"},"source":["# Paths variables. Must be declared based on the directory location you use."]},{"cell_type":"markdown","id":"vqa_qLFbTFv6","metadata":{"id":"vqa_qLFbTFv6"},"source":["## Google Drive directory"]},{"cell_type":"code","execution_count":34,"id":"o-wi_qovUZVP","metadata":{"id":"o-wi_qovUZVP"},"outputs":[],"source":["drive=0"]},{"cell_type":"code","execution_count":35,"id":"vheZon4cTJGl","metadata":{"id":"vheZon4cTJGl"},"outputs":[],"source":["# Define root directory from Google Drive\n","if drive==1:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\",force_remount=True)\n","    root_dir = \"/content/drive/My Drive/Tesi/\""]},{"cell_type":"markdown","id":"ff2bdd1a","metadata":{"id":"ff2bdd1a"},"source":["Json directories. The JSON must be already located here."]},{"cell_type":"code","execution_count":36,"id":"1e381278","metadata":{"id":"1e381278"},"outputs":[],"source":["if(drive==0):\n","  train_json='C:/Users/aldob/Documents/GitHub/Thesis/Codice_Tesi/Json_files/train_data.json'\n","  val_json='C:/Users/aldob/Documents/GitHub/Thesis/Codice_Tesi/Json_files/val_data.json'\n","  train_dir='E:/Tesi/Development_Set/Training_Set/Audios'\n","  val_dir='E:/Tesi/Development_Set/Validation_Set/Audios'\n","else:\n","  train_json=os.path.join(root_dir, \"Json_files/train_data.json\")\n","  val_json=os.path.join(root_dir, \"Json_files/val_data.json\")\n","  train_dir=os.path.join(root_dir, \"Training_Set/Audios\")\n","  val_dir=os.path.join(root_dir, \"Validation_Set/Audios\")"]},{"cell_type":"markdown","id":"mf6Y6ADWTmtG","metadata":{"id":"mf6Y6ADWTmtG"},"source":["Directories where you want to create/ are already created the episodes. 1 for spectrograms episodes and 1 for raw audio spectrograms."]},{"cell_type":"code","execution_count":37,"id":"iGn2Kn0hTjrz","metadata":{"id":"iGn2Kn0hTjrz"},"outputs":[],"source":["if(drive==0):\n","  \n","  directory_path_spectrograms_episodes='E:/Tesi/episodes_pth'\n","  directory_path_training_stats='E:/Tesi/training_stats_pth'\n"," \n","  directory_path_inference_stats='E:/Tesi/inference_stats_pth'\n","\n","\n","  directory_path_spectrograms_inference='E:/Tesi/spectrograms_support_and_query_for_inference_pth'\n","  directory_path_spectrograms_inference_pcen='E:/Tesi/spectrograms_support_and_query_for_inference_pcen_pth'\n","  directory_path_spectrograms_inference_AST='E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth'\n","  directory_path_raw_audio_inference='E:/Tesi/raw_audios_support_and_query_for_inference_pth'\n","\n","  \n","\n","  \n","\n","  training_directory_path_spectrograms_episodes=\"E:/Tesi/episodes_event_spectrogram_22KHz_pth\"\n","\n","  training_directory_path_raw_audio_episodes='E:/Tesi/episodes_raw_audio_pth'\n","  training_directory_path_spectrograms_episodes_AST=\"E:/Tesi/episodes_pth\"\n","  training_directory_path_spectrograms_frame_negative_support_undersampling_episodes= \"E:/Tesi/episodes_balanced_support_set_pth\"\n","  training_directory_path_spectrograms__all_negative_supports_episodes= \"E:/Tesi/episodes_all_negative_support_pth\"\n","\n","else:\n","  training_directory_path_raw_audio_episodes=os.path.join(root_dir, \"episodes_raw_audio_pth\")\n","  \n","  training_directory_path_spectrograms_episodes=os.path.join(root_dir, \"episodes_pth\")\n","  training_directory_path_spectrograms_frame_negative_support_undersampling_episodes=os.path.join(root_dir, \"episodes_balanced_support_set_pth\")\n","  training_directory_path_spectrograms__all_negative_supports_episodes=os.path.join(root_dir, \"episodes_all_negative_support_pth\")\n","\n","\n","  directory_path_spectrograms_audio_inference=os.path.join(root_dir, \"audios_inference_pth\")\n","\n","  directory_path_training_stats=(root_dir, \"training_stats_pth\")\n","  directory_path_inference_stats=(root_dir, \"inference_stats_pth\")\n","\n"]},{"cell_type":"markdown","id":"5782b063","metadata":{"id":"5782b063"},"source":["Directory where to save the models to avoid make more than once the training phase."]},{"cell_type":"code","execution_count":38,"id":"590f9864","metadata":{"id":"590f9864"},"outputs":[],"source":["if(drive==0):\n","  checkpoint_path=(\"E:/Tesi/Models_pth\")\n","else:\n","  checkpoint_path=os.path.join(root_dir, \"Models_pth\")"]},{"cell_type":"markdown","id":"c09451c2","metadata":{"id":"c09451c2"},"source":["# Utilities"]},{"cell_type":"markdown","id":"ea0ac919","metadata":{"id":"ea0ac919"},"source":["## Support Set and Query set dictionaries creation utilities"]},{"cell_type":"code","execution_count":57,"id":"2c22620b","metadata":{"id":"2c22620b"},"outputs":[],"source":["def count_number_class(set):\n","    i=0\n","    for classe,file in set.items():\n","        i=i+1\n","    return i"]},{"cell_type":"code","execution_count":58,"id":"c0055bb2","metadata":{"id":"c0055bb2"},"outputs":[],"source":["def count_number_events(set):\n","    event_classes={}\n","    for classe,file in set.items():\n","        i=0\n","        for audio,events in file.items():\n","            for evento in events:\n","                i=i+1\n","        event_classes[classe]=i\n","    return event_classes"]},{"cell_type":"code","execution_count":59,"id":"15a74a5a","metadata":{"id":"15a74a5a"},"outputs":[],"source":["def return_max_num_events(set):\n","    events=count_number_events(set)\n","    i=0\n","    for classe,event_number in events.items():\n","        if events[classe]>i:\n","            i=events[classe]\n","    return i"]},{"cell_type":"code","execution_count":60,"id":"872c8813","metadata":{"id":"872c8813"},"outputs":[],"source":["def return_number_frames(set,durata_frame_in_secondi,frame_length=1,hop_length=1):\n","    num_frames_min=0\n","    num_frames_max=0\n","    for classe,file in set.items():\n","        for audio,events in file.items():\n","             for evento in events:\n","                intervallo=evento['end']-evento['start']\n","                sovrapposizione=hop_length/frame_length #quanto effettivamente le frame si sovrappongono l'un l'altra\n","                num_frames_evento=intervallo/(durata_frame_in_secondi*sovrapposizione)              #durata di 0.01 se frame=10ms.\n","                if(num_frames_evento>num_frames_max):\n","                    num_frames_max=num_frames_evento\n","                if (num_frames_min==0):\n","                    num_frames_min=num_frames_evento\n","                elif (num_frames_min>num_frames_evento):\n","                    num_frames_min=num_frames_evento\n","    return num_frames_max,num_frames_min"]},{"cell_type":"markdown","id":"9c02e7d8","metadata":{"id":"9c02e7d8"},"source":["## Labels utilities"]},{"cell_type":"code","execution_count":61,"id":"e66a19ec","metadata":{"id":"e66a19ec"},"outputs":[],"source":["def return_audios_list(set):\n","    audio_list=[]\n","    for key in set:\n","            for file_name in set[key]:\n","                if(file_name not in audio_list):\n","                    audio_list.append(file_name)\n","    return audio_list"]},{"cell_type":"code","execution_count":62,"id":"23c2fe70","metadata":{"id":"23c2fe70"},"outputs":[],"source":["def get_classname_from_label_index(dictionary, label):\n","    for key, val in dictionary.items():\n","        if val == label:\n","            return key\n","    return None"]},{"cell_type":"markdown","id":"680e5ab3","metadata":{"id":"680e5ab3"},"source":["## Support and query set creation utilities"]},{"cell_type":"code","execution_count":63,"id":"31782413","metadata":{"id":"31782413"},"outputs":[],"source":["def get_frames(audio, frame_length, hop_length):\n","    frames = []\n","    num_samples = len(audio)\n","    for starter in range(0, num_samples-frame_length + 1, hop_length):\n","        end = starter + frame_length\n","        frames.append(audio[starter:end])\n","    return frames"]},{"cell_type":"code","execution_count":64,"id":"010cdc43","metadata":{"id":"010cdc43"},"outputs":[],"source":["def conta_episodi(set):\n","    num_episodi=0\n","    for classe,struct in set.items():\n","        num_episodi+=len(struct)\n","    return num_episodi\n","\n"]},{"cell_type":"markdown","id":"e6800a3e","metadata":{"id":"e6800a3e"},"source":["## Conversion utility to get the file names in a directory. This is done to verify if all the episodes training are available."]},{"cell_type":"code","execution_count":65,"id":"6eb98a55","metadata":{"id":"6eb98a55"},"outputs":[],"source":["def get_episodes_files_name(directory_path):\n","\n","    file_pattern = os.path.join(directory_path, '*.pth')\n","    pth_files = glob.glob(file_pattern)\n","\n","\n","    pth_file_names = [os.path.basename(file) for file in pth_files]\n","\n","    return pth_file_names"]},{"cell_type":"markdown","id":"4edfc582","metadata":{},"source":["## Creation query and support set for AST and Wav2Vec"]},{"cell_type":"code","execution_count":66,"id":"4309265c","metadata":{},"outputs":[],"source":["def load_audio(audio_path):\n","    audio, sample_rate = librosa.load(audio_path, sr=16000)\n","    return audio, sample_rate"]},{"cell_type":"code","execution_count":67,"id":"10ac4452","metadata":{},"outputs":[],"source":["# this function returns the audio batch of raw audio in bath audio preprocessed\n","def feature_extractor_events(events,feature_extractor):\n","    #feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n","    batch_feature_extractor = []\n","    # for each audio in the batch\n","    for index in range(0, len(events)):\n","        # compute spectogram\n","        output = feature_extractor(events[index], sampling_rate=16000, return_tensors=\"pt\")\n","        # store spectogram\n","        batch_feature_extractor.append((output))\n","    # model wants in input a tensor with shape [num_batch, num_frame, num_mel]\n","    events= torch.stack(batch_feature_extractor) # stack all the feature extracted events \n","    \n","    return events"]},{"cell_type":"code","execution_count":68,"id":"e9b83312","metadata":{},"outputs":[],"source":["# this function returns the audio batch of raw audio in bath audio preprocessed\n","def feature_extractor_frame(frames,feature_extractor):\n","    #feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n","    frames_feature = []\n","    # for each audio in the batch\n","    for frame in frames:\n","        # compute spectogram\n","        output = feature_extractor(frame, sampling_rate=16000, return_tensors=\"pt\")\n","        # store spectogram\n","        frames_feature.append((output))\n","    frames_extracted = torch.stack(frames_feature) # stack all the feature extracted events \n","    \n","    return frames_extracted"]},{"cell_type":"markdown","id":"82d32eeb","metadata":{},"source":["## Event equalizer functions"]},{"cell_type":"code","execution_count":69,"id":"4a3963f8","metadata":{},"outputs":[],"source":["def padding_equalizer(events_list):\n","   \n","\n","    min=0\n","    max=0\n","\n","    for event in events_list:\n","        length=len(event)\n","        if(min==0):min=length\n","        if(min>length): min=length\n","        if(max<length):max=length\n","    \n","    equalized_events=[]\n","    for event in events_list:\n","        event=torch.tensor(event)\n","        if event.size(0) < max:\n","            \n","            padding = torch.zeros(max - event.size(0),event.size(1), dtype=event.dtype)\n","            \n","            event = torch.cat([event, padding])\n","            \n","        else:\n","            pass\n","        equalized_events.append(event)\n","    return equalized_events\n"]},{"cell_type":"markdown","id":"acd33432","metadata":{},"source":["# Folder creation with all audios for training/inference"]},{"cell_type":"code","execution_count":52,"id":"e90e6005","metadata":{},"outputs":[],"source":["import os\n","import shutil\n","\n","def unificatore_file_audio(root_directory):\n","    audios_directory = os.path.join(root_directory, \"Audios\")\n","    os.makedirs(audios_directory, exist_ok=True)\n","    for folder_name in os.listdir(root_directory):\n","        folder_path = os.path.join(root_directory, folder_name)\n","        \n","        if os.path.isdir(folder_path):\n","            \n","            for file_audio in os.listdir(folder_path):\n","                \n","                if file_audio.endswith('.wav'):\n","                    file_path = os.path.join(folder_path, file_audio)\n","                    destination_path=os.path.join(audios_directory, file_audio)\n","                    if not os.path.exists(destination_path):\n","                        shutil.copy(file_path, audios_directory)\n","                        print(\"File {} copiato con successo!\".format(file_audio))\n","\n","\n","root = \"E:/Tesi/Development_Set/Training_Set\"  # Sostituire con il proprio percorso della directory principale\n","#unificatore_file_audio(root)\n","root=\"E:/Tesi/Development_Set/Validation_Set\"\n","#unificatore_file_audio(root)"]},{"cell_type":"markdown","id":"f78f2f77","metadata":{"id":"f78f2f77"},"source":["# Support and query dictionaries set creation"]},{"cell_type":"code","execution_count":53,"id":"1ed0a1fb","metadata":{"id":"1ed0a1fb"},"outputs":[],"source":["def get_support_set(path):\n","    class_list=[]\n","    csv_files={}\n","    audio_files={}\n","    class_dictionary={}\n","    with open(path, 'r') as file:\n","        dati = json.load(file)\n","\n","    for classe, value in dati.items():\n","        class_list+={classe}\n","        i=0\n","    for classe in class_list:\n","        class_dictionary[classe]=i\n","        i=i+1\n","\n","\n","    for classe in class_list:\n","        for csv in dati[classe]:\n","            if classe in csv_files:\n","                csv_files[classe].append(csv)\n","            else:\n","                csv_files[classe] = [csv]\n","            if(dati[classe][csv][\"Support\"]!={}):\n","                              #verifico che ci siano eventi positivi in quell'audio della classe\n","                if classe not in audio_files:\n","                    audio_files[classe] = {}\n","\n","\n","                if dati[classe][csv][\"Audio\"] not in audio_files[classe]:\n","                    audio_files[classe][dati[classe][csv][\"Audio\"]] = {}\n","                #audio_files[classe][dati[classe][csv][\"Audio\"]]=dati[classe][csv][\"Audio\"][\"Supports\"] Se volessi un formato diverso con anche i nomi degli eventi(seg0,1 ect...)\n","                lista_start_e_end_time_eventi=[]\n","                for evento in dati[classe][csv][\"Support\"]:\n","                    lista_start_e_end_time_eventi.append(dati[classe][csv][\"Support\"][evento])\n","\n","                audio_files[classe][dati[classe][csv][\"Audio\"]]=lista_start_e_end_time_eventi\n","\n","\n","    return audio_files\n","\n","\n","\n","\n","def get_query_set(path):\n","    class_list=[]\n","    csv_files={}\n","    audio_files={}\n","    \n","    with open(path, 'r') as file:\n","        dati = json.load(file)\n","    for classe, value in dati.items():\n","            class_list+={classe}\n","        #print(len(class_list))\n","    for classe in class_list:\n","        for csv in dati[classe]:\n","            if classe in csv_files:\n","                csv_files[classe].append(csv)\n","            else:\n","                csv_files[classe] = [csv]\n","            if(dati[classe][csv][\"Query\"]!={}):\n","                            #verifico che ci siano eventi positivi in quell'audio della classe\n","                if classe not in audio_files:\n","                    audio_files[classe] = {}\n","\n","                if dati[classe][csv][\"Audio\"] not in audio_files[classe]:\n","                    audio_files[classe][dati[classe][csv][\"Audio\"]] = {}\n","                #audio_files[classe][dati[classe][csv][\"Audio\"]]=dati[classe][csv][\"Audio\"][\"Supports\"] Se volessi un formato diverso con anche i nomi degli eventi(seg0,1 ect...)\n","                lista_start_e_end_time_eventi=[]\n","                for evento in dati[classe][csv][\"Query\"]:\n","                    lista_start_e_end_time_eventi.append(dati[classe][csv][\"Query\"][evento])\n","\n","                audio_files[classe][dati[classe][csv][\"Audio\"]]=lista_start_e_end_time_eventi\n","    return audio_files"]},{"cell_type":"code","execution_count":54,"id":"2d3e088d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":951,"status":"ok","timestamp":1725027357725,"user":{"displayName":"Aldo Barca","userId":"06199393374380275251"},"user_tz":-120},"id":"2d3e088d","outputId":"7acc630c-a938-494a-a561-4e862ac99925"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'GRN': {'e1.wav': [{'start': 1512.64, 'end': 1514.47}, {'start': 1515.84, 'end': 1517.56}, {'start': 1518.64, 'end': 1520.15}, {'start': 1521.24, 'end': 1522.71}, {'start': 1523.8, 'end': 1525.29}], 'h1.wav': [{'start': 1625.64, 'end': 1626.99}, {'start': 1629.18, 'end': 1630.88}, {'start': 1640.23, 'end': 1642.78}, {'start': 1647.07, 'end': 1649.21}, {'start': 1652.98, 'end': 1655.44}], 'y1.wav': [{'start': 1213.32, 'end': 1214.22}, {'start': 1215.68, 'end': 1217.09}, {'start': 1218.24, 'end': 1219.5}, {'start': 1220.5, 'end': 1221.77}, {'start': 1222.79, 'end': 1224.58}]}, 'GIG': {'e1.wav': [{'start': 1953.05, 'end': 1955.71}], 'h1.wav': [{'start': 2059.19, 'end': 2059.68}, {'start': 2061.96, 'end': 2063.16}, {'start': 2076.6, 'end': 2077.7}, {'start': 2079.09, 'end': 2079.9}, {'start': 2087.86, 'end': 2088.6}], 'y1.wav': [{'start': 297.91, 'end': 298.31}, {'start': 371.73, 'end': 373.15}, {'start': 1241.2, 'end': 1241.54}, {'start': 1242.24, 'end': 1242.57}, {'start': 1290.4, 'end': 1294.42}]}, 'SQT': {'e1.wav': [{'start': 48.54, 'end': 49.33}, {'start': 60.63, 'end': 63.59}, {'start': 65.62, 'end': 66.75}, {'start': 67.32, 'end': 68.71}, {'start': 69.36, 'end': 70.07}], 'h1.wav': [{'start': 1764.04, 'end': 1765.37}, {'start': 1858.85, 'end': 1859.63}, {'start': 1859.76, 'end': 1861.18}, {'start': 1885.9, 'end': 1887.64}, {'start': 1890.02, 'end': 1891.08}], 'y1.wav': [{'start': 1236.32, 'end': 1237.07}, {'start': 1254.53, 'end': 1256.06}, {'start': 1259.6, 'end': 1260.56}, {'start': 1289.44, 'end': 1290.45}, {'start': 1329.97, 'end': 1330.48}]}, 'RUM': {'n1.wav': [{'start': 2230.62, 'end': 2231.74}, {'start': 2232.29, 'end': 2235.85}, {'start': 2327.55, 'end': 2329.24}, {'start': 2330.32, 'end': 2331.44}, {'start': 2343.42, 'end': 2345.16}]}, 'WHP': {'a1.wav': [{'start': 307.13, 'end': 308.08}, {'start': 310.6, 'end': 311.68}, {'start': 314.03, 'end': 314.98}, {'start': 317.42, 'end': 318.4}, {'start': 321.28, 'end': 322.5}]}, 'CALL': {'sp4f1_2015_04_24-05_00_00_0002.wav': [{'start': 132.109926, 'end': 132.251}, {'start': 132.913393, 'end': 133.056565}, {'start': 186.791, 'end': 186.936222}, {'start': 187.26, 'end': 187.352}, {'start': 193.29317, 'end': 193.465748}]}, 'SNMK': {'dcase_MK2.wav': [{'start': 414.351, 'end': 414.435}, {'start': 896.198, 'end': 896.249}, {'start': 1328.55, 'end': 1328.589}, {'start': 1328.762, 'end': 1328.811}, {'start': 1355.732, 'end': 1355.781}], 'dcase_MK1.wav': [{'start': 3.076, 'end': 3.118}, {'start': 559.751, 'end': 559.783}, {'start': 560.719, 'end': 560.744}, {'start': 744.132, 'end': 744.175}, {'start': 910.761, 'end': 910.799}]}, 'CCMK': {'dcase_MK2.wav': [{'start': 10.847, 'end': 11.007}, {'start': 11.311, 'end': 11.508}, {'start': 11.691, 'end': 11.81}, {'start': 16.112, 'end': 16.265}, {'start': 19.343, 'end': 19.488}], 'dcase_MK1.wav': [{'start': 0.284, 'end': 0.425}, {'start': 0.569, 'end': 0.666}, {'start': 8.956, 'end': 9.087}, {'start': 13.665, 'end': 13.755}, {'start': 15.576, 'end': 15.651}]}, 'AGGM': {'dcase_MK1.wav': [{'start': 995.447, 'end': 996.037}, {'start': 996.454, 'end': 996.851}, {'start': 1000.189, 'end': 1000.364}, {'start': 1005.385, 'end': 1005.521}, {'start': 1023.2, 'end': 1023.651}]}, 'SOCM': {'dcase_MK2.wav': [{'start': 0.811, 'end': 1.039}, {'start': 1.434, 'end': 1.842}, {'start': 2.257, 'end': 2.561}, {'start': 3.242, 'end': 3.658}, {'start': 7.171, 'end': 7.358}], 'dcase_MK1.wav': [{'start': 1.111, 'end': 1.272}, {'start': 4.692, 'end': 4.919}, {'start': 1604.823, 'end': 1605.321}]}, 'BTBW': {'2015-09-25_04-00-00_unit10.wav': [{'start': 97.229, 'end': 97.379}, {'start': 783.927, 'end': 784.077}, {'start': 847.616, 'end': 847.766}, {'start': 1147.947, 'end': 1148.097}, {'start': 1378.689, 'end': 1378.839}], '2015-09-21_06-00-00_unit05.wav': [{'start': 958.458, 'end': 958.608}, {'start': 2572.967, 'end': 2573.117}, {'start': 3642.086, 'end': 3642.236}, {'start': 5837.309, 'end': 5837.459}, {'start': 5861.39, 'end': 5861.54}], '2015-09-04_08-04-59_unit03.wav': [{'start': 115.675, 'end': 115.825}, {'start': 120.024, 'end': 120.174}, {'start': 127.057, 'end': 127.207}, {'start': 127.943, 'end': 128.093}, {'start': 130.824, 'end': 130.974}]}, 'CHSP': {'2015-09-25_04-00-00_unit10.wav': [{'start': 888.24, 'end': 888.39}, {'start': 1672.74, 'end': 1672.89}, {'start': 3914.655, 'end': 3914.805}, {'start': 4940.898, 'end': 4941.048}, {'start': 4946.721, 'end': 4946.871}], '2015-10-14_23-59-59_unit05.wav': [{'start': 141.378, 'end': 141.528}, {'start': 144.211, 'end': 144.361}, {'start': 246.586, 'end': 246.736}, {'start': 419.283, 'end': 419.433}, {'start': 420.829, 'end': 420.979}]}, 'COYE': {'2015-09-25_04-00-00_unit10.wav': [{'start': 54.056, 'end': 54.206}, {'start': 116.995, 'end': 117.145}, {'start': 155.302, 'end': 155.452}, {'start': 168.251, 'end': 168.401}, {'start': 228.673, 'end': 228.823}], '2015-09-21_06-00-00_unit05.wav': [{'start': 182.041, 'end': 182.191}, {'start': 346.923, 'end': 347.073}, {'start': 608.61, 'end': 608.76}, {'start': 610.54, 'end': 610.69}, {'start': 1218.597, 'end': 1218.747}], '2015-09-11_06-00-00_unit07.wav': [{'start': 310.962, 'end': 311.112}, {'start': 544.099, 'end': 544.249}, {'start': 551.343, 'end': 551.493}, {'start': 552.746, 'end': 552.896}, {'start': 563.14, 'end': 563.29}], '2015-09-04_08-04-59_unit03.wav': [{'start': 60.909, 'end': 61.059}, {'start': 61.4, 'end': 61.55}, {'start': 132.485, 'end': 132.635}, {'start': 135.259, 'end': 135.409}, {'start': 138.93, 'end': 139.08}]}, 'GCTH': {'2015-09-25_04-00-00_unit10.wav': [{'start': 44.891, 'end': 45.041}, {'start': 1161.114, 'end': 1161.264}, {'start': 1220.284, 'end': 1220.434}, {'start': 1299.559, 'end': 1299.709}, {'start': 1507.758, 'end': 1507.908}], '2015-09-21_06-00-00_unit05.wav': [{'start': 1268.791, 'end': 1268.941}, {'start': 1275.547, 'end': 1275.697}, {'start': 1290.889, 'end': 1291.039}, {'start': 1301.35, 'end': 1301.5}, {'start': 1319.567, 'end': 1319.717}], '2015-10-14_23-59-59_unit05.wav': [{'start': 1797.007, 'end': 1797.157}, {'start': 1835.846, 'end': 1835.996}, {'start': 1927.499, 'end': 1927.649}, {'start': 2888.416, 'end': 2888.566}, {'start': 3342.55, 'end': 3342.7}], '2015-09-11_06-00-00_unit07.wav': [{'start': 19.839, 'end': 19.989}, {'start': 1122.845, 'end': 1122.995}, {'start': 2773.534, 'end': 2773.684}, {'start': 2790.156, 'end': 2790.306}, {'start': 2791.304, 'end': 2791.454}]}, 'OVEN': {'2015-09-25_04-00-00_unit10.wav': [{'start': 30.937, 'end': 31.087}, {'start': 45.87, 'end': 46.02}, {'start': 60.045, 'end': 60.195}, {'start': 164.82, 'end': 164.97}, {'start': 415.899, 'end': 416.049}], '2015-09-21_06-00-00_unit05.wav': [{'start': 161.501, 'end': 161.651}, {'start': 210.054, 'end': 210.204}, {'start': 523.269, 'end': 523.419}, {'start': 654.888, 'end': 655.038}, {'start': 1828.407, 'end': 1828.557}], '2015-09-11_06-00-00_unit07.wav': [{'start': 0.546, 'end': 0.696}, {'start': 392.508, 'end': 392.658}, {'start': 399.077, 'end': 399.227}, {'start': 576.617, 'end': 576.767}, {'start': 877.853, 'end': 878.003}], '2015-09-04_08-04-59_unit03.wav': [{'start': 30.707, 'end': 30.857}, {'start': 59.096, 'end': 59.246}, {'start': 65.067, 'end': 65.217}, {'start': 70.812, 'end': 70.962}, {'start': 92.25, 'end': 92.4}]}, 'RBGR': {'2015-09-25_04-00-00_unit10.wav': [{'start': 15.872, 'end': 16.022}, {'start': 17.803, 'end': 17.953}, {'start': 20.745, 'end': 20.895}, {'start': 26.42, 'end': 26.57}, {'start': 29.209, 'end': 29.359}], '2015-09-21_06-00-00_unit05.wav': [{'start': 112.332, 'end': 112.482}, {'start': 113.883, 'end': 114.033}, {'start': 119.725, 'end': 119.875}, {'start': 121.586, 'end': 121.736}, {'start': 650.86, 'end': 651.01}], '2015-09-11_06-00-00_unit07.wav': [{'start': 8.416, 'end': 8.566}, {'start': 23.363, 'end': 23.513}, {'start': 23.538, 'end': 23.688}, {'start': 30.853, 'end': 31.003}, {'start': 104.552, 'end': 104.702}], '2015-09-04_08-04-59_unit03.wav': [{'start': 1598.059, 'end': 1598.209}, {'start': 1612.574, 'end': 1612.724}, {'start': 1620.698, 'end': 1620.848}, {'start': 1629.35, 'end': 1629.5}, {'start': 1976.121, 'end': 1976.271}]}, 'SAVS': {'2015-09-25_04-00-00_unit10.wav': [{'start': 2202.456, 'end': 2202.606}, {'start': 2306.595, 'end': 2306.745}, {'start': 2452.914, 'end': 2453.064}, {'start': 2468.87, 'end': 2469.02}, {'start': 2792.083, 'end': 2792.233}], '2015-10-14_23-59-59_unit05.wav': [{'start': 388.865, 'end': 389.015}, {'start': 390.107, 'end': 390.257}, {'start': 393.246, 'end': 393.396}, {'start': 683.116, 'end': 683.266}, {'start': 689.661, 'end': 689.811}]}, 'SWTH': {'2015-09-25_04-00-00_unit10.wav': [{'start': 9.699, 'end': 9.849}, {'start': 42.981, 'end': 43.131}, {'start': 51.53, 'end': 51.68}, {'start': 67.878, 'end': 68.028}, {'start': 74.412, 'end': 74.562}], '2015-09-21_06-00-00_unit05.wav': [{'start': 25.857, 'end': 26.007}, {'start': 284.052, 'end': 284.202}, {'start': 288.142, 'end': 288.292}, {'start': 421.163, 'end': 421.313}, {'start': 704.77, 'end': 704.92}], '2015-10-14_23-59-59_unit05.wav': [{'start': 316.327, 'end': 316.477}, {'start': 719.565, 'end': 719.715}, {'start': 896.294, 'end': 896.444}, {'start': 965.51, 'end': 965.66}, {'start': 997.787, 'end': 997.937}], '2015-09-11_06-00-00_unit07.wav': [{'start': 1.699, 'end': 1.849}, {'start': 9.631, 'end': 9.781}, {'start': 10.986, 'end': 11.136}, {'start': 23.833, 'end': 23.983}, {'start': 27.41, 'end': 27.56}], '2015-09-04_08-04-59_unit03.wav': [{'start': 45.474, 'end': 45.624}, {'start': 419.943, 'end': 420.093}, {'start': 540.649, 'end': 540.799}, {'start': 672.232, 'end': 672.382}, {'start': 679.321, 'end': 679.471}]}, 'WTSP': {'2015-09-25_04-00-00_unit10.wav': [{'start': 394.154, 'end': 394.304}, {'start': 522.7, 'end': 522.85}, {'start': 816.873, 'end': 817.023}, {'start': 820.052, 'end': 820.202}, {'start': 824.044, 'end': 824.194}], '2015-10-14_23-59-59_unit05.wav': [{'start': 153.107, 'end': 153.257}, {'start': 279.772, 'end': 279.922}, {'start': 421.888, 'end': 422.038}, {'start': 426.496, 'end': 426.646}, {'start': 428.406, 'end': 428.556}]}, 'AMRE': {'2015-09-21_06-00-00_unit05.wav': [{'start': 1192.601, 'end': 1192.751}, {'start': 1906.809, 'end': 1906.959}, {'start': 1999.101, 'end': 1999.251}, {'start': 4659.362, 'end': 4659.512}, {'start': 4967.358, 'end': 4967.508}], '2015-09-11_06-00-00_unit07.wav': [{'start': 8.938, 'end': 9.088}, {'start': 928.224, 'end': 928.374}, {'start': 940.566, 'end': 940.716}, {'start': 1506.485, 'end': 1506.635}, {'start': 2668.144, 'end': 2668.294}], '2015-09-04_08-04-59_unit03.wav': [{'start': 332.574, 'end': 332.724}, {'start': 2910.106, 'end': 2910.256}, {'start': 3415.792, 'end': 3415.942}, {'start': 3786.908, 'end': 3787.058}, {'start': 5697.796, 'end': 5697.946}]}, 'BBWA': {'2015-09-21_06-00-00_unit05.wav': [{'start': 203.76, 'end': 203.91}, {'start': 531.786, 'end': 531.936}, {'start': 1604.055, 'end': 1604.205}, {'start': 1605.98, 'end': 1606.13}, {'start': 1611.539, 'end': 1611.689}], '2015-09-04_08-04-59_unit03.wav': [{'start': 87.803, 'end': 87.953}, {'start': 96.185, 'end': 96.335}, {'start': 318.719, 'end': 318.869}, {'start': 1107.597, 'end': 1107.747}, {'start': 1154.92, 'end': 1155.07}]}}\n","21 {'GRN': 15, 'GIG': 11, 'SQT': 15, 'RUM': 5, 'WHP': 5, 'CALL': 5, 'SNMK': 10, 'CCMK': 10, 'AGGM': 5, 'SOCM': 8, 'BTBW': 15, 'CHSP': 10, 'COYE': 20, 'GCTH': 20, 'OVEN': 20, 'RBGR': 20, 'SAVS': 10, 'SWTH': 25, 'WTSP': 10, 'AMRE': 15, 'BBWA': 10} 25\n","401.9999999999982 2.4999999999977263\n"]}],"source":["support_set=get_support_set(train_json)\n","print(support_set)\n","query_set=get_query_set(train_json)\n","number_classes_support_set=count_number_class(support_set)\n","number_events_support_set=count_number_events(support_set)\n","max_number_events=return_max_num_events(support_set)\n","num_max_frame,num_min_frame=return_number_frames(support_set,0.01)\n","print(number_classes_support_set,number_events_support_set,max_number_events)\n","print(num_max_frame,num_min_frame)\n"]},{"cell_type":"markdown","id":"803732a0","metadata":{"id":"803732a0"},"source":["# Training Data Labelling"]},{"cell_type":"markdown","id":"e832e21c","metadata":{"id":"e832e21c"},"source":["## Data Labelling Functions definition"]},{"cell_type":"code","execution_count":55,"id":"262bc7a2","metadata":{"id":"262bc7a2"},"outputs":[],"source":["def label_dataset(support_set,query_set=0,audio_list=0,frame_length=0.01):\n","    labels=[[] for _ in range(len(audio_list))]\n","    audio_id=0\n","    class_dictionary={}\n","    class_dictionary['UNKNOWN']=0\n","    for audio in audio_list:\n","        audio_path=os.path.join(train_dir,audio)\n","        audio_loaded, sr=librosa.load(audio_path)\n","        length_in_seconds=len(audio_loaded) / sr\n","        number_of_frames=math.ceil(length_in_seconds/frame_length)\n","        for i in range(number_of_frames):\n","            labels[audio_id].append(0)\n","        label_classe=1\n","        for classe,file in support_set.items():\n","            for audio_name,eventi in file.items():\n","                if audio ==audio_name:\n","                    for event in eventi:\n","                        event_start=event['start']\n","                        event_end=event['end']\n","                        frame_start=int(event_start/frame_length)\n","                        frame_end=int(event_end/frame_length)\n","\n","                        for i in range (frame_start,frame_end+1):\n","                            labels[audio_id][i]=label_classe\n","            class_dictionary[classe]=label_classe\n","            label_classe=label_classe+1\n","        \n","        if query_set!=0:\n","            label_classe=1\n","            for classe,file in query_set.items():\n","                for audio_name,eventi in file.items():\n","                    if audio == audio_name:\n","                        event_list=file[audio]\n","                        for event in event_list:\n","                            event_start=event['start']\n","                            event_end=event['end']\n","                            frame_start=int(event_start/frame_length)\n","                            frame_end=int(event_end/frame_length)\n","\n","                            for i in range (frame_start,frame_end):\n","                                labels[audio_id][i]=label_classe\n","\n","\n","                class_dictionary[classe]=label_classe\n","                label_classe=label_classe+1\n","\n","\n","        audio_id=audio_id+1\n","    return  labels,class_dictionary\n"]},{"cell_type":"markdown","id":"7eca0cde","metadata":{"id":"7eca0cde"},"source":["## Labelling"]},{"cell_type":"code","execution_count":56,"id":"cc17f99a","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":547},"executionInfo":{"elapsed":6022,"status":"error","timestamp":1725027380232,"user":{"displayName":"Aldo Barca","userId":"06199393374380275251"},"user_tz":-120},"id":"cc17f99a","outputId":"069b45fc-7f01-4eea-d004-72ead9c385be"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unexpected exception formatting exception. Falling back to standard exception\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\4210611774.py\", line 2, in <module>\n","    labels,label_mapping=label_dataset(support_set,query_set,audio_list_training)\n","                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1543620227.py\", line 8, in label_dataset\n","    audio_loaded, sr=librosa.load(audio_path)\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\audio.py\", line 176, in load\n","    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\audio.py\", line 209, in __soundfile_load\n","    context = sf.SoundFile(path)\n","              ^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\soundfile.py\", line 658, in __init__\n","    self._file = self._open(file, mode_int, closefd)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\soundfile.py\", line 1193, in _open\n","    if _os.path.isfile(file):\n","       ^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 2168, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\ultratb.py\", line 1454, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\ultratb.py\", line 1345, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\ultratb.py\", line 1192, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n","    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n","    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\ultratb.py\", line 1134, in get_records\n","    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n","            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stack_data\\core.py\", line 455, in style_with_executing_node\n","    class NewStyle(style):\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pygments\\style.py\", line 91, in __new__\n","    ndef[4] = colorformat(styledef[3:])\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pygments\\style.py\", line 58, in colorformat\n","    assert False, \"wrong color format %r\" % text\n","           ^^^^^\n","AssertionError: wrong color format 'ansiyellow'\n"]}],"source":["audio_list_training=return_audios_list(support_set)\n","labels,label_mapping=label_dataset(support_set,query_set,audio_list_training)\n","\n","train_file_paths=[]\n","for audio in audio_list_training:\n","    train_file_paths.append(os.path.join(train_dir,audio))\n","\n","label_mapping"]},{"cell_type":"code","execution_count":32,"id":"6098c53b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Counter({0: 352310, 3: 5396, 1: 2027, 2: 267})\n","Counter({0: 334710, 1: 11752, 2: 7798, 3: 5740})\n","Counter({0: 349325, 1: 6077, 2: 2917, 3: 1681})\n","Counter({0: 356235, 4: 3765})\n","Counter({0: 355732, 5: 4268})\n","Counter({0: 56439, 6: 3595})\n","Counter({0: 248928, 8: 11116, 10: 1004, 7: 386})\n","Counter({0: 186482, 8: 3175, 9: 2203, 7: 141, 10: 91})\n","Counter({0: 714061, 18: 3004, 13: 590, 15: 469, 11: 440, 16: 409, 14: 352, 19: 304, 17: 201, 12: 170})\n","Counter({0: 712699, 18: 4439, 16: 1258, 14: 546, 13: 457, 15: 231, 21: 170, 11: 105, 20: 95})\n","Counter({0: 704282, 15: 10696, 18: 2240, 16: 991, 11: 823, 20: 425, 13: 275, 21: 268})\n","Counter({0: 717006, 19: 1865, 12: 425, 17: 349, 18: 260, 14: 95})\n","Counter({0: 714528, 18: 3032, 16: 1015, 13: 590, 15: 559, 14: 151, 20: 125})\n"]}],"source":["for label in labels:\n","        count=Counter(label)\n","        print(count)"]},{"cell_type":"markdown","id":"86d793b6","metadata":{"id":"86d793b6"},"source":["# Data augmentation\n"]},{"cell_type":"code","execution_count":70,"id":"7fcd7200","metadata":{"id":"7fcd7200"},"outputs":[],"source":["def add_reverb(y, sr):\n","    # Convoluzione with reverb impuls\n","    seed = np.random.randn(int(sr * 0.3))\n","    reverb = np.convolve(y, seed, mode='full')[:len(y)]\n","    return reverb\n","\n","def distort(y, distortion_factor=0.3): #factor 0.1 or 0.3\n","    y_distorted = y * (1 + distortion_factor * np.sin(2 * np.pi * y))\n","    return y_distorted\n","\n","def add_noise(y, noise_factor=0.002): #between 0.001 and 0.003\n","    noise = np.random.randn(len(y))\n","    y_noisy = y + noise_factor * noise\n","    return y_noisy\n","\n","def butter_bandpass(lowcut, highcut, sr, order=5):\n","    nyq = 0.5 * sr\n","    low = lowcut / nyq\n","    high = highcut / nyq\n","    b, a = butter(order, [low, high], btype='band')\n","    return b, a\n","\n","def apply_filter(y, sr, lowcut=5000, highcut=11025):\n","    b, a = butter_bandpass(lowcut, highcut, sr, order=6)\n","    y_filtered = lfilter(b, a, y)\n","    return y_filtered\n","\n","\n","def time_stretch(y, rate=1.2):\n","    y_stretched = librosa.effects.time_stretch(y, rate=rate)\n","    return y_stretched\n","\n","def time_mask(spec, mask_param=1):  #between 0.5 and 1\n","    if isinstance(spec, np.ndarray):\n","        spec = torch.tensor(spec, dtype=torch.float32)\n","\n","    return torchaudio.transforms.TimeMasking(time_mask_param=mask_param)(spec)\n","\n","def frequency_mask(spec, mask_param=20): #between 10 and 20\n","    if isinstance(spec, np.ndarray):\n","        spec = torch.tensor(spec, dtype=torch.float32)\n","    return torchaudio.transforms.FrequencyMasking(freq_mask_param=mask_param)(spec)\n","\n","def preemphasis_filter(y, coef=0.97):\n","    # Applica il filtro di preenfasi\n","    y_filtered = np.append(y[0], y[1:] - coef * y[:-1])\n","    return y_filtered\n"]},{"cell_type":"code","execution_count":71,"id":"6c951fc6","metadata":{"id":"6c951fc6"},"outputs":[],"source":["def no_data_aug(y):\n","    return y\n","\n","def augment_audio(y, sr, n_versions=19):\n","    augmented_versions = []\n","    for _ in range(n_versions):\n","        y_aug = np.copy(y)\n","\n","        # Lista delle possibili trasformazioni\n","        transformations = [\n","            lambda y: add_reverb(y, sr),\n","            lambda y: distort(y, distortion_factor=random.uniform(0.1, 0.3)),\n","            lambda y: add_noise(y, noise_factor=random.uniform(0.001, 0.003)),\n","            lambda y: apply_filter(y, sr, lowcut=random.uniform(300, 1000), highcut=random.uniform(2000, 8000)),\n","            lambda y: time_stretch(y, rate=random.uniform(0.8, 1.2)),\n","            lambda y:preemphasis_filter(y,coef=random.uniform(0.94,0.98))\n","\n","        ]\n","\n","        # Creation of augmented data which at least 4 of the 7 trasformations.\n","        selected_transformations = random.sample(transformations, k=random.randint(4, len(transformations)))\n","\n","\n","        for transform in selected_transformations:\n","            y_aug = transform(y_aug)\n","\n","        augmented_versions.append(y_aug)\n","\n","    return augmented_versions\n","\n","\n","\n","\n","def augment_spectrogram(spectrogram):\n","    spectrogram_aug=spectrogram\n","\n","    random_float = random.random()\n","    if random_float < 0.33:\n","        spectrogram_aug=time_mask(spectrogram_aug)\n","        spectrogram_aug=frequency_mask(spectrogram_aug)\n","    elif 0.33 <= random_float < 0.66:\n","        spectrogram_aug=time_mask(spectrogram_aug)\n","    else:\n","        spectrogram_aug=frequency_mask(spectrogram_aug)\n","\n","    return spectrogram_aug\n","\n"]},{"cell_type":"markdown","id":"0d62e1e0","metadata":{"id":"0d62e1e0"},"source":["# Training Support and query set creation for each episode (for each audio 3/4 episodes usually). Total 54 episodes for training set"]},{"cell_type":"markdown","id":"0242945d","metadata":{"id":"0242945d"},"source":["### Support and query creation for from scratch model. The spectrograms are created and augmented based from a single frame. All negatives support frames taken in the support set."]},{"cell_type":"code","execution_count":32,"id":"2111e5ba","metadata":{"id":"2111e5ba"},"outputs":[],"source":["def create_support_and_query_sets_all_negative_support(labels,file_paths, support_set_dictionary,episodi_non_presenti,num_support_per_class=5):\n","    episodes = [] #episodes of training for the prototypical network. Considering an audio with 4 classes in it, we want 4 episodes.\n","    j=1\n","    #from the label[audio][frame] structure it returns a sequence of tuples. For each audio file it corrisponds the labels for the audio.\n","\n","    for file_id,label_list in enumerate(labels):\n","\n","        #dictionary which, for each label values contains every occurance of the label in a (file,label_id) format.\n","        #We'll use it to create the query and support_set.\n","\n","\n","            #label_name=get_classname_from_label_index(label_mapping,label)\n","            audio_name=audio_list_training[file_id]\n","            path=train_file_paths[file_id]\n","            #audio_raw,sr = librosa.load(path)\n","            if not os.path.exists(training_directory_path_spectrograms__all_negative_supports_episodes):\n","                os.makedirs(\"episodes_all_negative_support_pth\")\n","\n","            print(f\" Inizio creazione support e query per l'audio: {audio_name}\")\n","\n","            lista_classi_in_audio=set()\n","            for label in label_list:\n","                  if(label!=0):\n","                    lista_classi_in_audio.add(label)\n","            for i in lista_classi_in_audio:\n","                label_name=get_classname_from_label_index(label_mapping,i)\n","                if(j not in episodi_non_presenti):\n","                    print(f\" Episodio {j} già presente, si passa al successivo\")\n","                    j=j+1\n","                    continue\n","                audio_raw,sr = librosa.load(path)\n","                for classe,struct in support_set_dictionary.items():\n","                            for audio,eventi in struct.items():\n","\n","                                support_spectrograms=[]\n","                                negative_support_spectrograms=[]\n","                                if (classe==label_name and audio==audio_name):\n","                                    events=support_set_dictionary[classe][audio]\n","                                    events=events[:num_support_per_class]\n","                                    end_support=0\n","                                    start_negative_support_set=0\n","                                    for evento in events:\n","                                        start=int(evento['start']*sr)\n","                                        end=int(evento['end']*sr)\n","\n","                                        #creation support set spectrograms for \"unknown\" class.\n","                                        audio_event=audio_raw[start_negative_support_set:start]\n","                                        negative_frames=get_frames(audio_event,220,220)\n","                                        for frame in negative_frames:\n","                                            spectrogram_frame=librosa.feature.melspectrogram(y=frame,n_fft=220,hop_length=220,n_mels=128)\n","                                            negative_support_spectrograms.append((spectrogram_frame,0))\n","\n","\n","\n","                                        start_negative_support_set=end+1\n","                                        if(end>end_support):\n","                                            end_support=end\n","\n","                                        #audio_portion containing the event for the support set\n","                                        audio_event=audio_raw[start:end]\n","\n","                                        frames=get_frames(audio_event,220,220) #without overlapping, hop_length=frame_lenght\n","                                        spectrograms=[]\n","                                        for frame in frames:\n","                                            spectrogram_frame=librosa.feature.melspectrogram(y=frame,n_fft=220,hop_length=220,n_mels=128)\n","                                            spectrograms.append((spectrogram_frame,i))\n","                                            spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                                            spectrograms.append((spectrogram_augmented,i))\n","\n","\n","\n","\n","                                        #data augmentation section for raw audios, the objective is increasing nearly 20x fold the support dimension\n","                                        audios_augmented=augment_audio(audio_event,sr)\n","                                        for audio_augmented in audios_augmented:\n","                                            frames=get_frames(audio_augmented,220,220)\n","                                            for frame in frames:\n","                                                spectrogram_frame=librosa.feature.melspectrogram(y=frame,n_fft=220,hop_length=220,n_mels=128)\n","                                                #le frame me le faccio creare direttamente dal mel\n","                                                spectrograms.append((spectrogram_frame,i))\n","                                                spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                                                spectrograms.append((spectrogram_augmented,i))\n","\n","\n","\n","                                        support_spectrograms.extend(spectrograms)\n","\n","                                    support_spectrograms.extend((negative_support_spectrograms))\n","\n","\n","\n","                                    audio_query=audio_raw[end_support+220:]\n","                                    frames=get_frames(audio_query,220,220)\n","                                    spectrograms_query=[]\n","\n","                                    k=0\n","                                    print(len(frames))\n","                                    print(len(label_list)-(end_support/22050)/0.01)\n","\n","                                    for frame_id in range(int((end_support/22050)/0.01),len(label_list)):\n","                                        spectrogram_frame=librosa.feature.melspectrogram(y=frames[k],n_fft=220,hop_length=220,n_mels=128)\n","                                        spectrograms_query.append((spectrogram_frame,labels[file_id][frame_id]))\n","                                        k=k+1\n","                                    episode=(i,support_spectrograms,spectrograms_query)\n","                                    #saving the episode in a file.\n","                                    file_name=f'training_episode {j}.pth'\n","                                    file_path=os.path.join(training_directory_path_spectrograms__all_negative_supports_episodes, file_name)\n","                                    torch.save(episode,file_path)\n","                                    episodes.append(episode)\n","\n","                                    print(f\"Finito episodio,salvataggio con successo: {j}\")\n","                                    j=j+1\n","\n","    return 1"]},{"cell_type":"markdown","id":"31446256","metadata":{},"source":["### Support and query creation for from scratch model. The spectrograms are created and augmented based from a single frame. Negatives support frames  of the same dimension of positive support frame taken in the support set."]},{"cell_type":"code","execution_count":33,"id":"6a6778d1","metadata":{},"outputs":[],"source":["def create_support_and_query_sets_balanced_negative_support(labels,file_paths, support_set_dictionary,episodi_non_presenti,num_support_per_class=5):\n","    episodes = [] #episodes of training for the prototypical network. Considering an audio with 4 classes in it, we want 4 episodes.\n","    j=1\n","    #from the label[audio][frame] structure it returns a sequence of tuples. For each audio file it corrisponds the labels for the audio.\n","\n","    for file_id,label_list in enumerate(labels):\n","\n","        #dictionary which, for each label values contains every occurance of the label in a (file,label_id) format.\n","        #We'll use it to create the query and support_set.\n","\n","\n","            #label_name=get_classname_from_label_index(label_mapping,label)\n","            audio_name=audio_list_training[file_id]\n","            path=train_file_paths[file_id]\n","            #audio_raw,sr = librosa.load(path)\n","            if not os.path.exists(training_directory_path_spectrograms_frame_negative_support_undersampling_episodes):\n","                os.makedirs(\"episodes_balanced_support_set_pth\")\n","\n","            print(f\" Inizio creazione support e query per l'audio: {audio_name}\")\n","\n","            lista_classi_in_audio=set()\n","            for label in label_list:\n","                  if(label!=0):\n","                    lista_classi_in_audio.add(label)\n","            for i in lista_classi_in_audio:\n","                label_name=get_classname_from_label_index(label_mapping,i)\n","                if(j not in episodi_non_presenti):\n","                    print(f\" Episodio {j} già presente, si passa al successivo\")\n","                    j=j+1\n","                    continue\n","                audio_raw,sr = librosa.load(path)\n","                for classe,struct in support_set_dictionary.items():\n","                            for audio,eventi in struct.items():\n","\n","                                support_spectrograms=[]\n","                                negative_support_spectrograms=[]\n","                                if (classe==label_name and audio==audio_name):\n","                                    events=support_set_dictionary[classe][audio]\n","                                    events=events[:num_support_per_class]\n","                                    end_support=0\n","                                    start_negative_support_set=0\n","                                    for evento in events:\n","                                        start=int(evento['start']*sr)\n","                                        end=int(evento['end']*sr)\n","\n","                                        #creation support set spectrograms for \"unknown\" class.\n","                                        audio_event=audio_raw[start_negative_support_set:start]\n","                                        negative_frames=get_frames(audio_event,220,220)\n","                                        for frame in negative_frames:\n","                                            spectrogram_frame=librosa.feature.melspectrogram(y=frame,n_fft=220,hop_length=220,n_mels=128)\n","                                            negative_support_spectrograms.append((spectrogram_frame,0))\n","\n","\n","\n","                                        start_negative_support_set=end+1\n","                                        if(end>end_support):\n","                                            end_support=end\n","\n","                                        #audio_portion containing the event for the support set\n","                                        audio_event=audio_raw[start:end]\n","\n","                                        frames=get_frames(audio_event,220,220) #without overlapping, hop_length=frame_lenght\n","                                        spectrograms=[]\n","                                        for frame in frames:\n","                                            spectrogram_frame=librosa.feature.melspectrogram(y=frame,n_fft=220,hop_length=220,n_mels=128)\n","                                            spectrograms.append((spectrogram_frame,i))\n","                                            spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                                            spectrograms.append((spectrogram_augmented,i))\n","\n","\n","\n","\n","                                        #data augmentation section for raw audios, the objective is increasing nearly 20x fold the support dimension\n","                                        audios_augmented=augment_audio(audio_event,sr)\n","                                        for audio_augmented in audios_augmented:\n","                                            frames=get_frames(audio_augmented,220,220)\n","                                            for frame in frames:\n","                                                spectrogram_frame=librosa.feature.melspectrogram(y=frame,n_fft=220,hop_length=220,n_mels=128)\n","                                                #le frame me le faccio creare direttamente dal mel\n","                                                spectrograms.append((spectrogram_frame,i))\n","                                                spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                                                spectrograms.append((spectrogram_augmented,i))\n","\n","\n","\n","                                        support_spectrograms.extend(spectrograms)\n","\n","                                    \n","                                    negative_samples_to_keep=len(support_spectrograms)\n","                                    print(negative_samples_to_keep)\n","                                    print(len(negative_support_spectrograms))\n","                                    if(len(negative_support_spectrograms)>=negative_samples_to_keep):\n","                                        negative_support_spectrograms=random.sample(negative_support_spectrograms,negative_samples_to_keep)\n","                                    \n","                                    support_spectrograms.extend((negative_support_spectrograms))\n","\n","\n","\n","                                    audio_query=audio_raw[end_support+220:]\n","                                    frames=get_frames(audio_query,220,220)\n","                                    spectrograms_query=[]\n","\n","                                    k=0\n","                                    print(len(frames))\n","                                    print(len(label_list)-(end_support/22050)/0.01)\n","\n","                                    for frame_id in range(int((end_support/22050)/0.01),len(label_list)):\n","                                        spectrogram_frame=librosa.feature.melspectrogram(y=frames[k],n_fft=220,hop_length=220,n_mels=128)\n","                                        spectrograms_query.append((spectrogram_frame,labels[file_id][frame_id]))\n","                                        k=k+1\n","                                    episode=(i,support_spectrograms,spectrograms_query)\n","                                    #saving the episode in a file.\n","                                    file_name=f'training_episode {j}.pth'\n","                                    file_path=os.path.join(training_directory_path_spectrograms_frame_negative_support_undersampling_episodes, file_name)\n","                                    torch.save(episode,file_path)\n","                                    episodes.append(episode)\n","\n","                                    print(f\"Finito episodio,salvataggio con successo: {j}\")\n","                                    j=j+1\n","\n","    return 1"]},{"cell_type":"markdown","id":"1e6f3adf","metadata":{},"source":["### Support and query creation for from scratch model. The spectrograms are created and augmented based from an event /audio portion. All negatives frames of negative support set  taken in the support set."]},{"cell_type":"code","execution_count":34,"id":"95aa1e3c","metadata":{},"outputs":[],"source":["def create_support_and_query_sets_with_new_spectrogram_creation(labels,file_paths, support_set_dictionary,episodi_non_presenti,num_support_per_class=5):\n","    episodes = [] #episodes of training for the prototypical network. Considering an audio with 4 classes in it, we want 4 episodes.\n","    j=1\n","    #from the label[audio][frame] structure it returns a sequence of tuples. For each audio file it corrisponds the labels for the audio.\n","\n","    for file_id,label_list in enumerate(labels):\n","\n","        #dictionary which, for each label values contains every occurance of the label in a (file,label_id) format.\n","        #We'll use it to create the query and support_set.\n","\n","\n","            \n","            audio_name=audio_list_training[file_id]\n","            path=train_file_paths[file_id]\n","            \n","            if not os.path.exists(training_directory_path_spectrograms_episodes):\n","                os.makedirs(training_directory_path_spectrograms_episodes)\n","\n","            print(f\" Inizio creazione support e query per l'audio: {audio_name}\")\n","\n","            lista_classi_in_audio=set()\n","            for label in label_list:\n","                  if(label!=0):\n","                    lista_classi_in_audio.add(label)\n","            for i in lista_classi_in_audio:\n","                label_name=get_classname_from_label_index(label_mapping,i)\n","                if(j not in episodi_non_presenti):\n","                    print(f\" Episodio {j} già presente, si passa al successivo\")\n","                    j=j+1\n","                    continue\n","                audio_raw,sr = librosa.load(path)\n","                for classe,struct in support_set_dictionary.items():\n","                            for audio,eventi in struct.items():\n","\n","                                support_spectrograms=[]\n","                                negative_support_spectrograms=[]\n","                                if (classe==label_name and audio==audio_name):\n","                                    events=support_set_dictionary[classe][audio]\n","                                    events=events[:num_support_per_class]\n","                                    end_support=0\n","                                    start_negative_support_set=0\n","\n","                                    mean_duration=0\n","                                    for evento in events:\n","                                        start=int(evento['start']*sr)\n","                                        end=int(evento['end']*sr)\n","                                        if mean_duration==0: mean_duration=end-start\n","                                        else: mean_duration+=end-start\n","                                    mean_duration=mean_duration/5\n","\n","                                    for evento in events:\n","                                        start=int(evento['start']*sr)\n","                                        end=int(evento['end']*sr)\n","\n","                                        #creation support set spectrograms for \"unknown\" class.\n","                                        audio_event=audio_raw[start_negative_support_set:start]\n","                                        negative_frames=get_frames(audio_event,int(mean_duration),int(mean_duration))\n","                                        \n","                                        for negative_frame in  negative_frames:\n","                                            spectrogram_frames=librosa.feature.melspectrogram(y=negative_frame,n_fft=220,hop_length=110,n_mels=128)\n","                                        \n","                                            for l in range(0,spectrogram_frames.shape[1],2):\n","                                                end_frame = l + 2\n","                                                if end_frame <= spectrogram_frames.shape[1]:\n","                                                    spectrogram_frame = spectrogram_frames[:, l:end_frame]\n","                                                    negative_support_spectrograms.append((spectrogram_frame,0))\n","                                                 \n","\n","\n","                                        start_negative_support_set=end+1\n","                                        if(end>end_support):\n","                                            end_support=end\n","\n","                                        #audio_portion containing the event for the support set\n","                                        audio_event=audio_raw[start:end]\n","                                        spectrograms=[]\n","                                        frames=librosa.feature.melspectrogram(y=audio_event,n_fft=220,hop_length=110,n_mels=128) \n","        \n","                                        for  l in range(0,frames.shape[1],2):\n","                                            end_frame = l + 2\n","                                            if end_frame <= frames.shape[1]:\n","                                                spectrogram_frame = frames[:, l:end_frame]\n","                                                \n","                                                spectrograms.append((spectrogram_frame,i))\n","                                                spectrogram_frame=torch.tensor(spectrogram_frame)\n","                                                spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                                                spectrograms.append((spectrogram_augmented,i))\n","\n","\n","\n","\n","                                        #data augmentation section for raw audios, the objective is increasing nearly 20x fold the support dimension\n","                                        audios_augmented=augment_audio(audio_event,sr)\n","                                        for audio_augmented in audios_augmented:\n","                                            spectrogram_frames=librosa.feature.melspectrogram(y=audio_augmented,n_fft=220,hop_length=110,n_mels=128)\n","                                            for  l in range(0,spectrogram_frames.shape[1],2):\n","                                                end_frame = l + 2\n","                                                if end_frame <= spectrogram_frames.shape[1]:\n","                                                    spectrogram_frame = spectrogram_frames[:, l:end_frame]      \n","                                                    spectrograms.append((spectrogram_frame,i))\n","                                                    spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                                                    spectrograms.append((spectrogram_augmented,i))\n","\n","\n","                                        support_spectrograms.extend(spectrograms)\n","\n","                                    \"\"\"\n","                                    negative_samples_to_keep=len(support_spectrograms)\n","                                    print(negative_samples_to_keep)\n","                                    print(len(negative_support_spectrograms))\n","                                    if(len(negative_support_spectrograms)>=negative_samples_to_keep):\n","                                        negative_support_spectrograms=random.sample(negative_support_spectrograms,negative_samples_to_keep)\n","                                    \"\"\"\n","                                    support_spectrograms.extend((negative_support_spectrograms))\n","\n","\n","\n","                                    audio_query=audio_raw[end_support+220:]\n","                                    \n","                                    frames=get_frames(audio_query,int(mean_duration),int(mean_duration))\n","                                    query_spectrograms=[]\n","                                    z=len(labels[file_id])\n","                                    print(z)\n","                                    k=int((end_support/22050)/0.01)\n","                                    x=0\n","                                    frames=get_frames(audio_query,int(mean_duration),int(mean_duration))\n","\n","                                    for frame in frames:\n","                                        spectrogram_frames=librosa.feature.melspectrogram(y=frame,n_fft=220,hop_length=110,n_mels=128) #labels must go from labels(k) to labels(k+)\n","                                        for l in range(0,spectrogram_frames.shape[1],2):\n","                                            end_frame = l + 2\n","                                            if end_frame <= spectrogram_frames.shape[1]:\n","                                                spectrogram_frame = spectrogram_frames[:, l:end_frame]      \n","                                                x+=1\n","                                                \n","                                                if(x%2)==0:\n","                                                    if(k>z):break\n","                                                    query_spectrograms.append((spectrogram_frame,labels[file_id][k]))\n","                                                    \n","                                                    k+=1\n","                                                else: \n","                                                    if(k>z):break\n","                                                    \n","                                                    if labels[file_id][k]!=labels[file_id][k+1]: \n","                                                        query_spectrograms.append((spectrogram_frame,i)) #there is a frame 50% 0 and 50% 1, must be choosen if put 0 or 1.\n","                                                    else: \n","                                                        query_spectrograms.append((spectrogram_frame,labels[file_id][k]))\n","\n","                                    episode=(i,support_spectrograms,query_spectrograms)\n","                                    print(len(episode[1]))\n","                                    print(len(episode[2]))\n","                                    #saving the episode in a file.\n","                                    file_name=f'training_episode {j}.pth'\n","                                    file_path=os.path.join(training_directory_path_spectrograms_episodes, file_name)\n","                                    torch.save(episode,file_path)\n","                                    episodes.append(episode)\n","\n","                                    print(f\"Finito episodio,salvataggio con successo: {j}\")\n","                                    j=j+1\n","\n","    return 1"]},{"cell_type":"markdown","id":"7ba3d9ba","metadata":{},"source":["### Support and query Set Creation for the AST Model.The spectrograms are created and augmented based from an event /audio portion. All negatives frames of negative support set  taken in the support set. Sample rate of 16Khz as AudioSet. Sperimental use of the PCEN also in episodes creation and therefore for training"]},{"cell_type":"code","execution_count":35,"id":"6634c2bf","metadata":{},"outputs":[],"source":["def create_support_and_query_sets_with_new_spectrogram_creationAST_16KHz(labels,file_paths, support_set_dictionary,episodi_non_presenti,num_support_per_class=5):\n","    episodes = [] #episodes of training for the prototypical network. Considering an audio with 4 classes in it, we want 4 episodes.\n","    j=1\n","    #from the label[audio][frame] structure it returns a sequence of tuples. For each audio file it corrisponds the labels for the audio.\n","\n","    for file_id,label_list in enumerate(labels):\n","\n","        #dictionary which, for each label values contains every occurance of the label in a (file,label_id) format.\n","        #We'll use it to create the query and support_set.\n","\n","            \n","            audio_name=audio_list_training[file_id]\n","            path=train_file_paths[file_id]\n","            \n","            if not os.path.exists(training_directory_path_spectrograms_episodes_AST):\n","                os.makedirs(training_directory_path_spectrograms_episodes_AST)\n","\n","            print(f\" Inizio creazione support e query per l'audio: {audio_name}\")\n","\n","            lista_classi_in_audio=set()\n","            for label in label_list:\n","                  if(label!=0):\n","                    lista_classi_in_audio.add(label)\n","            for i in lista_classi_in_audio:\n","                label_name=get_classname_from_label_index(label_mapping,i)\n","                if(j not in episodi_non_presenti):\n","                    print(f\" Episodio {j} già presente, si passa al successivo\")\n","                    j=j+1\n","                    continue\n","                audio_raw,sr = librosa.load(path,sr=16000)\n","                for classe,struct in support_set_dictionary.items():\n","                            for audio,eventi in struct.items():\n","\n","                                support_spectrograms=[]\n","                                negative_support_spectrograms=[]\n","                                if (classe==label_name and audio==audio_name):\n","                                    events=support_set_dictionary[classe][audio]\n","                                    events=events[:num_support_per_class]\n","                                    end_support=0\n","                                    start_negative_support_set=0\n","\n","                                    mean_duration=0\n","                                    for evento in events:\n","                                        start=int(evento['start']*sr)\n","                                        end=int(evento['end']*sr)\n","                                        if mean_duration==0: mean_duration=end-start\n","                                        else: mean_duration+=end-start\n","                                    mean_duration=mean_duration/5\n","\n","                                    for evento in events:\n","                                        start=int(evento['start']*sr)\n","                                        end=int(evento['end']*sr)\n","\n","                                        #creation support set spectrograms for \"unknown\" class.\n","                                        audio_event=audio_raw[start_negative_support_set:start]\n","                                        negative_frames=get_frames(audio_event,int(mean_duration),int(mean_duration))\n","                                        \n","                                        for negative_frame in  negative_frames:\n","                                            spectrogram_frames=librosa.feature.melspectrogram(y=negative_frame,n_fft=220,hop_length=110,n_mels=128)\n","                                            spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))\n","                                        \n","                                            for l in range(0,spectrogram_frames.shape[1],2):\n","                                                end_frame = l + 2\n","                                                if end_frame <= spectrogram_frames.shape[1]:\n","                                                    spectrogram_frame = spectrogram_frames[:, l:end_frame]\n","                                                    negative_support_spectrograms.append((spectrogram_frame,0))\n","                                                 \n","\n","\n","                                        start_negative_support_set=end+1\n","                                        if(end>end_support):\n","                                            end_support=end\n","\n","                                        #audio_portion containing the event for the support set\n","                                        audio_event=audio_raw[start:end]\n","                                        spectrograms=[]\n","                                        frames=librosa.feature.melspectrogram(y=audio_event,n_fft=220,hop_length=110,n_mels=128) \n","                                        frames = librosa.pcen(spectrogram_frames * (2**31))\n","        \n","                                        for  l in range(0,frames.shape[1],2):\n","                                            end_frame = l + 2\n","                                            if end_frame <= frames.shape[1]:\n","                                                spectrogram_frame = frames[:, l:end_frame]\n","                                                \n","                                                spectrograms.append((spectrogram_frame,i))\n","                                                spectrogram_frame=torch.tensor(spectrogram_frame)\n","                                                spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                                                spectrograms.append((spectrogram_augmented,i))\n","\n","\n","\n","\n","                                        #data augmentation section for raw audios, the objective is increasing nearly 20x fold the support dimension\n","                                        audios_augmented=augment_audio(audio_event,sr)\n","                                        for audio_augmented in audios_augmented:\n","                                            spectrogram_frames=librosa.feature.melspectrogram(y=audio_augmented,n_fft=220,hop_length=110,n_mels=128)\n","                                            spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))\n","                                            for  l in range(0,spectrogram_frames.shape[1],2):\n","                                                end_frame = l + 2\n","                                                if end_frame <= spectrogram_frames.shape[1]:\n","                                                    spectrogram_frame = spectrogram_frames[:, l:end_frame]      \n","                                                    spectrograms.append((spectrogram_frame,i))\n","                                                    spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                                                    spectrograms.append((spectrogram_augmented,i))\n","\n","\n","                                        support_spectrograms.extend(spectrograms)\n","\n","                                   \n","                                    support_spectrograms.extend((negative_support_spectrograms))\n","\n","\n","\n","                                    audio_query=audio_raw[end_support+160:]\n","                                    \n","                                    frames=get_frames(audio_query,int(mean_duration),int(mean_duration))\n","                                    query_spectrograms=[]\n","                                    z=len(labels[file_id])\n","                                    print(z)\n","                                    k=int((end_support/16000)/0.01)\n","                                    x=0\n","                                    frames=get_frames(audio_query,int(mean_duration),int(mean_duration))\n","\n","                                    for frame in frames:\n","                                        spectrogram_frames=librosa.feature.melspectrogram(y=frame,n_fft=220,hop_length=110,n_mels=128) #labels must go from labels(k) to labels(k+)\n","                                        spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))\n","                                        for l in range(0,spectrogram_frames.shape[1],2):\n","                                            end_frame = l + 2\n","                                            if end_frame <= spectrogram_frames.shape[1]:\n","                                                spectrogram_frame = spectrogram_frames[:, l:end_frame]      \n","                                                x+=1\n","                                                \n","                                                if(x%2)==0:\n","                                                    if(k>z):break\n","                                                    query_spectrograms.append((spectrogram_frame,labels[file_id][k]))\n","                                                    \n","                                                    k+=1\n","                                                else: \n","                                                    if(k>z):break\n","                                                    \n","                                                    if labels[file_id][k]!=labels[file_id][k+1]: \n","                                                        query_spectrograms.append((spectrogram_frame,i)) #there is a frame 50% 0 and 50% 1, must be choosen if put 0 or 1.\n","                                                    else: \n","                                                        query_spectrograms.append((spectrogram_frame,labels[file_id][k]))\n","\n","                                    episode=(i,support_spectrograms,query_spectrograms)\n","                                    print(len(episode[1]))\n","                                    print(len(episode[2]))\n","                                    #saving the episode in a file.\n","                                    file_name=f'training_episode {j}.pth'\n","                                    file_path=os.path.join(training_directory_path_spectrograms_episodes_AST, file_name)\n","                                    torch.save(episode,file_path)\n","                                    episodes.append(episode)\n","\n","                                    print(f\"Finito episodio,salvataggio con successo: {j}\")\n","                                    j=j+1\n","\n","    return 1"]},{"cell_type":"markdown","id":"99fe0b18","metadata":{"id":"99fe0b18"},"source":["### Support and query set creation for raw audios. The raw audios can be used then for Wav2Vec model as inputs"]},{"cell_type":"code","execution_count":36,"id":"d595e638","metadata":{"id":"d595e638"},"outputs":[],"source":["def create_support_and_query_sets_raw_audio(labels,file_paths, support_set_dictionary,episodi_non_presenti,num_support_per_class=5):\n","    episodes = [] #episodes of training for the prototypical network. Considering an audio with 4 classes in it, we want 4 episodes.\n","    j=1\n","    #from the label[audio][frame] structure it returns a sequence of tuples. For each audio file it corrisponds the labels for the audio.\n","\n","    for file_id,label_list in enumerate(labels):\n","\n","        #dictionary which, for each label values contains every occurance of the label in a (file,label_id) format.\n","        #We'll use it to create the query and support_set.\n","\n","\n","            #label_name=get_classname_from_label_index(label_mapping,label)\n","            audio_name=audio_list_training[file_id]\n","            path=train_file_paths[file_id]\n","            #audio_raw,sr = librosa.load(path)\n","            if not os.path.exists(training_directory_path_raw_audio_episodes):\n","                os.makedirs(\"episodes_raw_audio_pth\")\n","\n","            print(f\" Inizio creazione support e query per l'audio: {audio_name}\")\n","\n","            lista_classi_in_audio=set()\n","            for label in label_list:\n","                  if(label!=0):\n","                    lista_classi_in_audio.add(label)\n","            for i in lista_classi_in_audio:\n","                label_name=get_classname_from_label_index(label_mapping,i)\n","                if(j not in episodi_non_presenti):\n","                    print(f\" Episodio {j} già presente, si passa al successivo\")\n","                    j=j+1\n","                    continue\n","                audio_raw,sr = librosa.load(path,sr=16000)\n","                for classe,struct in support_set_dictionary.items():\n","                            for audio,eventi in struct.items():\n","\n","                                support_raw_audios=[]\n","                                negative_support_raw_audios=[]\n","                                if (classe==label_name and audio==audio_name):\n","                                    events=support_set_dictionary[classe][audio]\n","                                    events=events[:num_support_per_class]\n","                                    end_support=0\n","                                    start_negative_support_set=0\n","                                    for evento in events:\n","                                        start=int(evento['start']*sr)\n","                                        end=int(evento['end']*sr)\n","\n","                                        #creation support set raw audio for \"unknown\" class.\n","                                        audio_event=audio_raw[start_negative_support_set:start]\n","                                        negative_frames=get_frames(audio_event,160,160)\n","                                        for frame in negative_frames:\n","                                            negative_support_raw_audios.append((frame,0))\n","\n","\n","\n","                                        start_negative_support_set=end+1\n","                                        if(end>end_support):\n","                                            end_support=end\n","\n","                                        #audio_portion containing the event for the support set\n","                                        audio_event=audio_raw[start:end]\n","\n","                                        frames=get_frames(audio_event,160,160) #without overlapping, hop_length=frame_lenght\n","                                        raw_audios=[]\n","                                        for frame in frames:\n","\n","                                            raw_audios.append((frame,i))\n","\n","\n","                                        #data augmentation section for raw audios, the objective is increasing nearly 20x fold the support dimension\n","                                        audios_augmented=augment_audio(audio_event,sr)\n","                                        for audio_augmented in audios_augmented:\n","                                            frames=get_frames(audio_augmented,160,160)\n","                                            for frame in frames:\n","                                                raw_audios.append((frame,i))\n","\n","\n","\n","                                        support_raw_audios.extend(raw_audios)\n","\n","                              \n","\n","                                    support_raw_audios.extend((negative_support_raw_audios))\n","\n","\n","\n","                                    audio_query=audio_raw[end_support+160:]\n","                                    frames=get_frames(audio_query,160,160)\n","                                    raw_audio_query=[]\n","                                    print(len(frames))\n","                                    print(len(label_list)-(end_support/16000)/0.01)\n","                                    k=0\n","\n","                                    for frame_id in range(int((end_support/16000)/0.01),len(frames)+int((end_support/16000)/0.01)):\n","                                        frame=frames[k]\n","                                        raw_audio_query.append((frame,labels[file_id][frame_id]))\n","                                        k=k+1\n","                                    episode=(i,support_raw_audios,raw_audio_query)\n","                                    #saving the episode in a file.\n","                                    file_name=f'training_episode {j}.pth'\n","                                    file_path=os.path.join(training_directory_path_raw_audio_episodes, file_name)\n","                                    torch.save(episode,file_path)\n","                                    episodes.append(episode)\n","\n","                                    print(f\"Finito episodio,salvataggio con successo: {j}\")\n","                                    j=j+1\n","\n","    return 1"]},{"cell_type":"code","execution_count":null,"id":"274cb668","metadata":{"id":"274cb668","outputId":"a16cf562-d783-4d59-9ef4-c8b84a0f685e"},"outputs":[{"data":{"text/plain":["54"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["n= conta_episodi(support_set)\n","n"]},{"cell_type":"markdown","id":"a69f1d9e","metadata":{"id":"a69f1d9e"},"source":["## Support and query Set creation for spectrograms + save"]},{"cell_type":"markdown","id":"6420a69a","metadata":{},"source":["### Creation with all negative samples for support set"]},{"cell_type":"code","execution_count":89,"id":"02bbb96f","metadata":{"id":"02bbb96f","outputId":"7f444586-ecb3-401e-cab3-d57ef78b0c66"},"outputs":[{"name":"stdout","output_type":"stream","text":["[38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]\n"," Inizio creazione support e query per l'audio: e1.wav\n"," Episodio 1 già presente, si passa al successivo\n"," Episodio 2 già presente, si passa al successivo\n"," Episodio 3 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: h1.wav\n"," Episodio 4 già presente, si passa al successivo\n"," Episodio 5 già presente, si passa al successivo\n"," Episodio 6 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: y1.wav\n"," Episodio 7 già presente, si passa al successivo\n"," Episodio 8 già presente, si passa al successivo\n"," Episodio 9 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: n1.wav\n"," Episodio 10 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: a1.wav\n"," Episodio 11 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: sp4f1_2015_04_24-05_00_00_0002.wav\n"," Episodio 12 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: dcase_MK2.wav\n"," Episodio 13 già presente, si passa al successivo\n"," Episodio 14 già presente, si passa al successivo\n"," Episodio 15 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: dcase_MK1.wav\n"," Episodio 16 già presente, si passa al successivo\n"," Episodio 17 già presente, si passa al successivo\n"," Episodio 18 già presente, si passa al successivo\n"," Episodio 19 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: 2015-09-25_04-00-00_unit10.wav\n"," Episodio 20 già presente, si passa al successivo\n"," Episodio 21 già presente, si passa al successivo\n"," Episodio 22 già presente, si passa al successivo\n"," Episodio 23 già presente, si passa al successivo\n"," Episodio 24 già presente, si passa al successivo\n"," Episodio 25 già presente, si passa al successivo\n"," Episodio 26 già presente, si passa al successivo\n"," Episodio 27 già presente, si passa al successivo\n"," Episodio 28 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: 2015-09-21_06-00-00_unit05.wav\n"," Episodio 29 già presente, si passa al successivo\n"," Episodio 30 già presente, si passa al successivo\n"," Episodio 31 già presente, si passa al successivo\n"," Episodio 32 già presente, si passa al successivo\n"," Episodio 33 già presente, si passa al successivo\n"," Episodio 34 già presente, si passa al successivo\n"," Episodio 35 già presente, si passa al successivo\n"," Episodio 36 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: 2015-09-04_08-04-59_unit03.wav\n"," Episodio 37 già presente, si passa al successivo\n","707695\n","706092.0\n","Finito episodio,salvataggio con successo: 38\n","712374\n","710760.0\n","Finito episodio,salvataggio con successo: 39\n","523559\n","522372.90249433107\n","Finito episodio,salvataggio con successo: 40\n","653533\n","652052.9024943311\n","Finito episodio,salvataggio con successo: 41\n","150545\n","150205.40136054426\n","Finito episodio,salvataggio con successo: 42\n","605865\n","604493.0022675737\n","Finito episodio,salvataggio con successo: 43\n"," Inizio creazione support e query per l'audio: 2015-10-14_23-59-59_unit05.wav\n","679441\n","677902.1043083901\n","Finito episodio,salvataggio con successo: 44\n","386605\n","385730.0\n","Finito episodio,salvataggio con successo: 45\n","652497\n","651018.9024943311\n","Finito episodio,salvataggio con successo: 46\n","621614\n","620206.3038548753\n","Finito episodio,salvataggio con successo: 47\n","678682\n","677144.4036281179\n","Finito episodio,salvataggio con successo: 48\n"," Inizio creazione support e query per l'audio: 2015-09-11_06-00-00_unit07.wav\n","665178\n","663671.0022675737\n","Finito episodio,salvataggio con successo: 49\n","441855\n","440854.60317460314\n","Finito episodio,salvataggio con successo: 50\n","633635\n","632199.7006802721\n","Finito episodio,salvataggio con successo: 51\n","711141\n","709529.8004535148\n","Finito episodio,salvataggio con successo: 52\n","718873\n","717244.0\n","Finito episodio,salvataggio con successo: 53\n","454199\n","453170.6031746032\n","Finito episodio,salvataggio con successo: 54\n","Creazione conclusa\n"]}],"source":["episodi_non_presenti=[]\n","for j in range(1,55):\n","    file_name=f'training_episode {j}.pth'\n","    file_path=os.path.join(training_directory_path_spectrograms__all_negative_supports_episodes, file_name)\n","    if os.path.exists(file_path):\n","       continue\n","\n","    else:\n","        episodi_non_presenti.append(j)\n","print(episodi_non_presenti)\n","\n","\n","codice_ritorno= create_support_and_query_sets_all_negative_support(labels,train_file_paths,support_set,episodi_non_presenti=episodi_non_presenti, num_support_per_class=5)\n","if(codice_ritorno==1):\n","    print(\"Creazione conclusa\")"]},{"cell_type":"markdown","id":"809a075c","metadata":{},"source":["### Creation with balanced support set"]},{"cell_type":"code","execution_count":null,"id":"2dde292c","metadata":{},"outputs":[],"source":["episodi_non_presenti=[]\n","for j in range(1,55):\n","    file_name=f'training_episode {j}.pth'\n","    file_path=os.path.join(training_directory_path_spectrograms_frame_negative_support_undersampling_episodes, file_name)\n","    if os.path.exists(file_path):\n","       continue\n","\n","    else:\n","        episodi_non_presenti.append(j)\n","print(episodi_non_presenti)\n","\n","\n","codice_ritorno= training_directory_path_spectrograms_frame_negative_support_undersampling_episodes(labels,train_file_paths,support_set,episodi_non_presenti=episodi_non_presenti, num_support_per_class=5)\n","if(codice_ritorno==1):\n","    print(\"Creazione conclusa\")"]},{"cell_type":"markdown","id":"b4b02886","metadata":{},"source":["### Creation with spectrograms created for mean positive events duration and not for the single frame."]},{"cell_type":"code","execution_count":141,"id":"f8cddd64","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 2]\n"," Inizio creazione support e query per l'audio: e1.wav\n","360000\n","184333\n","208173\n","Finito episodio,salvataggio con successo: 1\n","360000\n","205965\n","163770\n","Finito episodio,salvataggio con successo: 2\n"," Episodio 3 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: h1.wav\n"," Episodio 4 già presente, si passa al successivo\n"," Episodio 5 già presente, si passa al successivo\n"," Episodio 6 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: y1.wav\n"," Episodio 7 già presente, si passa al successivo\n"," Episodio 8 già presente, si passa al successivo\n"," Episodio 9 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: n1.wav\n"," Episodio 10 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: a1.wav\n"," Episodio 11 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: sp4f1_2015_04_24-05_00_00_0002.wav\n"," Episodio 12 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: dcase_MK2.wav\n"," Episodio 13 già presente, si passa al successivo\n"," Episodio 14 già presente, si passa al successivo\n"," Episodio 15 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: dcase_MK1.wav\n"," Episodio 16 già presente, si passa al successivo\n"," Episodio 17 già presente, si passa al successivo\n"," Episodio 18 già presente, si passa al successivo\n"," Episodio 19 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: 2015-09-25_04-00-00_unit10.wav\n"," Episodio 20 già presente, si passa al successivo\n"," Episodio 21 già presente, si passa al successivo\n"," Episodio 22 già presente, si passa al successivo\n"," Episodio 23 già presente, si passa al successivo\n"," Episodio 24 già presente, si passa al successivo\n"," Episodio 25 già presente, si passa al successivo\n"," Episodio 26 già presente, si passa al successivo\n"," Episodio 27 già presente, si passa al successivo\n"," Episodio 28 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: 2015-09-21_06-00-00_unit05.wav\n"," Episodio 29 già presente, si passa al successivo\n"," Episodio 30 già presente, si passa al successivo\n"," Episodio 31 già presente, si passa al successivo\n"," Episodio 32 già presente, si passa al successivo\n"," Episodio 33 già presente, si passa al successivo\n"," Episodio 34 già presente, si passa al successivo\n"," Episodio 35 già presente, si passa al successivo\n"," Episodio 36 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: 2015-09-04_08-04-59_unit03.wav\n"," Episodio 37 già presente, si passa al successivo\n"," Episodio 38 già presente, si passa al successivo\n"," Episodio 39 già presente, si passa al successivo\n"," Episodio 40 già presente, si passa al successivo\n"," Episodio 41 già presente, si passa al successivo\n"," Episodio 42 già presente, si passa al successivo\n"," Episodio 43 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: 2015-10-14_23-59-59_unit05.wav\n"," Episodio 44 già presente, si passa al successivo\n"," Episodio 45 già presente, si passa al successivo\n"," Episodio 46 già presente, si passa al successivo\n"," Episodio 47 già presente, si passa al successivo\n"," Episodio 48 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: 2015-09-11_06-00-00_unit07.wav\n"," Episodio 49 già presente, si passa al successivo\n"," Episodio 50 già presente, si passa al successivo\n"," Episodio 51 già presente, si passa al successivo\n"," Episodio 52 già presente, si passa al successivo\n"," Episodio 53 già presente, si passa al successivo\n"," Episodio 54 già presente, si passa al successivo\n","Creazione conclusa\n"]}],"source":["episodi_non_presenti=[]\n","for j in range(1,55):\n","    file_name=f'training_episode {j}.pth'\n","    file_path=os.path.join(training_directory_path_spectrograms_episodes, file_name)\n","    if os.path.exists(file_path):\n","       continue\n","\n","    else:\n","        episodi_non_presenti.append(j)\n","print(episodi_non_presenti)\n","\n","\n","codice_ritorno= create_support_and_query_sets_with_new_spectrogram_creation(labels,train_file_paths,support_set,episodi_non_presenti=episodi_non_presenti, num_support_per_class=5)\n","if(codice_ritorno==1):\n","    print(\"Creazione conclusa\")"]},{"cell_type":"markdown","id":"c19b7dc4","metadata":{},"source":["### Spectrogram Creation for AST with 16Khz"]},{"cell_type":"code","execution_count":36,"id":"aa979ad6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]\n"," Inizio creazione support e query per l'audio: e1.wav\n"," Episodio 1 già presente, si passa al successivo\n"," Episodio 2 già presente, si passa al successivo\n"," Episodio 3 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: h1.wav\n"," Episodio 4 già presente, si passa al successivo\n"," Episodio 5 già presente, si passa al successivo\n"," Episodio 6 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: y1.wav\n"," Episodio 7 già presente, si passa al successivo\n"," Episodio 8 già presente, si passa al successivo\n"," Episodio 9 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: n1.wav\n"," Episodio 10 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: a1.wav\n"," Episodio 11 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: sp4f1_2015_04_24-05_00_00_0002.wav\n"," Episodio 12 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: dcase_MK2.wav\n"," Episodio 13 già presente, si passa al successivo\n"," Episodio 14 già presente, si passa al successivo\n"," Episodio 15 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: dcase_MK1.wav\n"," Episodio 16 già presente, si passa al successivo\n"," Episodio 17 già presente, si passa al successivo\n"," Episodio 18 già presente, si passa al successivo\n"," Episodio 19 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: 2015-09-25_04-00-00_unit10.wav\n"," Episodio 20 già presente, si passa al successivo\n"," Episodio 21 già presente, si passa al successivo\n"," Episodio 22 già presente, si passa al successivo\n"," Episodio 23 già presente, si passa al successivo\n"," Episodio 24 già presente, si passa al successivo\n"," Episodio 25 già presente, si passa al successivo\n"," Episodio 26 già presente, si passa al successivo\n"," Episodio 27 già presente, si passa al successivo\n"," Episodio 28 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: 2015-09-21_06-00-00_unit05.wav\n"," Episodio 29 già presente, si passa al successivo\n"," Episodio 30 già presente, si passa al successivo\n"," Episodio 31 già presente, si passa al successivo\n"," Episodio 32 già presente, si passa al successivo\n"," Episodio 33 già presente, si passa al successivo\n"," Episodio 34 già presente, si passa al successivo\n"," Episodio 35 già presente, si passa al successivo\n"," Episodio 36 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: 2015-09-04_08-04-59_unit03.wav\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\feature\\spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n","  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["720000\n","11758\n","518606\n","Finito episodio,salvataggio con successo: 37\n","720000\n","12322\n","517792\n","Finito episodio,salvataggio con successo: 38\n","720000\n","8894\n","521213\n","Finito episodio,salvataggio con successo: 39\n","720000\n","147021\n","383064\n","Finito episodio,salvataggio con successo: 40\n","720000\n","51935\n","478170\n","Finito episodio,salvataggio con successo: 41\n","720000\n","419946\n","110143\n","Finito episodio,salvataggio con successo: 42\n","720000\n","86851\n","443289\n","Finito episodio,salvataggio con successo: 43\n"," Inizio creazione support e query per l'audio: 2015-10-14_23-59-59_unit05.wav\n","720000\n","33003\n","497123\n","Finito episodio,salvataggio con successo: 44\n","720000\n","247276\n","282865\n","Finito episodio,salvataggio con successo: 45\n","720000\n","52717\n","477411\n","Finito episodio,salvataggio con successo: 46\n","720000\n","75360\n","454817\n","Finito episodio,salvataggio con successo: 47\n","720000\n","33539\n","496562\n","Finito episodio,salvataggio con successo: 48\n"," Inizio creazione support e query per l'audio: 2015-09-11_06-00-00_unit07.wav\n","720000\n","43471\n","486684\n","Finito episodio,salvataggio con successo: 49\n","720000\n","206864\n","323290\n","Finito episodio,salvataggio con successo: 50\n","720000\n","66550\n","463606\n","Finito episodio,salvataggio con successo: 51\n","720000\n","9839\n","520311\n","Finito episodio,salvataggio con successo: 52\n","720000\n","4154\n","525976\n","Finito episodio,salvataggio con successo: 53\n","720000\n","197837\n","332321\n","Finito episodio,salvataggio con successo: 54\n","Creazione conclusa\n"]}],"source":["episodi_non_presenti=[]\n","for j in range(1,55):\n","    file_name=f'training_episode {j}.pth'\n","    file_path=os.path.join(training_directory_path_spectrograms_episodes_AST, file_name)\n","    if os.path.exists(file_path):\n","       continue\n","\n","    else:\n","        episodi_non_presenti.append(j)\n","print(episodi_non_presenti)\n","\n","\n","codice_ritorno= create_support_and_query_sets_with_new_spectrogram_creationAST_16KHz(labels,train_file_paths,support_set,episodi_non_presenti=episodi_non_presenti, num_support_per_class=5)\n","if(codice_ritorno==1):\n","    print(\"Creazione conclusa\")"]},{"cell_type":"markdown","id":"a4b18d76","metadata":{"id":"a4b18d76"},"source":["### Support and query Set creation for raw audio + save"]},{"cell_type":"code","execution_count":122,"id":"7631b7b4","metadata":{"id":"7631b7b4","outputId":"e835323f-2b68-4ac3-89dd-a89ad9f4526c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]\n"," Inizio creazione support e query per l'audio: e1.wav\n"," Episodio 1 già presente, si passa al successivo\n"," Episodio 2 già presente, si passa al successivo\n"," Episodio 3 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: h1.wav\n"," Episodio 4 già presente, si passa al successivo\n"," Episodio 5 già presente, si passa al successivo\n"," Episodio 6 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: y1.wav\n"," Episodio 7 già presente, si passa al successivo\n"," Episodio 8 già presente, si passa al successivo\n"," Episodio 9 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: n1.wav\n"," Episodio 10 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: a1.wav\n"," Episodio 11 già presente, si passa al successivo\n"," Inizio creazione support e query per l'audio: sp4f1_2015_04_24-05_00_00_0002.wav\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1472\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["40685\n","40687.43125\n","Finito episodio,salvataggio con successo: 12\n"," Inizio creazione support e query per l'audio: dcase_MK2.wav\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1904\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["259483\n","259485.2\n","Finito episodio,salvataggio con successo: 13\n","260696\n","260698.2\n","Finito episodio,salvataggio con successo: 14\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1344\n","  warnings.warn(\n","C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=816\n","  warnings.warn(\n","C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=624\n","  warnings.warn(\n","C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=784\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["125854\n","125855.9\n","Finito episodio,salvataggio con successo: 15\n"," Inizio creazione support e query per l'audio: dcase_MK1.wav\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1552\n","  warnings.warn(\n","C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1440\n","  warnings.warn(\n","C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1200\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["190525\n","190526.9\n","Finito episodio,salvataggio con successo: 16\n","89725\n","89726.90000000001\n","Finito episodio,salvataggio con successo: 17\n","31558\n","31559.900000000023\n","Finito episodio,salvataggio con successo: 18\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=672\n","  warnings.warn(\n","C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=512\n","  warnings.warn(\n","C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=400\n","  warnings.warn(\n","C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=688\n","  warnings.warn(\n","C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=608\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["101011\n","101012.1\n","Finito episodio,salvataggio con successo: 19\n"," Inizio creazione support e query per l'audio: 2015-09-25_04-00-00_unit10.wav\n","582115\n","582116.1\n","Finito episodio,salvataggio con successo: 20\n","225311\n","225312.90000000002\n","Finito episodio,salvataggio con successo: 21\n","697116\n","697117.7\n","Finito episodio,salvataggio con successo: 22\n","569208\n","569209.2\n","Finito episodio,salvataggio con successo: 23\n","678394\n","678395.1\n","Finito episodio,salvataggio con successo: 24\n","717063\n","717064.1\n","Finito episodio,salvataggio con successo: 25\n","440775\n","440776.7\n","Finito episodio,salvataggio con successo: 26\n","712542\n","712543.8\n","Finito episodio,salvataggio con successo: 27\n","637579\n","637580.6\n","Finito episodio,salvataggio con successo: 28\n"," Inizio creazione support e query per l'audio: 2015-09-21_06-00-00_unit05.wav\n","133845\n","133846.0\n","Finito episodio,salvataggio con successo: 29\n","598124\n","598125.3\n","Finito episodio,salvataggio con successo: 30\n","588027\n","588028.3\n","Finito episodio,salvataggio con successo: 31\n","537143\n","537144.3\n","Finito episodio,salvataggio con successo: 32\n","654898\n","654899.0\n","Finito episodio,salvataggio con successo: 33\n","649507\n","649508.0\n","Finito episodio,salvataggio con successo: 34\n","223248\n","223249.2\n","Finito episodio,salvataggio con successo: 35\n","558830\n","558831.1\n","Finito episodio,salvataggio con successo: 36\n"," Inizio creazione support e query per l'audio: 2015-09-04_08-04-59_unit03.wav\n","706901\n","706902.60625\n","Finito episodio,salvataggio con successo: 37\n","706091\n","706092.0\n","Finito episodio,salvataggio con successo: 38\n","710759\n","710760.0\n","Finito episodio,salvataggio con successo: 39\n","522371\n","522372.9\n","Finito episodio,salvataggio con successo: 40\n","652051\n","652052.9\n","Finito episodio,salvataggio con successo: 41\n","150204\n","150205.40000000002\n","Finito episodio,salvataggio con successo: 42\n","604492\n","604493.0\n","Finito episodio,salvataggio con successo: 43\n"," Inizio creazione support e query per l'audio: 2015-10-14_23-59-59_unit05.wav\n","677901\n","677902.1\n","Finito episodio,salvataggio con successo: 44\n","385729\n","385730.0\n","Finito episodio,salvataggio con successo: 45\n","651017\n","651018.9\n","Finito episodio,salvataggio con successo: 46\n","620205\n","620206.3\n","Finito episodio,salvataggio con successo: 47\n","677143\n","677144.4\n","Finito episodio,salvataggio con successo: 48\n"," Inizio creazione support e query per l'audio: 2015-09-11_06-00-00_unit07.wav\n","663670\n","663671.0\n","Finito episodio,salvataggio con successo: 49\n","440853\n","440854.6\n","Finito episodio,salvataggio con successo: 50\n","632198\n","632199.7\n","Finito episodio,salvataggio con successo: 51\n","709528\n","709529.8\n","Finito episodio,salvataggio con successo: 52\n","717243\n","717244.0\n","Finito episodio,salvataggio con successo: 53\n","453169\n","453170.60000000003\n","Finito episodio,salvataggio con successo: 54\n","Creazione conclusa\n"]}],"source":["episodi_non_presenti=[]\n","for j in range(1,55):\n","    file_name=f'training_episode {j}.pth'\n","    file_path=os.path.join(training_directory_path_raw_audio_episodes, file_name)\n","    if os.path.exists(file_path):\n","       continue\n","\n","    else:\n","        episodi_non_presenti.append(j)\n","print(episodi_non_presenti)\n","\n","\n","\n","codice_ritorno=create_support_and_query_sets_raw_audio(labels,train_file_paths,support_set,episodi_non_presenti=episodi_non_presenti, num_support_per_class=5)\n","if(codice_ritorno==1):\n","    print(\"Creazione conclusa\")"]},{"cell_type":"markdown","id":"98ec77fe","metadata":{"id":"98ec77fe"},"source":["# Prototypical Network"]},{"cell_type":"markdown","id":"5bcb236c","metadata":{"id":"5bcb236c"},"source":["## From scratch trained convolutional network encoder  definition"]},{"cell_type":"code","execution_count":72,"id":"e6fac510","metadata":{"id":"e6fac510"},"outputs":[],"source":["#Notes: using spectrograms of dimension 128*2 the max pooling is done only to the height, to avoid the dimension to become 0\n","class PrototypicalNetwork(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(PrototypicalNetwork, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d((2, 1)),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d((2, 1)),\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d((2, 1)),\n","            nn.Flatten(),\n","            nn.Linear(256 *int(input_dim[0]/8)*input_dim[1], hidden_dim),\n","            nn.ReLU(),\n","\n","        )\n","       \n","\n","        # Classification head used for the auxiliary task of multiclass classification.\n","        self.task_auxiliary_head = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embeddings = self.encoder(x)\n","        task_auxiliary_output = self.task_auxiliary_head(embeddings)\n","\n","\n","        return embeddings,2, task_auxiliary_output\n","\n","\n","#function not used.\n","def get_audio_spectrogram(filepath, n_mels=64, n_fft=1024, hop_length=512):\n","    waveform, sample_rate = librosa.load(filepath, sr=None)\n","    mel_spectrogram = librosa.feature.melspectrogram(\n","        y=waveform,\n","        sr=sample_rate,\n","        n_fft=n_fft,\n","        hop_length=hop_length,\n","        n_mels=n_mels\n","    )\n","\n","\n","    pcen_spectrogram = librosa.pcen(mel_spectrogram * (2**31))\n","    pcen_spectrogram = torch.tensor(pcen_spectrogram,dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n","\n","    return pcen_spectrogram"]},{"cell_type":"code","execution_count":73,"id":"8fd9ba82","metadata":{},"outputs":[],"source":["class PrototypicalNetwork_with_binary_head(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(PrototypicalNetwork_with_binary_head, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d((2, 1)),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d((2, 1)),\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d((2, 1)),\n","            nn.Flatten(),\n","            nn.Linear(256 *int(input_dim[0]/8)*input_dim[1], hidden_dim),\n","            nn.ReLU(),\n","\n","        )\n","       \n","\n","        # Classification head used for the auxiliary task of multiclass classification.\n","        self.task_auxiliary_head = nn.Linear(hidden_dim, output_dim)\n","        self.binary_head = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x):\n","        embeddings = self.encoder(x)\n","        task_auxiliary_output = self.task_auxiliary_head(embeddings)\n","        binary_output = self.binary_head(embeddings)\n","\n","\n","        return embeddings,binary_output, task_auxiliary_output\n","\n","\n","#function not used.\n","def get_audio_spectrogram(filepath, n_mels=64, n_fft=1024, hop_length=512):\n","    waveform, sample_rate = librosa.load(filepath, sr=None)\n","    mel_spectrogram = librosa.feature.melspectrogram(\n","        y=waveform,\n","        sr=sample_rate,\n","        n_fft=n_fft,\n","        hop_length=hop_length,\n","        n_mels=n_mels\n","    )\n","\n","\n","    pcen_spectrogram = librosa.pcen(mel_spectrogram * (2**31))\n","    pcen_spectrogram = torch.tensor(pcen_spectrogram,dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n","\n","    return pcen_spectrogram"]},{"cell_type":"markdown","id":"d5fe007c","metadata":{"id":"d5fe007c"},"source":["## AST Pre-trained encoder definition"]},{"cell_type":"code","execution_count":74,"id":"fc123358","metadata":{"id":"fc123358"},"outputs":[],"source":["class AST_Model(nn.Module):\n","    def __init__(self, output_dim):\n","        super().__init__()\n","        # load AST model\n","        self.encoder = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\",attn_implementation=\"sdpa\")\n","        #self.encoder=ASTModel.from_pretrained(\"MIT/ast-finetuned-speech-commands-v2\",attn_implementation=\"sdpa\")  ## va scelto uno dei 2,proviamo speech commands.\n","        # hidden_size = depth of the model\n","        self.hidden_size = self.encoder.config.hidden_size\n","        # freeze ast parameters\n","        for n, p in self.encoder.named_parameters():\n","            p.requires_grad = False\n","\n","\n","        self.intermediate_layer1 = nn.Linear(768, 768*4)\n","        self.intermediate_layer2 = nn.Linear(768*4, 768)\n","\n","  \n","\n","        self.task_auxiliary_head = nn.Sequential(\n","            nn.Linear(self.hidden_size, 384),\n","            nn.Linear(384, output_dim)\n","        )\n"," \n","\n","\n","\n","    def forward(self, x):\n","        \n","        embeddings = self.encoder(x)[\"last_hidden_state\"][:, 0, :]\n","        embeddings = self.intermediate_layer1(embeddings)\n","        embeddings=self.intermediate_layer2(embeddings)\n","\n","\n","        \n","        task_auxiliary_output = self.task_auxiliary_head(embeddings)\n","\n","        return embeddings,2,task_auxiliary_output\n"]},{"cell_type":"markdown","id":"a395583d","metadata":{"id":"a395583d"},"source":["Spectrogram concatenation on the time-axis to get a spectrogram usable for AST. Default spectrogram shape of (128,100)"]},{"cell_type":"code","execution_count":75,"id":"595885a5","metadata":{"id":"595885a5"},"outputs":[],"source":["def ASTSpectrogramConcatenation(tensor,time_spectrogram_dimension=130):\n","    shape=tensor.shape\n","    print(shape)\n","    #tensor = torch.cat(tensor, dim=2)\n","    new_spectrograms_count = math.floor(shape[0] // math.floor(time_spectrogram_dimension//2))\n","    tensor=tensor[:new_spectrograms_count*int(time_spectrogram_dimension//2),  :]\n","    tensor=tensor.view(new_spectrograms_count, 128, time_spectrogram_dimension)\n","    return tensor"]},{"cell_type":"markdown","id":"e6e2da23","metadata":{"id":"e6e2da23"},"source":["Label correction to obtain a suitable target for the query_set. If the spectrograms go from shape (x,128,2) to (x/50,128,100), the labels must be unified.\n"]},{"cell_type":"code","execution_count":76,"id":"564db254","metadata":{"id":"564db254"},"outputs":[],"source":["def ASTLabelCorrection(labels):\n","    new_labels=[]\n","    label_number=len(labels)\n","    new_labels_number=math.floor(label_number/65)\n","    labels=labels[:new_labels_number*65]\n","\n","    for i in range(new_labels_number):\n","        start_index = i * 65\n","        end_index = start_index + 65\n","        block=labels[start_index:end_index]\n","        #Let's take the most frequent label value.\n","        count=Counter(block)\n","        print(count)\n","        most_frequent_value = count.most_common(1)[0][0]\n","\n","        new_labels.append(most_frequent_value)\n","\n","    return new_labels"]},{"cell_type":"markdown","id":"cf8a0446","metadata":{"id":"cf8a0446"},"source":["## Wave2Vec Pre-trained encoder definition"]},{"cell_type":"code","execution_count":77,"id":"b7d0018e","metadata":{"id":"b7d0018e"},"outputs":[],"source":["class CustomWav2Vec2Model(nn.Module):\n","    def __init__(self, base_model_name, num_labels_task_b=22,config=None):\n","        super(CustomWav2Vec2Model, self).__init__()\n","\n","\n","        if config is None:\n","            self.wav2vec2_encoder = Wav2Vec2Model.from_pretrained(base_model_name, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\",ignore_mismatched_sizes=True)\n","        else:\n","            self.wav2vec2_encoder = Wav2Vec2Model.from_pretrained(base_model_name, config=config, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\",ignore_mismatched_sizes=True)\n","        for n, p in self.wav2vec2_encoder.named_parameters():\n","            p.requires_grad = False\n","\n","\n","        #intermediate layer to implement a learning process\n","        self.intermediate_layer = nn.Linear(self.wav2vec2_encoder.config.hidden_size, 768)\n","        # Classification head for the tasks.task b is  multiclass.\n","        self.classifier_task_b = nn.Linear(768, num_labels_task_b)\n","\n","        \n","\n","\n","    def forward(self, input_values, attention_mask=None):\n","\n","        outputs = self.wav2vec2_encoder(input_values, attention_mask=attention_mask)\n","        embeddings = outputs.last_hidden_state\n","        embeddings=self.intermediate_layer(embeddings)\n","        #pooled_output = hidden_states.mean(dim=1)  ##teoricamente embedding aggregato usando la media, non so se abbia senso usarlo\n","\n","        multiclass_classification_task = self.classifier_task_b(embeddings)\n","\n","        return embeddings,2, multiclass_classification_task"]},{"cell_type":"markdown","id":"2ed03dcb","metadata":{"id":"2ed03dcb"},"source":["Time-domain waveform concatenation to have a suitable input for Wav2Vec2. It's required at least a tensor with shape (batch_size,1000). Every frame has a shape of (220). Therefore we will concatenate 5 frame."]},{"cell_type":"code","execution_count":78,"id":"0355d248","metadata":{"id":"0355d248"},"outputs":[],"source":["def Wav2VecWaveformConcatenation(tensor,time_domain_waveform_dimension=3300):\n","    shape=tensor.shape\n","    #tensor = torch.cat(tensor, dim=2)\n","    new_waveform_count = math.floor(shape[0] // math.floor(time_domain_waveform_dimension//220))\n","    tensor=tensor[:new_waveform_count*int(time_domain_waveform_dimension//220),:]\n","    tensor=tensor.view(new_waveform_count, time_domain_waveform_dimension)\n","    return tensor"]},{"cell_type":"markdown","id":"d49af3b0","metadata":{"id":"d49af3b0"},"source":["Label correction for Wav2Vec. 5 frames are concatenated, therefore the corrispective frames must be aggregated"]},{"cell_type":"code","execution_count":79,"id":"2e7dd516","metadata":{"id":"2e7dd516"},"outputs":[],"source":["def Wav2VecLabelCorrection(labels,time_domain_waveform_dimension=3300):\n","    n_times=int(time_domain_waveform_dimension/220)\n","    new_labels=[]\n","    label_number=len(labels)\n","    new_labels_number=math.floor(label_number/n_times)\n","    labels=labels[:new_labels_number*n_times]\n","\n","    for i in range(new_labels_number):\n","        start_index = i * n_times\n","        end_index = start_index + n_times\n","        block=labels[start_index:end_index]\n","        #Let's take the most frequent label value.\n","        count=Counter(block)\n","        most_frequent_value = count.most_common(1)[0][0]\n","\n","        new_labels.append(most_frequent_value)\n","\n","    return new_labels"]},{"cell_type":"markdown","id":"1d380d50","metadata":{"id":"1d380d50"},"source":["# Cleanup phase"]},{"cell_type":"code","execution_count":33,"id":"07ab6534","metadata":{"id":"07ab6534"},"outputs":[],"source":["labels=0\n","ast_model=0\n","Wav2Vec_model=0\n","output=0\n","dataset_huggingface=0\n","inputs=0\n","feature_extractor=0\n","spectrogram=0\n"]},{"cell_type":"markdown","id":"aac8d874","metadata":{"id":"aac8d874"},"source":["# Training phase"]},{"cell_type":"markdown","id":"9kKnJ7-127gz","metadata":{"id":"9kKnJ7-127gz"},"source":["## Training utilities\n"]},{"cell_type":"code","execution_count":80,"id":"w-lD3eNo2_Ft","metadata":{"id":"w-lD3eNo2_Ft"},"outputs":[],"source":["def compute_class_weights(y):\n","\n","    class_counts = Counter(y)\n","\n","    total_samples = len(y)\n","\n","\n","    class_weights = {}\n","    for cls, count in class_counts.items():\n","        weight = total_samples / (len(class_counts) * count)\n","        class_weights[cls] = weight\n","\n","    return class_weights"]},{"cell_type":"code","execution_count":81,"id":"5cb950fa","metadata":{},"outputs":[],"source":["def query_set_majority_class_undersampling(query_samples,query_labels_binary_classification,query_labels):\n","    query_labels_binary_classification=query_labels_binary_classification.cpu().detach().numpy()\n","    query_labels=query_labels.cpu().detach().numpy()\n","\n","    num_zeros = np.sum(query_labels_binary_classification == 0)\n","    num_ones = np.sum(query_labels_binary_classification == 1)\n","    minority_class_count = min(num_zeros, num_ones)\n","\n","    zero_indices = np.where(query_labels_binary_classification == 0)[0]\n","    one_indices = np.where(query_labels_binary_classification == 1)[0]\n","    selected_zero_indices = np.random.choice(zero_indices, size=minority_class_count, replace=False)\n","    selected_one_indices = np.random.choice(one_indices, size=minority_class_count, replace=False)\n","    balanced_indices = np.concatenate([selected_zero_indices, selected_one_indices])\n","\n","    balanced_query_samples = query_samples[balanced_indices]\n","    balanced_query_labels_binary_classification = query_labels_binary_classification[balanced_indices]\n","    query_labels=query_labels[balanced_indices]\n","\n","\n","    balanced_query_samples=torch.tensor(balanced_query_samples)\n","    balanced_query_labels_binary_classification=torch.tensor(balanced_query_labels_binary_classification)\n","    query_labels=torch.tensor(query_labels)\n","\n","    return balanced_query_samples,balanced_query_labels_binary_classification,query_labels"]},{"cell_type":"markdown","id":"3d58493b","metadata":{},"source":["## Training function 1.0 definition, for all the various encoder at once"]},{"cell_type":"code","execution_count":43,"id":"b9601726","metadata":{},"outputs":[],"source":["def train_prototypical_network(model, dataloader, optimizer,  num_way, device,x=0,model_checkpoint=0,mode=\"classical_network\",preprocessing=\"both\",batch_size=32,class_unbalance=\"undersampling\",rolling_mean=False):\n","    model.train()\n","\n","    stats_dictionary={}\n","\n","    #let's verify if there is an already loaded checkpoint for the corrisponding encoder. In the checkpoint will be even the last episode done,\n","    #so we can start from the next one skipping the already done ones.\n","    if mode==\"AST\":\n","        batch_size=512\n","\n","    if(isinstance(model_checkpoint,int)):\n","        episode_done=0\n","    else:\n","        if x==1:\n","            path=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +\"_balanced_support_set.pth\")\n","            if(os.path.exists(path)):\n","                stats_dictionary=torch.load(path)\n","            episode_done=model_checkpoint['last_episode_done']\n","        elif x==2:\n","            path=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +\"_complete_support_set.pth\")\n","            if(os.path.exists(path)):\n","                stats_dictionary=torch.load(path)\n","            episode_done=model_checkpoint['last_episode_done']\n","        elif x==3:\n","            path=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +\"_event_spectrogram_set.pth\")\n","            if(os.path.exists(path)):\n","                stats_dictionary=torch.load(path)\n","            episode_done=model_checkpoint['last_episode_done']\n","        else: \n","            path=os.path.join(checkpoint_path, \"model_type_\" + str(mode)+\".pth\")\n","            if(os.path.exists(path)):\n","                stats_dictionary=torch.load(path)\n","            episode_done=model_checkpoint['last_episode_done']\n","\n","    #i is the number of the current episode\n","    i=0\n","\n","    j=0\n","    #j represents the number of total episode, here we got the number of episodes files in the directory.\n","    for file_name in dataloader:\n","        j=j+1\n","\n","    #for each episode file it will train the model\n","    for file_name in dataloader:\n","        #it skips all the episodes beneath the last episode trained.\n","        if (i<episode_done):\n","            i=i+1\n","            print(f'Episode {i} already trained')\n","            continue\n","        #load model checkpoint if There is an already partially trained.\n","        if(model_checkpoint!=0):\n","            model.load_state_dict(model_checkpoint['model_state_dict'])\n","            optimizer.load_state_dict(model_checkpoint['optimizer_state_dict'])\n","\n","        #based on the model type this notebook create a different directory with spectrograms or just raw audio. Based on the model requirements this will choose the directory to use\n","    \n","        if x==1:\n","            file_path=os.path.join(training_directory_path_spectrograms_frame_negative_support_undersampling_episodes, file_name)\n","        elif x==2:\n","            file_path=os.path.join(training_directory_path_spectrograms__all_negative_supports_episodes, file_name)\n","        else: \n","            file_path=os.path.join(training_directory_path_spectrograms_episodes, file_name)\n","\n","\n","        if i==0:\n","            file_path=os.path.join(training_directory_path_spectrograms_episodes_AST, file_name)\n","\n","\n","\n","\n","        episode=torch.load(file_path)\n","\n","        #from the already created episode now we will take the labels and the samples for query and support set.\n","        query_predictions=[]\n","    \n","        label_to_find_for_episode=episode[0]\n","        \n","        support_samples = [x[0] for x in episode[1]]\n","        support_labels = [x[1] for x in episode[1]]\n","\n","        query_samples = [x[0] for x in episode[2]]\n","        query_labels =[x[1] for x in episode[2]]\n","\n","        #query_labels for binary classification. In the audio if we are computing the F1 score we don't care about all the classes label, just the one to find.\n","        query_labels_binary_classification=[1 if element == label_to_find_for_episode else 0 for element in query_labels]\n","\n","        count_of_positive_frames = sum(n == 1 for n in query_labels_binary_classification)\n","        \n","        if mode==\"AST\":\n","           \n","            if count_of_positive_frames<batch_size:\n","                print(i)\n","                i+=1\n","                continue\n","\n","        if mode==\"AST\":\n","            \n","            support_samples=torch.tensor(support_samples)\n","            support_labels=torch.tensor(support_labels)\n","            print(support_samples.shape,support_labels.shape)\n","            mask_1 = support_labels == label_to_find_for_episode\n","            mask_0 = support_labels == 0    \n","\n","            \n","            \n","            support_samples = torch.cat((support_samples[mask_1], support_samples[mask_0]), dim=0)\n","            support_labels = torch.cat((support_labels[mask_1], support_labels[mask_0]), dim=0)\n","            \n","            \n","\n","        #when there are some del are just to avoid memory issues\n","        del episode\n","\n","      \n","\n","\n","        #based on the network type the type of the samples tensor changes. Float32 for classical network and Half(Float16) for\n","        if(mode==\"classical_network\"):\n","            support_samples = torch.tensor(support_samples,dtype=torch.float32)#.to(device)\n","            query_samples = torch.tensor(query_samples,dtype=torch.float32)#.to(device)\n","        else:\n","          support_samples = torch.tensor(support_samples,dtype=torch.float16).to(device)\n","          query_samples = torch.tensor(query_samples,dtype=torch.float16).to(device)\n","\n","\n","        #data normalization/standardization/both\n","        ##Va visto se quando uso AST devo standardizzare rispetto al loro dataset e non al mio\n","        if(preprocessing==\"normalization\"):\n","            min_val = support_samples.min()\n","            max_val = support_samples.max()\n","            support_samples = (support_samples - min_val) / (max_val - min_val)\n","            query_samples = (query_samples - min_val) / (max_val - min_val)\n","        elif(preprocessing==\"standardization\"):\n","            mean = support_samples.mean()\n","            std = support_samples.std()\n","            support_samples = (support_samples - mean) / std\n","            query_samples = (query_samples - mean) / std\n","        elif(preprocessing==\"both\"):\n","            mean = support_samples.mean()\n","            std = support_samples.std()\n","            support_samples = (support_samples - mean) / std\n","            query_samples = (query_samples - mean) / std\n","\n","            min_val = support_samples.min()\n","            max_val = support_samples.max()\n","            support_samples = (support_samples - min_val) / (max_val - min_val)\n","            query_samples = (query_samples - min_val) / (max_val - min_val)\n","\n","           \n","\n","\n","        # samples reshaping to fit the different necessary input shape for the 3 different models.\n","        if(mode==\"AST\"):\n","            feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n","            \n","            support_samples=support_samples.view(-1, 1, 128, 2)\n","            query_samples=query_samples.view(-1, 1, 128, 2)\n","            \"\"\"\n","            query_samples=ASTSpectrogramConcatenation(query_samples)\n","            query_labels=ASTLabelCorrection(query_labels)\n","            query_labels_binary_classification=ASTLabelCorrection(query_labels_binary_classification)\n","\n","            support_samples=ASTSpectrogramConcatenation(support_samples)\n","            support_labels=ASTLabelCorrection(support_labels)\n","            \"\"\"\n","        elif(mode==\"classical_network\"):\n","            \n","            support_samples=support_samples.view(-1, 1, 128, 2)\n","            query_samples=query_samples.view(-1, 1, 128, 2)\n","     \n","\n","\n","        #tensor conversion + to device for the samples and labels.\n","        support_labels = torch.tensor(support_labels)#.to(device)\n","        query_labels = torch.tensor(query_labels)#.to(device)\n","        query_labels_binary_classification=torch.tensor(query_labels_binary_classification).to(device)\n","        \n","\n","\n","        \"\"\"\n","        print(support_samples.shape)\n","        print(support_labels.shape)\n","         \n","        print(query_samples.shape)\n","        print(query_labels.shape)\n","        \"\"\"\n","        #there is a big problem with class imbalance(in the first episode 2k to 205k), it must be handled in some way.\n","        # 3 ways to handle this brutal situation.\n","\n","\n","        #1 Applying SMOTE directly. Then it will balance the classes sample. I'll do it anyway but in my opinion is really bad, overfitting assured...\n","\n","        if class_unbalance==\"SMOTE\":\n","            #smote strategy =auto therefore it increase the minority class to reach the number of majority class.\n","            smote = SMOTE(sampling_strategy='auto', random_state=13)\n","            balanced_query_samples, balanced_query_labels_binary_classification = smote.fit_resample(query_samples, query_labels_binary_classification)\n","            query_labels_binary_classification=torch.tensor(query_labels_binary_classification,dtype=torch.long).to(device)\n","\n","        elif class_unbalance==\"undersampling\":\n","            query_samples,query_labels_binary_classification,query_labels=query_set_majority_class_undersampling(query_samples,query_labels_binary_classification,query_labels)\n","            query_labels_binary_classification=torch.tensor(query_labels_binary_classification,dtype=torch.long)\n","            query_labels=torch.tensor(query_labels)\n","\n","        elif class_unbalance==\"both\":\n","            query_labels_binary_classification=query_labels_binary_classification.cpu().detach().numpy()\n","            num_zeros = np.sum(query_labels_binary_classification == 0)\n","            num_ones = np.sum(query_labels_binary_classification == 1)\n","            minority_class_count = min(num_zeros, num_ones)\n","\n","            minority_class = 0 if num_zeros < num_ones else 1\n","            minority_samples = query_samples[query_labels_binary_classification == minority_class]\n","            minority_labels = query_labels_binary_classification[query_labels_binary_classification == minority_class]\n","\n","\n","            #smote strategy =1.5 therefore it increase by 50% the number of samples.\n","            smote = SMOTE(sampling_strategy='1.5', random_state=13)\n","            minority_samples, minority_labels = smote.fit_resample(minority_samples, minority_labels)\n","\n","            minority_class_count=len(minority_labels)\n","            ############àTo be continued\n","            zero_indices = np.where(query_labels_binary_classification == 0)[0]\n","            one_indices = np.where(query_labels_binary_classification == 1)[0]\n","            selected_zero_indices = np.random.choice(zero_indices, size=minority_class_count, replace=False)\n","            selected_one_indices = np.random.choice(one_indices, size=minority_class_count, replace=False)\n","            balanced_indices = np.concatenate([selected_zero_indices, selected_one_indices])\n","\n","            balanced_query_samples = query_samples[balanced_indices]\n","            balanced_query_labels_binary_classification = query_labels_binary_classification[balanced_indices]\n","\n","            query_labels_binary_classification=torch.tensor(query_labels_binary_classification,dtype=torch.long).to(device)\n","        else:\n","            pass\n","\n","\n","       \n","        query_samples_size=query_samples.shape[0]\n","\n","        #creation of the Dataset using the tensor created before\n","        support_dataset = TensorDataset(support_samples, support_labels)\n","        query_dataset = TensorDataset(query_samples, query_labels, query_labels_binary_classification)\n","\n","        del support_samples\n","        del support_labels\n","        del query_samples\n","        \n","\n","        #loss function definition\n","        \n","        triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2) #, eps=1e-7?\n","        \n","\n","        # weight for the amount of labels in the batch.\n","        count=Counter(query_labels.tolist())\n","\n","        print(count)\n","        weight_dict=compute_class_weights(query_labels.tolist())\n","        weight_vector = [0.0] * num_way\n","        for classs, weight in weight_dict.items():\n","            weight_vector[classs] = weight\n","        class_weights = torch.tensor(weight_vector,dtype=torch.float32)\n","        cross_entropy_weighted = nn.CrossEntropyLoss(weight=class_weights) \n","        cross_entropy_weighted=cross_entropy_weighted\n","\n","        cross_entropy=nn.CrossEntropyLoss()\n","\n","\n","\n","        #loader for support and query set for the current episode. Shuffle=True for classical network, no shuffle for every type of encoders.\n","       \n","        support_loader = DataLoader(support_dataset, batch_size=batch_size, shuffle=False)\n","        query_loader = DataLoader(query_dataset, batch_size=batch_size, shuffle=False)\n","\n","      \n","        # Prototypes creation for all the support set for the episode.\n","\n","        #will be used 2 tensor, 1 for the positive and 1 for the negative prototype. For memory issue the prototype will be computed for each batch and will be made\n","        #the mean with the prototype for the same class of the subsequent batch.\n","        \n","        aggregated_positive_prototype=None\n","        aggregated_negative_prototype=None\n","        with torch.no_grad():\n","            if rolling_mean==False:\n","                negative_embeddings=[]\n","                positive_embeddings=[]\n","            for support_batch,support_labels_batch in support_loader:\n","\n","                if rolling_mean==True:\n","                    negative_embeddings=[]\n","                    positive_embeddings=[]\n","                \n","                support_batch = support_batch.to(device)\n","                support_labels_batch=support_labels_batch.to(device)\n","                \n","                \n","                \n","                if mode==\"classical_network\":\n","                    with autocast():\n","                        for x in range(0,support_batch.shape[0]):\n","                            \n","                            frame=support_batch[x].unsqueeze(0)\n","                            support_embeddings, _, _= model(frame)\n","                            torch.cuda.empty_cache()\n","                            if (support_labels_batch[x]==0):\n","                                negative_embeddings.append(support_embeddings)\n","                            else:\n","                                positive_embeddings.append(support_embeddings)\n","                elif mode==\"AST\":\n","                    if support_batch.shape[0]<batch_size:\n","                        continue\n","                    support_batch=support_batch.squeeze(dim=1)\n","                    support_batch = support_batch.permute(0, 2, 1)\n","                    support_batch = support_batch.reshape(-1, 1024)\n","                    support_batch = support_batch.unsqueeze(0)\n","                    with autocast():\n","                        most_frequent_label = np.bincount(support_labels_batch.cpu()).argmax()\n","                        batch_embedding,_,_=model(support_batch)\n","\n","                        if (most_frequent_label==0):\n","                            negative_embeddings.append(batch_embedding)\n","                        else:\n","                            positive_embeddings.append(batch_embedding)\n","                if rolling_mean==True:\n","                    if len(positive_embeddings) > 0:\n","                        positive_embeddings=torch.stack(positive_embeddings)\n","                        prototype = positive_embeddings.mean(dim=0)\n","                        \n","                        if aggregated_positive_prototype is None:\n","                            aggregated_positive_prototype=prototype\n","                        else:\n","                            aggregated_positive_prototype = (torch.stack([aggregated_positive_prototype, prototype])).mean(dim=0)\n","\n","                    if len(negative_embeddings) > 0:\n","                        negative_embeddings=torch.stack(negative_embeddings)\n","                        prototype = negative_embeddings.mean(dim=0)\n","                        if aggregated_negative_prototype is None:\n","                            aggregated_negative_prototype=prototype\n","                        else:\n","                            aggregated_negative_prototype = (torch.stack([aggregated_negative_prototype, prototype])).mean(dim=0)\n","\n","\n","            if rolling_mean==False:\n","                if len(positive_embeddings) > 0:\n","                    positive_embeddings=torch.stack(positive_embeddings)\n","                    prototype = positive_embeddings.mean(dim=0)\n","                    \n","                    if aggregated_positive_prototype is None:\n","                        aggregated_positive_prototype=prototype\n","                    else:\n","                        aggregated_positive_prototype = (torch.stack([aggregated_positive_prototype, prototype])).mean(dim=0)\n","\n","                if len(negative_embeddings) > 0:\n","                    negative_embeddings=torch.stack(negative_embeddings)\n","                    prototype = negative_embeddings.mean(dim=0)\n","                    if aggregated_negative_prototype is None:\n","                        aggregated_negative_prototype=prototype\n","                    else:\n","                        aggregated_negative_prototype = (torch.stack([aggregated_negative_prototype, prototype])).mean(dim=0)\n","\n","\n","            torch.cuda.empty_cache()\n","            negative_embeddings=[]\n","            positive_embeddings=[]\n","                \n","        #adding the negative and positive prototype for the entire episode.\n","        negative_prototype=aggregated_negative_prototype\n","        positive_prototype=aggregated_positive_prototype\n","        \n","        del support_batch\n","        del support_labels_batch\n","    \n","        torch.cuda.empty_cache()\n","\n","        #starting the query set phase. In this it will be trained the model based on the 2 loss function and then calculated the F1 score for the predictions done based on the prototypes created before.\n","        progress_bar = tqdm(total=query_samples_size)\n","        \n","        episode_loss=0\n","        for query_batch, query_labels_batch, query_labels_binary_classification_batch in query_loader:\n","\n","\n","            query_batch = query_batch.to(device)\n","            query_labels_batch = query_labels_batch.to(device)\n","            query_labels_binary_classification_batch = query_labels_binary_classification_batch.to(device)\n","\n","            cumulative_loss=0\n","            optimizer.zero_grad()\n","\n","            if mode==\"AST\":\n","                with autocast():\n","                    if query_batch.shape[0]<batch_size:\n","                        continue\n","                    query_batch=query_batch.squeeze(dim=1)\n","                    query_batch = query_batch.permute(0, 2, 1)\n","                    query_batch = query_batch.reshape(-1, 1024)\n","                    query_batch = query_batch.unsqueeze(0)\n","                    with autocast():\n","                        most_frequent_label = np.bincount(query_labels_batch.cpu()).argmax()\n","                        most_frequent_binary_label=np.bincount(query_labels_batch.cpu()).argmax()\n","\n","                        query_embeddings,_,multiclass_query_output = model(query_batch)\n","                        most_frequent_label=torch.tensor(most_frequent_label)\n","                        most_frequent_label=most_frequent_label.unsqueeze(0).to(device)\n","                        auxiliary_task_loss=cross_entropy(multiclass_query_output, most_frequent_label)\n","                        \n","                        if(most_frequent_binary_label==0):\n","                            binary_loss = triplet_loss(query_embeddings, negative_prototype,positive_prototype)\n","                        else:\n","                            binary_loss = triplet_loss(query_embeddings, positive_prototype,negative_prototype)\n","\n","        \n","                        loss = binary_loss + 0.8*auxiliary_task_loss \n","                            \n","                        query_embeddings.detach()\n","                        multiclass_query_output.detach()\n","\n","                        if(cumulative_loss==0):\n","                            cumulative_loss=loss\n","                        else: \n","                            cumulative_loss+=loss\n","\n","                        if(episode_loss==0):\n","                            episode_loss=loss\n","                        else:\n","                            episode_loss+=loss\n","                        with torch.no_grad():\n","                            distance_from_negative = F.pairwise_distance(query_embeddings,negative_prototype)\n","                            distance_from_positive=F.pairwise_distance(query_embeddings, positive_prototype)\n","                        if(distance_from_positive<distance_from_negative):\n","                            for _ in range(0,batch_size):\n","                                query_predictions.append(1)\n","                        else:\n","                            for _ in range(0,batch_size):\n","                                query_predictions.append(0)\n","\n","                        progress_bar.update(batch_size)\n","\n","                        \n","            elif mode==\"classical_network\":\n","                for y in range(0,query_batch.shape[0]):\n","                    \n","                    #with autocast():\n","                        \n","                    frame=query_batch[y]\n","                    frame=frame.unsqueeze(0)\n","                    query_embeddings, _, multiclass_query_output = model(frame)\n","\n","                    \"\"\"\"\n","                    print(f'support_embeddings :{support_embeddings.shape}')\n","                    print(f'binary_support_output :{binary_support_output.shape}')\n","                    print(f'multiclass_support_output :{multiclass_support_output.shape}')\n","                    print(f'query_embeddings :{query_embeddings.shape}')\n","                    print(f'binary_query_output :{binary_query_output.shape}')\n","                    print(f'multiclass_query_output :{multiclass_query_output.shape}')\n","                    \n","\n","                    #let's replicate the label value for the sample for each 10 token created by the model.\n","                    \n","                    if (mode==\"Wav2Vec\"):\n","                        print(multiclass_query_output.shape)\n","                        multiclass_query_output = multiclass_query_output.permute(0, 2, 1)\n","                        print(multiclass_query_output.shape)\n","                    \n","                        print(query_labels_batch.shape)\n","                        query_labels_batch = query_labels_batch.unsqueeze(1).expand(-1, 10)\n","                    \"\"\"\n","\n","\n","                    multiclass_query_output=multiclass_query_output.float()\n","                    \n","                    \n","                    with autocast():\n","                        frame_label=query_labels_batch[y]\n","                        if mode==\"classical_network\":\n","                            frame_label=frame_label.unsqueeze(0)\n","                        if(class_unbalance==\"undersampling\"):\n","                            auxiliary_task_loss=cross_entropy(multiclass_query_output, frame_label)\n","                        else:\n","                            auxiliary_task_loss = cross_entropy_weighted(multiclass_query_output, frame_label)\n","\n","                    \n","                    if(query_labels_binary_classification_batch[y]==0):\n","                        binary_loss = triplet_loss(query_embeddings, negative_prototype,positive_prototype)\n","                    else:\n","                        binary_loss = triplet_loss(query_embeddings, positive_prototype,negative_prototype)\n","\n","                    \n","                    loss = binary_loss + 0.8*auxiliary_task_loss  ## va scelto il coefficente dell'auxiliary task.\n","                    \n","                    query_embeddings.detach()\n","                    multiclass_query_output.detach()\n","\n","                    if(cumulative_loss==0):\n","                        cumulative_loss=loss\n","                    else: \n","                        cumulative_loss+=loss\n","\n","                    if(episode_loss==0):\n","                        episode_loss=loss\n","                    else:\n","                        episode_loss+=loss\n","                    \n","                    with torch.no_grad():\n","                        distance_from_negative = F.pairwise_distance(query_embeddings,negative_prototype)\n","                        distance_from_positive=F.pairwise_distance(query_embeddings, positive_prototype)\n","                    \n","                    if(mode==\"Wav2Vec\"):\n","                        \n","                        distance_from_negative=distance_from_negative.squeeze()\n","                        distance_from_positive=distance_from_positive.squeeze()\n","                        distance_from_negative=distance_from_negative.mean(dim=0)\n","                        distance_from_positive=distance_from_positive.mean(dim=0)\n","                    \n","                    \n","                    if(distance_from_positive<distance_from_negative):\n","                        query_predictions.append(1)\n","                    else:\n","                        query_predictions.append(0)\n","\n","                    progress_bar.update(1)\n","                    \n","                    \n","                    del query_embeddings\n","                    del multiclass_query_output\n","                    del distance_from_negative\n","                    del distance_from_positive\n","            torch.cuda.empty_cache()\n","            \n","\n","            if mode==\"AST\":\n","                mean_loss=cumulative_loss\n","            elif mode==\"classical_network\":\n","                mean_loss=cumulative_loss/query_batch.shape[0]\n","            \n","            mean_loss.backward()\n","            optimizer.step()\n","            \n","            del query_batch\n","            del query_labels_batch\n","            del query_labels_binary_classification_batch\n","\n","\n","        if mode==\"AST\":episode_loss=episode_loss*batch_size/query_labels.shape[0]\n","        elif mode==\"classical_network\":\n","            episode_loss=episode_loss/query_labels.shape[0]\n","        #Code to verify an eventual unbalancing of the classes(spoiler 2k to 205k in episode 1) SMOTE + data augmentation...\n","        query_predictions_np = np.array(query_predictions)\n","        count_of_ones = np.sum(query_predictions_np)\n","        count_of_ones_2=np.sum(query_labels_binary_classification.cpu().detach().numpy())\n","        \n","\n","        print(f'Numero frame classificate positive:{count_of_ones},Numero frame realmente positive:{count_of_ones_2}')\n","        \n","\n","        #Let's compute the F1 score for the whole episode.\n","        if len(query_predictions_np)<len(query_labels_binary_classification.cpu().detach().numpy()):\n","            query_labels_binary_classification=query_labels_binary_classification[:len(query_predictions_np)]\n","            \n","        f1 = f1_score(query_labels_binary_classification.cpu().detach().numpy(), query_predictions_np, average='binary')\n","        precision=precision_score(query_labels_binary_classification.cpu().detach().numpy(), query_predictions_np)\n","        recall=recall_score(query_labels_binary_classification.cpu().detach().numpy(), query_predictions_np)\n","        cm = confusion_matrix(query_labels_binary_classification.cpu().detach().numpy(), query_predictions_np)\n","        \n","\n","\n","        print(f'Episode {i + 1}/{j}, Episode Mean Loss: {episode_loss.item()}')\n","        print(f'Episode {i + 1}/{j}, F1_Score: {f1}')\n","        print(f'Episode {i + 1}/{j}, Precision: {precision}')\n","        print(f'Episode {i + 1}/{j}, Recall: {recall}')\n","\n","            \n","        if(class_unbalance==\"undersampling\"):\n","            sampling=\"_undersampled\"\n","        else:\n","            sampling=\"\"\n","        dictionary={}\n","        dictionary[\"F1_Score\"]=f1\n","        dictionary[\"Precision\"]=precision\n","        dictionary[\"Recall\"]=recall\n","        dictionary[\"Confusion_Matrix\"]=cm\n","        \n","        string=\"Episode: \"+str(i+1)\n","        stats_dictionary[string]=dictionary\n","        for param_group in optimizer.param_groups:\n","            current_lr = param_group['lr']\n","\n","\n","        #saving stats\n","\n","        if not os.path.exists(directory_path_training_stats):\n","            os.makedirs(directory_path_training_stats)\n","        torch.save({'stats_dict':stats_dictionary,\n","                    'learning rate':current_lr,\n","                    'class_distribution_query_set':count\n","        },os.path.join(directory_path_training_stats, \"training_stats_\" + str(mode) +str(x)+ \".pth\"))\n","\n","\n","\n","        #saving the model parameters based on the encoder type.\n","\n","        if rolling_mean==True:\n","            rolling=\"_rolling_\"\n","        else:\n","            rolling=\"\"\n","        \n","        if mode==\"AST\" or mode==\"Wav2Vec\":\n","            os.makedirs(checkpoint_path, exist_ok=True)\n","            torch.save({\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'last_episode_done':i+1\n","            }, os.path.join(checkpoint_path, \"model_type_\" + str(mode)+str(rolling) +\".pth\"))\n","            \n","        else: \n","            if x==1:\n","                os.makedirs(checkpoint_path, exist_ok=True)\n","                torch.save({\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'last_episode_done':i+1\n","                }, os.path.join(checkpoint_path, \"model_type_\" + str(mode)+str(rolling)+\"_balanced_support_set.pth\"))\n","            elif x==2:\n","                os.makedirs(checkpoint_path, exist_ok=True)\n","                torch.save({\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'last_episode_done':i+1\n","                }, os.path.join(checkpoint_path, \"model_type_\" + str(mode)+str(rolling)+\"_complete_support_set.pth\"))\n","            else: \n","                os.makedirs(checkpoint_path, exist_ok=True)\n","                torch.save({\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'last_episode_done':i+1\n","                }, os.path.join(checkpoint_path, \"model_type_\" + str(mode)+str(rolling)+\"_event_spectrogram.pth\"))\n","        \n","            \n","        i=i+1\n"]},{"cell_type":"markdown","id":"9f2cfed8","metadata":{},"source":["## Training procedure for Wav2Vec"]},{"cell_type":"code","execution_count":44,"id":"aaa80410","metadata":{},"outputs":[],"source":["def train_prototypical_network_Wav2Vec(model, dataloader, optimizer,  num_way, device,x=0,model_checkpoint=0,mode=\"Wav2Vec\",preprocessing=\"both\",batch_size=32,class_unbalance=\"undersampling\"):\n","    model.train()\n","    model=model.to(device)\n","    stats_dictionary={}\n","    if(isinstance(model_checkpoint,int)):\n","        episode_done=0\n","    else:\n","        if x==1:\n","            path=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +\"_balanced_support_set.pth\")\n","            if(os.path.exists(path)):\n","                stats_dictionary=torch.load(path)\n","            episode_done=model_checkpoint['last_episode_done']\n","        elif x==2:\n","            path=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +\"_complete_support_set.pth\")\n","            if(os.path.exists(path)):\n","                stats_dictionary=torch.load(path)\n","            episode_done=model_checkpoint['last_episode_done']\n","        elif x==3:\n","            path=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +\"_event_spectrogram_set.pth\")\n","            if(os.path.exists(path)):\n","                stats_dictionary=torch.load(path)\n","            episode_done=model_checkpoint['last_episode_done']\n","        else:\n","            path=os.path.join(checkpoint_path, \"model_type_\" + str(mode)+\".pth\")\n","            if(os.path.exists(path)):\n","                stats_dictionary=torch.load(path)\n","            episode_done=model_checkpoint['last_episode_done']\n","\n","    #i is the number of the current episode\n","    i=0\n","\n","    j=0\n","    for file_name in dataloader:\n","        j=j+1\n","\n","    #for each episode file it will train the model\n","    for file_name in dataloader:\n","        #it skips all the episodes beneath the last episode trained.\n","        if (i<episode_done):\n","            i=i+1\n","            print(f'Episode {i} already trained')\n","            continue\n","        #load model checkpoint if There is an already partially trained.\n","        if(model_checkpoint!=0):\n","            model.load_state_dict(model_checkpoint['model_state_dict'])\n","            optimizer.load_state_dict(model_checkpoint['optimizer_state_dict'])\n","\n","        file_path=os.path.join(training_directory_path_raw_audio_episodes, file_name)\n","        episode=torch.load(file_path)\n","\n","        query_predictions=[]\n","\n","        label_to_find_for_episode=episode[0]\n","\n","        support_samples = [x[0] for x in episode[1]]\n","        support_labels = [x[1] for x in episode[1]]\n","\n","        query_samples = [x[0] for x in episode[2]]\n","        query_labels =[x[1] for x in episode[2]]\n","\n","\n","        support_samples=torch.tensor(support_samples)\n","        support_labels=torch.tensor(support_labels)\n","        print(support_samples.shape,support_labels.shape)\n","      \n","        mask_1 = support_labels == label_to_find_for_episode\n","        mask_0 = support_labels == 0\n","\n","\n","\n","        support_samples = torch.cat((support_samples[mask_1], support_samples[mask_0]), dim=0)\n","        support_labels = torch.cat((support_labels[mask_1], support_labels[mask_0]), dim=0)\n","\n","        processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","        processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n","\n","\n","        del episode\n","        query_labels_binary_classification=[1 if element == label_to_find_for_episode else 0 for element in query_labels]\n","\n","        count_of_ones = np.sum(query_labels_binary_classification)\n","        if(count_of_ones)<batch_size:\n","            print(i)\n","            i+=1\n","            continue\n","\n","        support_samples = torch.tensor(support_samples,dtype=torch.float16).to(device)\n","        query_samples = torch.tensor(query_samples,dtype=torch.float16).to(device)\n","       \n","        mean = support_samples.mean()\n","        std = support_samples.std()\n","        support_samples = (support_samples - mean) / std\n","        query_samples = (query_samples - mean) / std\n","\n","        min_val = support_samples.min()\n","        max_val = support_samples.max()\n","        support_samples = (support_samples - min_val) / (max_val - min_val)\n","        query_samples = (query_samples - min_val) / (max_val - min_val)\n","        \n","        processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","\n","\n","        support_labels = torch.tensor(support_labels)#.to(device)\n","        query_labels = torch.tensor(query_labels)#.to(device)\n","        print(query_labels.shape)\n","        query_labels_binary_classification=torch.tensor(query_labels_binary_classification).to(device)\n","\n","       \n","        query_samples,query_labels_binary_classification,query_labels=query_set_majority_class_undersampling(query_samples,query_labels_binary_classification,query_labels)\n","        query_labels_binary_classification=torch.tensor(query_labels_binary_classification,dtype=torch.long)\n","        query_labels=torch.tensor(query_labels)\n","\n","        query_samples_size=query_samples.shape[0]\n","\n","        support_dataset = TensorDataset(support_samples, support_labels)\n","        query_dataset = TensorDataset(query_samples, query_labels, query_labels_binary_classification)\n","\n","        del support_samples\n","        del support_labels\n","        del query_samples\n","\n","        triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2) #, eps=1e-7?\n","\n","       \n","        # weight for the amount of labels in the batch.\n","        count=Counter(query_labels.tolist())\n","\n","       \n","        weight_dict=compute_class_weights(query_labels.tolist())\n","        weight_vector = [0.0] * num_way\n","        for classs, weight in weight_dict.items():\n","            weight_vector[classs] = weight\n","        class_weights = torch.tensor(weight_vector,dtype=torch.float32)\n","        cross_entropy_weighted = nn.CrossEntropyLoss(weight=class_weights)\n","        cross_entropy_weighted=cross_entropy_weighted\n","\n","        cross_entropy=nn.CrossEntropyLoss()\n","\n","        support_loader = DataLoader(support_dataset, batch_size=batch_size, shuffle=False)\n","        query_loader = DataLoader(query_dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","        aggregated_positive_prototype=None\n","        aggregated_negative_prototype=None\n","        negative_embeddings=[]\n","        positive_embeddings=[]\n","\n","        with torch.no_grad():\n","                for support_batch,support_labels_batch in support_loader:\n","                    \n","                    \n","                    support_batch = support_batch\n","                    support_labels_batch=support_labels_batch\n","                    if support_batch.shape[0]<batch_size:\n","                        break\n","                    support_batch = support_batch.view(-1)\n","                    #the output of processor is a dictionay, using .input_values we are taking the right output.\n","                    support_batch=processor(support_batch, return_tensors=\"pt\", sampling_rate=16000).input_values\n","                    support_batch=torch.tensor(support_batch,dtype=torch.half).to(device)\n","                \n","                    with autocast():\n","                        most_frequent_label = np.bincount(support_labels_batch.cpu()).argmax()\n","                        torch.cuda.empty_cache()\n","                        batch_embedding,_,_=model(support_batch)\n","                    \n","                    if (most_frequent_label==0):\n","                        negative_embeddings.append(batch_embedding)\n","                    else:\n","                        positive_embeddings.append(batch_embedding)\n","        \n","                if len(positive_embeddings) > 0:\n","                    positive_embeddings=torch.stack(positive_embeddings)\n","                    prototype = positive_embeddings.mean(dim=0)\n","\n","                    if aggregated_positive_prototype is None:\n","                        aggregated_positive_prototype=prototype\n","                    else:\n","                        aggregated_positive_prototype = (torch.stack([aggregated_positive_prototype, prototype])).mean(dim=0)\n","\n","                if len(negative_embeddings) > 0:\n","                    negative_embeddings=torch.stack(negative_embeddings)\n","                    prototype = negative_embeddings.mean(dim=0)\n","                    if aggregated_negative_prototype is None:\n","                        aggregated_negative_prototype=prototype\n","                    else:\n","                        aggregated_negative_prototype = (torch.stack([aggregated_negative_prototype, prototype])).mean(dim=0)\n","\n","\n","        torch.cuda.empty_cache()\n","        negative_embeddings=[]\n","        positive_embeddings=[]\n","        negative_prototype=aggregated_negative_prototype\n","        positive_prototype=aggregated_positive_prototype\n","\n","        progress_bar = tqdm(total=query_samples_size)\n","\n","        episode_loss=0\n","        for query_batch, query_labels_batch, query_labels_binary_classification_batch in query_loader:\n","\n","            \n","            query_batch = query_batch.to(device)\n","            query_labels_batch = query_labels_batch.to(device)\n","            query_labels_binary_classification_batch = query_labels_binary_classification_batch.to(device)\n","\n","            cumulative_loss=0\n","            if query_labels_batch.shape[0]<batch_size:\n","                break\n","            most_frequent_label = np.bincount(query_labels_batch.cpu()).argmax()\n","            most_frequent_binary_label=np.bincount(query_labels_batch.cpu()).argmax()\n","\n","            query_batch = query_batch.view(-1)\n","        \n","            \n","            query_batch=processor(query_batch, return_tensors=\"pt\", sampling_rate=16000).input_values\n","\n","            query_batch=torch.tensor(query_batch,dtype=torch.half).to(device)\n","\n","            with autocast():\n","                query_embeddings,_,multiclass_query_output = model(query_batch)\n","\n","            aggregated_labels=[]\n","            l=0\n","            for h in range(0,32,2):\n","                if l==0:\n","                    if query_labels_batch[0]!= query_labels_batch[1]:\n","                        l+=1\n","                        continue\n","\n","                if query_labels_batch[h]==query_labels_batch[h+1]:\n","                    aggregated_labels.append(query_labels_batch[h])\n","                else: \n","                    if query_labels_batch[h]==0:\n","                        aggregated_labels.append(query_labels_batch[h+1])\n","                    else : \n","                        aggregated_labels.append(query_labels_batch[h])\n","                    \n","            \n","            if len(aggregated_labels)>15:\n","                aggregated_labels=aggregated_labels[0:15]\n","                    \n","            aggregated_labels=torch.tensor(aggregated_labels).unsqueeze(0).to(device)\n","            \n","            multiclass_query_output=multiclass_query_output.permute(0,2,1)\n","            auxiliary_task_loss=cross_entropy(multiclass_query_output, aggregated_labels)\n","            \n","            if(most_frequent_binary_label==0):\n","                binary_loss = triplet_loss(query_embeddings, negative_prototype,positive_prototype)\n","            else:\n","                binary_loss = triplet_loss(query_embeddings, positive_prototype,negative_prototype)\n","\n","            loss = binary_loss + 0.8*auxiliary_task_loss\n","\n","            query_embeddings.detach()\n","            multiclass_query_output.detach()\n","            \n","            \n","\n","            if(cumulative_loss==0):\n","                cumulative_loss=loss\n","            else:\n","                cumulative_loss+=loss\n","\n","            if(episode_loss==0):\n","                episode_loss=loss\n","            else:\n","                episode_loss+=loss\n","\n","            distance_from_negative = F.pairwise_distance(query_embeddings,negative_prototype)\n","            distance_from_positive=F.pairwise_distance(query_embeddings, positive_prototype)\n","\n","\n","            for h in range (0,15):\n","                if(distance_from_positive[0][h]<distance_from_negative[0][h]):\n","                        query_predictions.append(1)\n","                        query_predictions.append(1)\n","                else:\n","                        query_predictions.append(0)\n","                        query_predictions.append(0)\n","            \n","            query_predictions.append(query_predictions[-1])\n","            query_predictions.append(query_predictions[-1])\n","            \n","            progress_bar.update(batch_size)\n","\n","            mean_loss=cumulative_loss\n","\n","            optimizer.zero_grad()\n","            mean_loss.backward()\n","            optimizer.step()\n","\n","            del query_batch\n","            del query_labels_batch\n","            del query_labels_binary_classification_batch\n","\n","            episode_loss=episode_loss*batch_size/query_labels.shape[0]\n","\n","        query_predictions_np = np.array(query_predictions)\n","        count_of_ones = np.sum(query_predictions_np)\n","        count_of_ones_2=np.sum(query_labels_binary_classification.cpu().detach().numpy())\n","        print(f'Predictate Positive:{count_of_ones}, Realmente Positive:{count_of_ones_2}')\n","      \n","        if len(query_predictions_np)<len(query_labels_binary_classification.cpu().detach().numpy()):\n","            query_labels_binary_classification=query_labels_binary_classification[:len(query_predictions_np)]\n","\n","        f1 = f1_score(query_labels_binary_classification.cpu().detach().numpy(), query_predictions_np, average='binary')\n","        precision=precision_score(query_labels_binary_classification.cpu().detach().numpy(), query_predictions_np)\n","        recall=recall_score(query_labels_binary_classification.cpu().detach().numpy(), query_predictions_np)\n","        cm = confusion_matrix(query_labels_binary_classification.cpu().detach().numpy(), query_predictions_np)\n","\n","\n","\n","        print(f'Episode {i + 1}/{j}, Episode Mean Loss: {episode_loss.item()}')\n","        print(f'Episode {i + 1}/{j}, F1_Score: {f1}')\n","        print(f'Episode {i + 1}/{j}, Precision: {precision}')\n","        print(f'Episode {i + 1}/{j}, Recall: {recall}')\n","\n","\n","\n","        dictionary={}\n","        dictionary[\"F1_Score\"]=f1\n","        dictionary[\"Precision\"]=precision\n","        dictionary[\"Recall\"]=recall\n","        dictionary[\"Confusion_Matrix\"]=cm\n","\n","        string=\"Episode: \"+str(i+1)\n","        stats_dictionary[string]=dictionary\n","        for param_group in optimizer.param_groups:\n","            current_lr = param_group['lr']\n","\n","\n","        #saving stats\n","\n","        if not os.path.exists(directory_path_training_stats):\n","            os.makedirs(directory_path_training_stats)\n","        torch.save({'stats_dict':stats_dictionary,\n","                    'learning rate':current_lr,\n","                    'class_distribution_query_set':count\n","        },os.path.join(directory_path_training_stats, \"training_stats_\" + str(mode) + \".pth\"))\n","\n","\n","\n","        #saving the model parameters based on the encoder type.\n","\n","        os.makedirs(checkpoint_path, exist_ok=True)\n","        torch.save({\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'last_episode_done':i+1\n","        }, os.path.join(checkpoint_path, \"model_type_\" + str(mode) +\".pth\"))\n","        i=i+1\n"]},{"cell_type":"markdown","id":"8a7f8aa6","metadata":{},"source":["## Running 1.0 version"]},{"cell_type":"code","execution_count":82,"id":"2bf0d93d","metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#if i=1 then Wav2Vec, if i=0 AST. If i=2 CNN with negative support undersampling, if i=3 CNN with full negative support, if i=4 CNN with event spectrogram creation and full negative support.\n","i=3\n","\n","rolling_mean=False\n","\n","\n","torch.cuda.empty_cache()\n","if (i==1):\n","  file_names=get_episodes_files_name(training_directory_path_raw_audio_episodes)\n","elif i==2:\n","  file_names=get_episodes_files_name(training_directory_path_spectrograms_frame_negative_support_undersampling_episodes)\n","elif i==3:\n","  file_names=get_episodes_files_name(training_directory_path_spectrograms__all_negative_supports_episodes)\n","elif i==4 :\n","  file_names=get_episodes_files_name(training_directory_path_spectrograms_episodes)\n","elif i==0:\n","   file_names=get_episodes_files_name(training_directory_path_spectrograms_episodes_AST)\n","else: \n","   file_names=0\n"," \n","\n","if i==0:\n","   \"\"\"\n","   print(len(file_names))\n","   episodes_to_remove={9,10,11,13,18,21,28,39,45,49}\n","   file_names=[elemento for h,elemento in enumerate(file_names) if h not in episodes_to_remove]\n","   print(len(file_names))\n","   \"\"\"\n","#num_way=len(label_mapping)\n","num_way=22\n","input_dim=0\n","input_dim = (128, 2)  # Dimension for the spectrogram in our cases.\n","\n","hidden_dim = 768 #for the convolutional neural network encoder, choosen the same as AST and Wav2Vec\n","output_dim = num_way  # Dimensionality of the output space (prototype dimension)\n","\n","\n","CPU=False\n","\n","\n","if(CPU==True):\n","    device = torch.device(\"cpu\")\n","\n","else:\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","if i==1:\n","  \"\"\"\n","  config = Wav2Vec2Config(\n","      sample_rate=22050,\n","      max_sample_size=220,\n","      min_sample_size=220\n","  )\n","  \"\"\"\n","  Wav2Vec_model=0\n","  Wav2Vec_model = CustomWav2Vec2Model(\n","    base_model_name=\"facebook/wav2vec2-large-960h-lv60-self\",\n","    num_labels_task_b=22,\n","    \n",").cuda()\n","\n","\n","\n","if i==0:\n","  ast_model=0\n","  ast_model=AST_Model(output_dim).to(device)\n","\n","if (i>1 or i<0):\n","  classic_model=0\n","  classic_model=PrototypicalNetwork(input_dim=input_dim,output_dim=output_dim,hidden_dim=hidden_dim).to(device)\n","\n","\n","if i==0:\n","  optimizer = torch.optim.Adam(ast_model.parameters(), lr=0.0001)\n","  mode=\"AST\"\n","elif i==1:\n","  optimizer = torch.optim.Adam(Wav2Vec_model.parameters(), lr=0.001)\n","  mode=\"Wav2Vec\"\n","else:\n","  optimizer = torch.optim.Adam(classic_model.parameters(), lr=0.001)\n","  mode=\"classical_network\"\n","\n","if rolling_mean==True:\n","    rolling=\"_rolling_\"\n","else: \n","    rolling=\"\"\n","\n","\n","if i==0 or i==1:\n","  model_info=0\n","  x=0\n","  if os.path.isfile(os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+\".pth\")):\n","      checkpoint_file=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+ \".pth\")\n","      model_info=torch.load(checkpoint_file)\n","elif i==2:\n","    x=1\n","    model_info=0\n","    if os.path.isfile(os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+\"_balanced_support_set.pth\")):\n","        checkpoint_file=os.path.join(checkpoint_path, \"model_type_\" + str(mode)+str(rolling)+ \"_balanced_support_set.pth\")\n","        model_info=torch.load(checkpoint_file)\n","elif i==3:\n","    x=2\n","    model_info=0\n","    if os.path.isfile(os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+\"_complete_support_set.pth\")):\n","        checkpoint_file=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+ \"_complete_support_set.pth\")\n","        model_info=torch.load(checkpoint_file)\n","elif i==4:\n","    x=3\n","    model_info=0\n","    if os.path.isfile(os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+ \"_event_spectrogram.pth\")):\n","        checkpoint_file=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+ \"_event_spectrogram.pth\")\n","        model_info=torch.load(checkpoint_file)\n","else: \n","    model_info=0\n","\n","\n","if i==0:train_prototypical_network(ast_model, file_names, optimizer, num_way, device,mode=mode,model_checkpoint=model_info)\n","elif i==1:train_prototypical_network_Wav2Vec(Wav2Vec_model, file_names, optimizer, num_way, device,mode=mode,model_checkpoint=model_info)\n","else:train_prototypical_network(classic_model, file_names, optimizer, num_way, device,x=x,mode=mode,model_checkpoint=model_info)"]},{"cell_type":"markdown","id":"f68e2a06","metadata":{},"source":["# Inference"]},{"cell_type":"markdown","id":"75532806","metadata":{},"source":["## Support and Query Set Creation for just an audio"]},{"cell_type":"markdown","id":"6e5a529d","metadata":{},"source":["### Creation of support and query dictionaries containing positive events start and end"]},{"cell_type":"code","execution_count":82,"id":"cd563b89","metadata":{},"outputs":[],"source":["def get_support_set_inference(audio_name):\n","    audio_support_events=[]\n","    with open(val_json, 'r') as file:\n","        dati = json.load(file)\n","        for classe,value in dati.items():\n","            for csv in dati[classe]:\n","                if(dati[classe][csv][\"Support\"]!={}):\n","                    if dati[classe][csv][\"Audio\"] ==audio_name:\n","\n","                        lista_start_e_end_time_eventi=[]\n","                        for evento in dati[classe][csv][\"Support\"]:\n","                            lista_start_e_end_time_eventi.append(dati[classe][csv][\"Support\"][evento])\n","                        audio_support_events=lista_start_e_end_time_eventi\n","    return audio_support_events\n","\n","def get_query_set_inference(audio_name):\n","    audio_query_events=[]\n","    with open(val_json, 'r') as file:\n","        dati = json.load(file)\n","        for classe,value in dati.items():\n","            for csv in dati[classe]:\n","                if(dati[classe][csv][\"Query\"]!={}):\n","                    if dati[classe][csv][\"Audio\"] ==audio_name:\n","\n","                        lista_start_e_end_time_eventi=[]\n","                        for evento in dati[classe][csv][\"Query\"]:\n","                            lista_start_e_end_time_eventi.append(dati[classe][csv][\"Query\"][evento])\n","                        audio_query_events=lista_start_e_end_time_eventi\n","    return audio_query_events         "]},{"cell_type":"markdown","id":"374253de","metadata":{},"source":["### Audio Labeling"]},{"cell_type":"code","execution_count":83,"id":"3bbf2a57","metadata":{},"outputs":[],"source":["def label_audio(support_set,query_set,audio,frame_length=0.01):\n","    labels=[]\n","    class_dictionary={}\n","    class_dictionary['UNKNOWN']=0\n","    audio_path=os.path.join(val_dir,audio)\n","    audio_loaded, sr=librosa.load(audio_path)\n","    length_in_seconds=len(audio_loaded) / sr\n","\n","    number_of_frames=math.ceil(length_in_seconds/frame_length)\n","    for i in range(number_of_frames):\n","        labels.append(0)\n","    label_classe=1\n","    \n","    for event in support_set:\n","        event_start=event['start']\n","        event_end=event['end']\n","        frame_start=int(event_start/frame_length)\n","        frame_end=int(event_end/frame_length)\n","\n","        for i in range (frame_start,frame_end+1):\n","            labels[i]=label_classe\n","\n","\n","\n","    for event in query_set:\n","        event_start=event['start']\n","        event_end=event['end']\n","        frame_start=int(event_start/frame_length)\n","        frame_end=int(event_end/frame_length)\n","\n","        for i in range (frame_start,frame_end):\n","            labels[i]=label_classe\n","    return labels"]},{"cell_type":"markdown","id":"28e78825","metadata":{},"source":["## Support and query set creation in Spectrograms format. Uniform spectrogram creation."]},{"cell_type":"code","execution_count":84,"id":"306364af","metadata":{},"outputs":[],"source":["def create_support_and_query_set_inference_audio_spectrograms(labels,audio_name, support_set_events,num_support_per_class=5,pcen=False,augment_negative_support=False):\n","\n","    if pcen==False:\n","        if not os.path.exists(directory_path_spectrograms_inference):\n","            os.makedirs(directory_path_spectrograms_inference)\n","    else: \n","         if not os.path.exists(directory_path_spectrograms_inference_pcen):\n","            os.makedirs(directory_path_spectrograms_inference_pcen)\n","\n","   \n","    print(f\" Inizio creazione support e query set spectrograms per l'audio: {audio_name}\")\n","\n","    path=os.path.join(val_dir,audio_name)\n","    audio_raw,sr = librosa.load(path)\n","    events=support_set_events[:num_support_per_class]\n","    end_support=0\n","    start_negative_support_set=0\n","    support_spectrograms=[]\n","    negative_support_spectrograms=[]\n","\n","\n","    mean_duration=0\n","    for evento in events:\n","        start=int(evento['start']*sr)\n","        end=int(evento['end']*sr)\n","        if mean_duration==0: mean_duration=end-start\n","        else: mean_duration+=end-start\n","    mean_duration=mean_duration/5\n","\n","    for evento in events:\n","        start=int(evento['start']*sr)\n","        end=int(evento['end']*sr)\n","        \n","        #creation support set spectrograms for \"unknown\" class.\n","        audio_event=audio_raw[start_negative_support_set:start]\n","        \n","        \"\"\"\n","        negative_frames=get_frames(audio_event,220,220)\n","        for frame in negative_frames:\n","        \"\"\"\n","\n","        if augment_negative_support==True:\n","            audios_augmented=augment_audio(audio_event,sr)\n","            for audio_augmented in audios_augmented:\n","\n","                negative_audio_augmented=get_frames(audio_augmented,int(mean_duration),int(mean_duration))\n","                for negative_portion in negative_audio_augmented:\n","                    spectrogram_frames=librosa.feature.melspectrogram(y=negative_portion,n_fft=220,hop_length=110,n_mels=128)\n","                    if pcen==True:\n","                        spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))  \n","                    for  i in range(0,spectrogram_frames.shape[1],2):\n","                        end_frame = i + 2\n","                        if end_frame <= spectrogram_frames.shape[1]:\n","                            spectrogram_frame = spectrogram_frames[:, i:end_frame] \n","                            spectrogram_frame=torch.tensor(spectrogram_frame)     \n","                            negative_support_spectrograms.append((spectrogram_frame,0))\n","                            spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                            spectrogram_augmented=torch.tensor(spectrogram_augmented)\n","                            negative_support_spectrograms.append((spectrogram_augmented,0))\n","        else: \n","            negative_frames=get_frames(audio_event,int(mean_duration),int(mean_duration))\n","            for negative_frame in  negative_frames:\n","                spectrogram_frames=librosa.feature.melspectrogram(y=negative_frame,n_fft=220,hop_length=110,n_mels=128)\n","                if pcen==True:\n","                    spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))  #PCEN\n","        \n","                for i in range(0,spectrogram_frames.shape[1],2):\n","                    end_frame = i + 2\n","                    if end_frame <= spectrogram_frames.shape[1]:\n","                        spectrogram_frame = spectrogram_frames[:, i:end_frame]\n","                        spectrogram_frame=torch.tensor(spectrogram_frame)\n","                        negative_support_spectrograms.append((spectrogram_frame,0))\n","                \n","\n","\n","        start_negative_support_set=end+1\n","        if(end>end_support):\n","            end_support=end\n","\n","        #audio_portion containing the event for the support set\n","        audio_event=audio_raw[start:end]\n","\n","         #without overlapping, hop_length=frame_lenght\n","        spectrograms=[]\n","        \n","        spectrogram_frames=librosa.feature.melspectrogram(y=audio_event,n_fft=220,hop_length=110,n_mels=128)\n","        if pcen==True:\n","            spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))  \n","        for  i in range(0,spectrogram_frames.shape[1],2):\n","\n","            end_frame = i + 2\n","            if end_frame <= spectrogram_frames.shape[1]:\n","                spectrogram_frame = spectrogram_frames[:, i:end_frame]\n","                spectrogram_frame=torch.tensor(spectrogram_frame)\n","                spectrograms.append((spectrogram_frame,1))\n","                \n","                spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                spectrograms.append((spectrogram_augmented,1))\n","\n","\n","\n","        #data augmentation section for raw audios, the objective is increasing nearly 20x fold the support dimension\n","        audios_augmented=augment_audio(audio_event,sr)\n","        for audio_augmented in audios_augmented:\n","            spectrogram_frames=librosa.feature.melspectrogram(y=audio_augmented,n_fft=220,hop_length=110,n_mels=128)\n","            if pcen==True:\n","                spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))  \n","            for  i in range(0,spectrogram_frames.shape[1],2):\n","                end_frame = i + 2\n","                if end_frame <= spectrogram_frames.shape[1]:\n","                    spectrogram_frame = spectrogram_frames[:, i:end_frame] \n","                    spectrogram_frame=torch.tensor(spectrogram_frame)     \n","                    spectrograms.append((spectrogram_frame,1))\n","                    spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                    spectrogram_augmented=torch.tensor(spectrogram_augmented)\n","                    spectrograms.append((spectrogram_augmented,1))\n","\n","\n","\n","        support_spectrograms.extend(spectrograms)\n","    #all negative spectrogram if this code is commented, otherwise balanced support set.\n","\n","    \"\"\"\n","    negative_samples_to_keep=len(support_spectrograms)\n","    print(negative_samples_to_keep)\n","    print(len(negative_support_spectrograms))\n","    if(len(negative_support_spectrograms)>=negative_samples_to_keep):\n","        negative_support_spectrograms=random.sample(negative_support_spectrograms,negative_samples_to_keep)\n","    \"\"\"\n","\n","    \n","    support_spectrograms.extend((negative_support_spectrograms))\n","\n","    query_spectrograms=[]\n","    audio_query=audio_raw[end_support+220:]\n","\n","    j=len(labels)\n","    k=int((end_support/22050)/0.01)\n","    x=0\n","    frames=get_frames(audio_query,int(mean_duration),int(mean_duration))\n","\n","    for frame in frames:\n","        spectrogram_frames=librosa.feature.melspectrogram(y=frame,n_fft=220,hop_length=110,n_mels=128) #labels must go from labels(k) to labels(k+)\n","        if pcen==True:\n","            spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))  \n","        for i in range(0,spectrogram_frames.shape[1],2):\n","            end_frame = i + 2\n","            if end_frame <= spectrogram_frames.shape[1]:\n","                spectrogram_frame = spectrogram_frames[:, i:end_frame] \n","                spectrogram_frame=torch.tensor(spectrogram_frame)    \n","                x+=1\n","                if(x%2)==0:\n","                    query_spectrograms.append((spectrogram_frame,labels[k]))\n","                    if(k+1>j):break\n","                    k+=1\n","                else: \n","                    if labels[k]!=labels[k+1]: \n","                        query_spectrograms.append((spectrogram_frame,1)) #there is a frame 50% 0 and 50% 1, must be choosen if put 0 or 1.\n","                    else: \n","                        query_spectrograms.append((spectrogram_frame,labels[k]))\n","\n","   \n","\n","\n","    audio_support_and_query_set=(support_spectrograms,query_spectrograms)\n","    file_name=f'inference_audio{audio_name}.pth'\n","\n","    if pcen==False:\n","        file_path=os.path.join(directory_path_spectrograms_inference, file_name)\n","    else: \n","        file_path=os.path.join(directory_path_spectrograms_inference_pcen, file_name)\n","    torch.save(audio_support_and_query_set,file_path)\n","    print(\"Creazione dati conclusa\")\n","    return audio_support_and_query_set                   \n","\n","\n"]},{"cell_type":"markdown","id":"e0e1d1b1","metadata":{},"source":["## Support and query set creation for inference audio based on sample rate 16KHz, therefore suitable for AST"]},{"cell_type":"code","execution_count":85,"id":"ef86b58c","metadata":{},"outputs":[],"source":["def create_support_and_query_set_inference_audio_spectrogramsAST(labels,audio_name, support_set_events,num_support_per_class=5,pcen=True,augment_negative_support=False):\n","    pcen=True\n","  \n","\n","    if not os.path.exists(directory_path_spectrograms_inference_AST):\n","        os.makedirs(directory_path_spectrograms_inference_AST)\n","\n","   \n","    print(f\" Inizio creazione support e query set spectrograms per l'audio: {audio_name}\")\n","\n","    path=os.path.join(val_dir,audio_name)\n","    audio_raw,sr = librosa.load(path,sr=16000)\n","    events=support_set_events[:num_support_per_class]\n","    end_support=0\n","    start_negative_support_set=0\n","    support_spectrograms=[]\n","    negative_support_spectrograms=[]\n","\n","    \n","    mean_duration=0\n","    for evento in events:\n","        start=int(evento['start']*sr)\n","        end=int(evento['end']*sr)\n","        if mean_duration==0: mean_duration=end-start\n","        else: mean_duration+=end-start\n","    mean_duration=mean_duration/5\n","    \n","    for evento in events:\n","        start=int(evento['start']*sr)\n","        end=int(evento['end']*sr)\n","        \n","        #creation support set spectrograms for \"unknown\" class.\n","        audio_event=audio_raw[start_negative_support_set:start]\n","        \n","        \"\"\"\n","        negative_frames=get_frames(audio_event,220,220)\n","        for frame in negative_frames:\n","        \"\"\"\n","\n","        if augment_negative_support==True:\n","            audios_augmented=augment_audio(audio_event,sr)\n","            for audio_augmented in audios_augmented:\n","\n","                negative_audio_augmented=get_frames(audio_augmented,int(mean_duration),int(mean_duration))\n","                for negative_portion in negative_audio_augmented:\n","                    spectrogram_frames=librosa.feature.melspectrogram(y=negative_portion,n_fft=220,hop_length=110,n_mels=128)\n","                    if pcen==True:\n","                        spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))  \n","                    for  i in range(0,spectrogram_frames.shape[1],2):\n","                        end_frame = i + 2\n","                        if end_frame <= spectrogram_frames.shape[1]:\n","                            spectrogram_frame = spectrogram_frames[:, i:end_frame] \n","                            spectrogram_frame=torch.tensor(spectrogram_frame)     \n","                            negative_support_spectrograms.append((spectrogram_frame,0))\n","                            spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                            spectrogram_augmented=torch.tensor(spectrogram_augmented)\n","                            negative_support_spectrograms.append((spectrogram_augmented,0))\n","        else: \n","            negative_frames=get_frames(audio_event,int(mean_duration),int(mean_duration))\n","            for negative_frame in  negative_frames:\n","                spectrogram_frames=librosa.feature.melspectrogram(y=negative_frame,n_fft=220,hop_length=110,n_mels=128)\n","                if pcen==True:\n","                    spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))  #PCEN\n","        \n","                for i in range(0,spectrogram_frames.shape[1],2):\n","                    end_frame = i + 2\n","                    if end_frame <= spectrogram_frames.shape[1]:\n","                        spectrogram_frame = spectrogram_frames[:, i:end_frame]\n","                        spectrogram_frame=torch.tensor(spectrogram_frame)\n","                        negative_support_spectrograms.append((spectrogram_frame,0))\n","                \n","\n","\n","        start_negative_support_set=end+1\n","        if(end>end_support):\n","            end_support=end\n","\n","        #audio_portion containing the event for the support set\n","        audio_event=audio_raw[start:end]\n","\n","         #without overlapping, hop_length=frame_lenght\n","        spectrograms=[]\n","        \n","        spectrogram_frames=librosa.feature.melspectrogram(y=audio_event,n_fft=220,hop_length=110,n_mels=128)\n","        if pcen==True:\n","            spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))  \n","        for  i in range(0,spectrogram_frames.shape[1],2):\n","\n","            end_frame = i + 2\n","            if end_frame <= spectrogram_frames.shape[1]:\n","                spectrogram_frame = spectrogram_frames[:, i:end_frame]\n","                spectrogram_frame=torch.tensor(spectrogram_frame)\n","                spectrograms.append((spectrogram_frame,1))\n","                \n","                spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                spectrograms.append((spectrogram_augmented,1))\n","\n","\n","\n","        #data augmentation section for raw audios, the objective is increasing nearly 20x fold the support dimension\n","        audios_augmented=augment_audio(audio_event,sr)\n","        for audio_augmented in audios_augmented:\n","            spectrogram_frames=librosa.feature.melspectrogram(y=audio_augmented,n_fft=220,hop_length=110,n_mels=128)\n","            if pcen==True:\n","                spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))  \n","            for  i in range(0,spectrogram_frames.shape[1],2):\n","                end_frame = i + 2\n","                if end_frame <= spectrogram_frames.shape[1]:\n","                    spectrogram_frame = spectrogram_frames[:, i:end_frame] \n","                    spectrogram_frame=torch.tensor(spectrogram_frame)     \n","                    spectrograms.append((spectrogram_frame,1))\n","                    spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                    spectrogram_augmented=torch.tensor(spectrogram_augmented)\n","                    spectrograms.append((spectrogram_augmented,1))\n","\n","\n","\n","        support_spectrograms.extend(spectrograms)\n","    #all negative spectrogram if this code is commented, otherwise balanced support set.\n","\n","    \"\"\"\n","    negative_samples_to_keep=len(support_spectrograms)\n","    print(negative_samples_to_keep)\n","    print(len(negative_support_spectrograms))\n","    if(len(negative_support_spectrograms)>=negative_samples_to_keep):\n","        negative_support_spectrograms=random.sample(negative_support_spectrograms,negative_samples_to_keep)\n","    \"\"\"\n","\n","    \n","    support_spectrograms.extend((negative_support_spectrograms))\n","\n","    query_spectrograms=[]\n","    audio_query=audio_raw[end_support+220:]\n","\n","    j=len(labels)\n","    k=int((end_support/16000)/0.01)\n","    x=0\n","    frames=get_frames(audio_query,int(mean_duration),int(mean_duration))\n","\n","    for frame in frames:\n","        spectrogram_frames=librosa.feature.melspectrogram(y=frame,n_fft=220,hop_length=110,n_mels=128) #labels must go from labels(k) to labels(k+)\n","        if pcen==True:\n","            spectrogram_frames = librosa.pcen(spectrogram_frames * (2**31))  \n","        for i in range(0,spectrogram_frames.shape[1],2):\n","            end_frame = i + 2\n","            if end_frame <= spectrogram_frames.shape[1]:\n","                spectrogram_frame = spectrogram_frames[:, i:end_frame] \n","                spectrogram_frame=torch.tensor(spectrogram_frame)    \n","                x+=1\n","                if(x%2)==0:\n","                    query_spectrograms.append((spectrogram_frame,labels[k]))\n","                    if(k+1>j):break\n","                    k+=1\n","                else: \n","                    if labels[k]!=labels[k+1]: \n","                        query_spectrograms.append((spectrogram_frame,1)) #there is a frame 50% 0 and 50% 1, must be choosen if put 0 or 1.\n","                    else: \n","                        query_spectrograms.append((spectrogram_frame,labels[k]))\n","\n","   \n","\n","\n","    audio_support_and_query_set=(support_spectrograms,query_spectrograms)\n","    file_name=f'inference_audio{audio_name}.pth'\n","\n","   \n","    file_path=os.path.join(directory_path_spectrograms_inference_AST, file_name)\n","    torch.save(audio_support_and_query_set,file_path)\n","    print(\"Creazione dati conclusa\")\n","    return audio_support_and_query_set                   \n","\n"]},{"cell_type":"markdown","id":"5e7820d9","metadata":{},"source":["## Divergent spectrogram creation."]},{"cell_type":"code","execution_count":86,"id":"a759527d","metadata":{},"outputs":[],"source":["def create_support_and_query_set_inference_audio_spectrograms_not_uniform(labels,audio_name, support_set_events,num_support_per_class=5):\n","            \n","    if not os.path.exists(directory_path_spectrograms_inference):\n","        os.makedirs(directory_path_spectrograms_inference)\n","\n","   \n","    print(f\" Inizio creazione support e query set spectrograms per l'audio: {audio_name}\")\n","\n","    path=os.path.join(val_dir,audio_name)\n","    audio_raw,sr = librosa.load(path)\n","    events=support_set_events[:num_support_per_class]\n","    end_support=0\n","    start_negative_support_set=0\n","    support_spectrograms=[]\n","    negative_support_spectrograms=[]\n","\n","\n","    mean_duration=0\n","    for evento in events:\n","        start=int(evento['start']*sr)\n","        end=int(evento['end']*sr)\n","        if mean_duration==0: mean_duration=end-start\n","        else: mean_duration+=end-start\n","    mean_duration=mean_duration/5\n","\n","    for evento in events:\n","        start=int(evento['start']*sr)\n","        end=int(evento['end']*sr)\n","        \n","        #creation support set spectrograms for \"unknown\" class.\n","        audio_event=audio_raw[start_negative_support_set:start]\n","        \n","        \"\"\"\n","        negative_frames=get_frames(audio_event,220,220)\n","        for frame in negative_frames:\n","        \"\"\"\n","        negative_frames=get_frames(audio_event,sr*mean_duration,sr*mean_duration)\n","\n","        for negative_frame in  negative_frames:\n","            spectrogram_frames=librosa.feature.melspectrogram(y=negative_frame,n_fft=220,hop_length=110,n_mels=128)\n","        #spectrogram_frames=torch.tensor(spectrogram_frames)\n","            for i in range(0,spectrogram_frames.shape[1],2):\n","                end_frame = i + 2\n","                if end_frame <= spectrogram_frames.shape[1]:\n","                    spectrogram_frame = spectrogram_frames[:, i:end_frame]\n","                    negative_support_spectrograms.append((spectrogram_frame,0))\n","            \n","\n","\n","        start_negative_support_set=end+1\n","        if(end>end_support):\n","            end_support=end\n","\n","        #audio_portion containing the event for the support set\n","        audio_event=audio_raw[start:end]\n","\n","         #without overlapping, hop_length=frame_lenght\n","        spectrograms=[]\n","        \n","        spectrogram_frames=librosa.feature.melspectrogram(y=audio_event,n_fft=220,hop_length=110,n_mels=128)\n","        for  i in range(0,spectrogram_frames.shape[1],2):\n","\n","            end_frame = i + 2\n","            if end_frame <= spectrogram_frames.shape[1]:\n","                spectrogram_frame = spectrogram_frames[:, i:end_frame]\n","                \n","                spectrograms.append((spectrogram_frame,1))\n","                spectrogram_frame=torch.tensor(spectrogram_frame)\n","                spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                spectrograms.append((spectrogram_augmented,1))\n","\n","\n","\n","        #data augmentation section for raw audios, the objective is increasing nearly 20x fold the support dimension\n","        audios_augmented=augment_audio(audio_event,sr)\n","        for audio_augmented in audios_augmented:\n","            \n","           \n","            spectrogram_frames=librosa.feature.melspectrogram(y=audio_augmented,n_fft=220,hop_length=110,n_mels=128)\n","            for  i in range(0,spectrogram_frames.shape[1],2):\n","\n","                end_frame = i + 2\n","                if end_frame <= spectrogram_frames.shape[1]:\n","                    spectrogram_frame = spectrogram_frames[:, i:end_frame]      \n","                    spectrograms.append((spectrogram_frame,1))\n","                    spectrogram_augmented=augment_spectrogram(spectrogram_frame)\n","                    spectrograms.append((spectrogram_augmented,1))\n","\n","\n","\n","        support_spectrograms.extend(spectrograms)\n","    \"\"\"\n","    negative_samples_to_keep=len(support_spectrograms)\n","    print(negative_samples_to_keep)\n","    print(len(negative_support_spectrograms))\n","    if(len(negative_support_spectrograms)>=negative_samples_to_keep):\n","        negative_support_spectrograms=random.sample(negative_support_spectrograms,negative_samples_to_keep)\n","    \"\"\"\n","\n","\n","\n","    \n","    negative_samples_to_keep=len(support_spectrograms)\n","    print(negative_samples_to_keep)\n","    print(len(negative_support_spectrograms))\n","    if(len(negative_support_spectrograms)>=negative_samples_to_keep):\n","        negative_support_spectrograms=random.sample(negative_support_spectrograms,negative_samples_to_keep)\n","\n","\n","\n","    support_spectrograms.extend((negative_support_spectrograms))\n","\n","    audio_query=audio_raw[end_support+220:]\n","\n","    frames=get_frames(audio_query,220,220)\n","    query_spectrograms=[]\n","    k=0\n","    print(len(frames))\n","    print(len(labels)-(end_support/22050)/0.01)\n","\n","    for frame_id in range(int((end_support/22050)/0.01),len(labels)):\n","        spectrogram_frame=librosa.feature.melspectrogram(y=frames[k],n_fft=220,hop_length=220,n_mels=128)\n","        query_spectrograms.append((spectrogram_frame,labels[frame_id]))\n","        k=k+1\n","\n","    audio_support_and_query_set=(support_spectrograms,query_spectrograms)\n","    file_name=f'inference_audio{audio_name}.pth'\n","    file_path=os.path.join(directory_path_spectrograms_inference, file_name)\n","    torch.save(audio_support_and_query_set,file_path)\n","    print(\"Creazione dati conclusa\")\n","    return audio_support_and_query_set     "]},{"cell_type":"markdown","id":"dcbbd38a","metadata":{},"source":["## Support and query set creation in Raw Audio format"]},{"cell_type":"code","execution_count":87,"id":"3c0a3968","metadata":{},"outputs":[],"source":["def create_support_and_query_set_inference_audio_raw(labels,audio_name, support_set_events,num_support_per_class=5):\n","\n","    if not os.path.exists(directory_path_raw_audio_inference):\n","        os.makedirs(directory_path_raw_audio_inference)\n","        \n","\n","    print(f\" Inizio creazione support e query set spectrograms per l'audio: {audio_name}\")\n","\n","    path=os.path.join(val_dir,audio_name)\n","    audio_raw,sr = librosa.load(path,sr=16000)\n","    events=support_set_events[:num_support_per_class]\n","    end_support=0\n","    start_negative_support_set=0\n","    support_raw_audios=[]\n","    negative_support_raw_audios=[]\n","    for evento in events:\n","        start=int(evento['start']*sr)\n","        end=int(evento['end']*sr)\n","\n","        #creation support set spectrograms for \"unknown\" class.\n","        audio_event=audio_raw[start_negative_support_set:start]\n","        negative_frames=get_frames(audio_event,160,160)\n","        for frame in negative_frames:\n","           negative_support_raw_audios.append((frame,0))\n","\n","\n","\n","        start_negative_support_set=end+1\n","        if(end>end_support):\n","            end_support=end\n","\n","        #audio_portion containing the event for the support set\n","        audio_event=audio_raw[start:end]\n","\n","        frames=get_frames(audio_event,160,160) #without overlapping, hop_length=frame_lenght\n","        raw_audios=[]\n","        for frame in frames:\n","            raw_audios.append((frame,1))\n","\n","\n","\n","        #data augmentation section for raw audios, the objective is increasing nearly 20x fold the support dimension\n","        audios_augmented=augment_audio(audio_event,sr)\n","        for audio_augmented in audios_augmented:\n","            frames=get_frames(audio_augmented,160,160)\n","            for frame in frames:\n","                raw_audios.append((frame,1))\n","\n","\n","\n","        support_raw_audios.extend(raw_audios)\n","\n","    \n","\n","    support_raw_audios.extend((negative_support_raw_audios))\n","\n","    audio_query=audio_raw[end_support+160:]\n","\n","    frames=get_frames(audio_query,160,160)\n","    raw_audio_query=[]\n","    k=0\n","   \n","\n","    for frame_id in range(int((end_support/16000)/0.01),len(frames)+int((end_support/16000)/0.01)):\n","        frame=frames[k]\n","        raw_audio_query.append((frame,labels[frame_id]))\n","        k=k+1\n","\n","    audio_support_and_query_set=(support_raw_audios,raw_audio_query)\n","    file_name=f'inference_audio{audio_name}.pth'\n","    file_path=os.path.join(directory_path_raw_audio_inference, file_name)\n","    torch.save(audio_support_and_query_set,file_path)\n","    return audio_support_and_query_set   "]},{"cell_type":"markdown","id":"85dde127","metadata":{},"source":["## Metric section"]},{"cell_type":"code","execution_count":88,"id":"0d38fb74","metadata":{},"outputs":[],"source":["import numpy as np\n","import mir_eval\n","import scipy\n","\n","\n","def fast_intersect(ref, est):\n","    \"\"\"Find all intersections between reference events and estimated events (fast).\n","    Best-case complexity: O(N log N + M log M) where N=length(ref) and M=length(est)\n","\n","    Parameters\n","    ----------\n","    ref: np.ndarray [shape=(2, n)], real-valued\n","         Array of reference events. Each column is an event.\n","         The first row denotes onset times and the second row denotes offset times.\n","\n","    est: np.ndarray [shape=(2, m)], real-valued\n","         Array of estimated events. Each column is an event.\n","         The first row denotes onset times and the second row denotes offset times.\n","\n","\n","    Returns\n","    -------\n","    matches: list of sets, length n, integer-valued\n","         Property: matches[i] contains the set of all indices j such that\n","            (ref[0, i]<=est[1, j]) AND (ref[1, i]>=est[0, j])\n","    \"\"\"\n","    ref_on_argsort = np.argsort(ref[0, :])\n","    ref_off_argsort = np.argsort(ref[1, :])\n","\n","    est_on_argsort = np.argsort(est[0, :])\n","    est_off_argsort = np.argsort(est[1, :])\n","\n","    est_on_maxindex = est.shape[1]\n","    est_off_minindex = 0\n","    estref_matches = [set()] * ref.shape[1]\n","    refest_matches = [set()] * ref.shape[1]\n","    for ref_id in range(ref.shape[1]):\n","        ref_onset = ref[0, ref_on_argsort[ref_id]]\n","        est_off_sorted = est[1, est_off_argsort[est_off_minindex:]]\n","        search_result = np.searchsorted(est_off_sorted, ref_onset, side=\"left\")\n","        est_off_minindex += search_result\n","        refest_match = est_off_argsort[est_off_minindex:]\n","        refest_matches[ref_on_argsort[ref_id]] = set(refest_match)\n","\n","        ref_offset = ref[1, ref_off_argsort[-1 - ref_id]]\n","        est_on_sorted = est[0, est_on_argsort[: (1 + est_on_maxindex)]]\n","        search_result = np.searchsorted(est_on_sorted, ref_offset, side=\"right\")\n","        est_on_maxindex = search_result - 1\n","        estref_match = est_on_argsort[: (1 + est_on_maxindex)]\n","        estref_matches[ref_off_argsort[-1 - ref_id]] = set(estref_match)\n","\n","    zip_iterator = zip(refest_matches, estref_matches)\n","    matches = [x.intersection(y) for (x, y) in zip_iterator]\n","    return matches\n","\n","\n","def iou(ref, est, method=\"fast\"):\n","    \"\"\"Compute pairwise \"intersection over union\" (IOU) metric between reference\n","    events and estimated events.\n","\n","    Let us denote by a_i and b_i the onset and offset of reference event i.\n","    Let us denote by u_j and v_j the onset and offset of estimated event j.\n","\n","    The IOU between events i and j is defined as\n","        (min(b_i, v_j)-max(a_i, u_j)) / (max(b_i, v_j)-min(a_i, u_j))\n","    if the events are non-disjoint, and equal to zero otherwise.\n","\n","    Parameters\n","    ----------\n","    ref: np.ndarray [shape=(2, n)], real-valued\n","         Array of reference events. Each column is an event.\n","         The first row denotes onset times and the second row denotes offset times.\n","\n","    est: np.ndarray [shape=(2, m)], real-valued\n","         Array of estimated events. Each column is an event.\n","         The first row denotes onset times and the second row denotes offset times.\n","\n","    method: str, optional.\n","         If \"fast\" (default), computes pairwise intersections via a custom\n","         dynamic programming algorithm, see fast_intersect.\n","         If \"slow\", computes pairwise intersections via bruteforce quadratic\n","         search, see slow_intersect.\n","\n","    Returns\n","    -------\n","    S: scipy.sparse.dok.dok_matrix, real-valued\n","        Sparse 2-D matrix. S[i,j] contains the IOU between ref[i] and est[j]\n","        if these events are non-disjoint and zero otherwise.\n","    \"\"\"\n","    n_refs = ref.shape[1]\n","    n_ests = est.shape[1]\n","    S = scipy.sparse.dok_matrix((n_refs, n_ests))\n","\n","    if method == \"fast\":\n","        matches = fast_intersect(ref, est)\n","    elif method == \"slow\":\n","        matches = slow_intersect(ref, est)\n","\n","    for ref_id in range(n_refs):\n","        matching_ests = matches[ref_id]\n","        ref_on = ref[0, ref_id]\n","        ref_off = ref[1, ref_id]\n","\n","        for matching_est_id in matching_ests:\n","            est_on = est[0, matching_est_id]\n","            est_off = est[1, matching_est_id]\n","            intersection = min(ref_off, est_off) - max(ref_on, est_on)\n","            union = max(ref_off, est_off) - min(ref_on, est_on)\n","            intersection_over_union = intersection / union\n","            S[ref_id, matching_est_id] = intersection_over_union\n","\n","    return S\n","\n","\n","def match_events(ref, est, min_iou=0.0, method=\"fast\"):\n","    \"\"\"\n","    Compute a maximum matching between reference and estimated event times,\n","    subject to a criterion of minimum intersection-over-union (IOU).\n","\n","    Given two lists of events ``ref`` (reference) and ``est`` (estimated),\n","    we seek the largest set of correspondences ``(ref[i], est[j])`` such that\n","        ``iou(ref[i], est[j]) <= min_iou``\n","    and such that each ``ref[i]`` and ``est[j]`` is matched at most once.\n","\n","    This function is strongly inspired by mir_eval.onset.util.match_events.\n","    It relies on mir_eval's implementation of the Hopcroft-Karp algorithm from\n","    maximum bipartite graph matching. However, one important difference is that\n","    mir_eval's distance function relies purely on onset times, whereas this function\n","    considers both onset times and offset times to compute the IOU metric between\n","    reference events and estimated events.\n","\n","    Parameters\n","    ----------\n","    ref: np.ndarray [shape=(2, n)], real-valued\n","         Array of reference events. Each column is an event.\n","         The first row denotes onset times and the second row denotes offset times.\n","\n","    est: np.ndarray [shape=(2, m)], real-valued\n","         Array of estimated events. Each column is an event.\n","         The first row denotes onset times and the second row denotes offset times.\n","\n","    min_iou: real number in [0, 1). Default: 0.\n","         Threshold for minimum amount of intersection over union (IOU) to match\n","         any two events. See the iou method for implementation details.\n","\n","    method: str, optional.\n","         If \"fast\" (default), computes pairwise intersections via a custom\n","         dynamic programming algorithm, see fast_intersect.\n","         If \"slow\", computes pairwise intersections via bruteforce quadratic\n","         search, see slow_intersect.\n","\n","    Returns\n","    -------\n","    matching : list of tuples\n","        Every tuple corresponds to a match between one reference event and\n","        one estimated event.\n","            ``matching[i] == (i, j)`` where ``ref[i]`` matches ``est[j]``.\n","        Note that all values i and j appear at most once in the list.\n","    \"\"\"\n","\n","    # Intersect reference events and estimated events\n","    S = iou(ref, est, method=method)\n","\n","    # Threshold intersection-over-union (IOU) ratio\n","    S_bool = scipy.sparse.dok_matrix(S > min_iou)\n","    hits = S_bool.keys()\n","\n","    # Construct the bipartite graph\n","    G = {}\n","    for ref_i, est_i in hits:\n","        if est_i not in G:\n","            G[est_i] = []\n","        G[est_i].append(ref_i)\n","\n","    # Apply Hopcroft-Karp algorithm (from mir_eval package)\n","    # to obtain maximum bipartite graph matching\n","    matching = sorted(mir_eval.util._bipartite_match(G).items())\n","    return matching\n","\n","\n","def slow_intersect(ref, est):\n","    \"\"\"Find all intersections between reference events and estimated events (slow).\n","    Best-case complexity: O(N*M) where N=ref.shape[1] and M=est.shape[1]\n","\n","    Parameters\n","    ----------\n","    ref: np.ndarray [shape=(2, n)], real-valued\n","         Array of reference events. Each column is an event.\n","         The first row denotes onset times and the second row denotes offset times.\n","\n","    est: np.ndarray [shape=(2, m)], real-valued\n","         Array of estimated events. Each column is an event.\n","         The first row denotes onset times and the second row denotes offset times.\n","\n","\n","    Returns\n","    -------\n","    matches: list of sets, length n, integer-valued\n","         Property: matches[i] contains the set of all indices j such that\n","            (ref[0, i]<=est[1, j]) AND (ref[1, i]>=est[0, j])\n","    \"\"\"\n","    matches = []\n","    for i in range(ref.shape[1]):\n","        matches.append(\n","            set(\n","                [\n","                    j\n","                    for j in range(est.shape[1])\n","                    if ((ref[0, i] <= est[1, j]) and (ref[1, i] >= est[0, j]))\n","                ]\n","            )\n","        )\n","    return matches"]},{"cell_type":"code","execution_count":89,"id":"c4c22da0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [-pred_file PRED_FILE]\n","                             [-ref_files_path REF_FILES_PATH]\n","                             [-team_name TEAM_NAME] [-dataset DATASET]\n","                             [-savepath SAVEPATH]\n","ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\aldob\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3765acdaeec305910eb5ab3da3e48fe2de235787c.json\n"]},{"ename":"SystemExit","evalue":"2","output_type":"error","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}],"source":["\n","\n","MIN_EVAL_VALUE = 0.00001\n","N_SHOTS = 5\n","MIN_IOU_TH = 0.3\n","PRED_FILE_HEADER = [\"Audiofilename\",\"Starttime\",\"Endtime\"]\n","POS_VALUE = 'POS'\n","UNK_VALUE = 'UNK'\n","\n","def remove_shots_from_ref(ref_df, number_shots=5):\n","    \n","    ref_pos_indexes = select_events_with_value(ref_df, value=POS_VALUE)\n","    ref_n_shot_index = ref_pos_indexes[number_shots-1]\n","    # remove all events (pos and UNK) that happen before this 5th event\n","    events_to_drop = ref_df.index[ref_df['Endtime'] <= ref_df.iloc[ref_n_shot_index]['Endtime']].tolist()\n","\n","    return ref_df.drop(events_to_drop)\n","\n","def select_events_with_value(data_frame, value=POS_VALUE):\n","\n","    indexes_list = data_frame.index[data_frame[\"Q\"] == value].tolist()\n","\n","    return indexes_list\n","\n","def build_matrix_from_selected_rows(data_frame, selected_indexes_list ):\n","\n","    matrix_data = np.ones((2, len(selected_indexes_list)))* -1\n","    for n, idx in enumerate(selected_indexes_list):\n","        matrix_data[0, n] = data_frame.loc[idx].Starttime # start time for event n\n","        matrix_data[1, n] = data_frame.loc[idx].Endtime\n","    return matrix_data\n","\n","\n","def compute_tp_fp_fn(pred_events_df, ref_events_df):\n","    # inputs: dataframe with predicted events, dataframe with reference events and their value (POS, UNK, NEG)\n","    # output: True positives, False Positives, False negatives counts and total number of pos events in ref.\n","\n","    # makes one pass with bipartite graph matching between pred events and ref positive events\n","    # get TP\n","    # make second pass with remaining pred events and ref Unk events\n","    # compute FP as the number of remaining predicted events after the two rounds of matches.\n","    # FN is the remaining unmatched pos events in ref.\n","\n","    ref_pos_indexes = select_events_with_value(ref_events_df, value=POS_VALUE)\n","\n","    if \"Q\" not in pred_events_df.columns:\n","        pred_events_df[\"Q\"] = POS_VALUE\n","\n","    #sort events by starttime\n","    pred_events_df = pred_events_df.sort_values(by='Starttime', axis=0, ascending=True)\n","    pred_pos_indexes = select_events_with_value(pred_events_df, value=POS_VALUE)\n","\n","    ref_1st_round = build_matrix_from_selected_rows(ref_events_df, ref_pos_indexes)\n","    pred_1st_round = build_matrix_from_selected_rows(pred_events_df, pred_pos_indexes)\n","\n","    m_pos = metrics.match_events(ref_1st_round, pred_1st_round, min_iou=MIN_IOU_TH)\n","    matched_ref_indexes = [ri for ri, pi in m_pos] \n","    matched_pred_indexes = [pi for ri, pi in m_pos]\n","\n","\n","    ref_unk_indexes = select_events_with_value(ref_events_df, value=UNK_VALUE)\n","    ref_2nd_round = build_matrix_from_selected_rows(ref_events_df, ref_unk_indexes)\n","\n","    unmatched_pred_events = list(set(range(pred_1st_round.shape[1])) - set(matched_pred_indexes))\n","    pred_2nd_round = pred_1st_round[:, unmatched_pred_events]\n","\n","    m_unk = metrics.match_events(ref_2nd_round, pred_2nd_round, min_iou=MIN_IOU_TH)\n","\n","    # print(\"# Positive matches between Ref and Pred :\", len(m_pos))\n","    # print(\"# matches with Unknown events: \", len(m_unk))\n","    \n","    tp = len(m_pos)\n","    fp = pred_1st_round.shape[1] - tp - len(m_unk)\n","    \n","    ## compute unmatched pos ref events:\n","    count_unmached_pos_ref_events = len(ref_pos_indexes) - tp\n","\n","    fn = count_unmached_pos_ref_events\n","\n","    total_n_POS_events = len(ref_pos_indexes)\n","    return tp, fp, fn, total_n_POS_events\n","\n","def compute_scores_per_class(counts_per_class):\n","\n","    scores_per_class = {}\n","    for cl in counts_per_class.keys():\n","        tp = counts_per_class[cl][\"TP\"]\n","        fp = counts_per_class[cl][\"FP\"]\n","        fn = counts_per_class[cl][\"FN\"]\n","\n","            \n","        # to compute the harmonic mean we need to have all entries as non zero\n","        precision = tp/(tp+fp) if tp+fp != 0 else MIN_EVAL_VALUE  # case where no predictions were made \n","        if precision < MIN_EVAL_VALUE:\n","            precision = MIN_EVAL_VALUE\n","        recall = tp/(fn+tp) if tp != 0 else MIN_EVAL_VALUE\n","        fmeasure = tp/(tp+0.5*(fp+fn)) if tp != 0 else MIN_EVAL_VALUE\n","        \n","        scores_per_class[cl] = {\"precision\": precision, \"recall\": recall, \"f-measure\": fmeasure}\n","\n","    return scores_per_class\n","    \n","def compute_scores_from_counts(counts):\n","    tp = counts[\"TP\"]\n","    fp = counts[\"FP\"]\n","    fn = counts[\"FN\"]\n","\n","    # to compute the harmonic mean we need to have all entries as non zero\n","    precision = tp/(tp+fp) if tp+fp != 0 else MIN_EVAL_VALUE  # case where no predictions were made \n","    if precision < MIN_EVAL_VALUE:\n","        precision = MIN_EVAL_VALUE \n","    recall = tp/(fn+tp) if tp != 0 else MIN_EVAL_VALUE\n","    fmeasure = tp/(tp+0.5*(fp+fn)) if tp != 0 else MIN_EVAL_VALUE\n","\n","    scores = {\"precision\": precision, \"recall\": recall, \"f-measure\": fmeasure}\n","    \n","    return scores\n","\n","\n","def build_report(main_set_scores, scores_per_miniset, scores_per_audiofile, save_path, main_set_name=\"EVAL\", team_name=\"test_team\" , **kwargs):\n","    \n","\n","    # datetime object containing current date and time\n","    now = datetime.now()\n","    date_string = now.strftime(\"%d%m%Y_%H_%M_%S\")\n","    # print(\"date and time =\", date_string)\t\n","\n","    #make dict:\n","    report = {\n","            'team_name': team_name,\n","            \"set_name\": main_set_name,\n","            \"report_date\": date_string,\n","            \"overall_scores\": main_set_scores,\n","            \"scores_per_subset\": scores_per_miniset,\n","            \"scores_per_audiofile\": scores_per_audiofile\n","    }\n","    if \"scores_per_class\" in kwargs.keys():\n","        report[\"scores_per_class\"] = kwargs['scores_per_class']\n","\n","    with open(os.path.join(save_path,\"Evaluation_report_\" + team_name + \"_\" + main_set_name + '_' + date_string + '.json'), 'w') as outfile:\n","        json.dump(report, outfile)\n","\n","    return\n","\n","def build_mini_report_bootstrapped_results(low, high, meanFmeasure, low_precision,  mean_precision, high_precision, low_recall, \n","                                    mean_recall, high_recall, save_path, main_set_name=\"EVAL\", team_name=\"test_team\" ):\n","    \n","\n","    # datetime object containing current date and time\n","    now = datetime.now()\n","    date_string = now.strftime(\"%d%m%Y_%H_%M_%S\")\n","    # print(\"date and time =\", date_string)\t\n","\n","    #make dict:\n","    report = {\n","            'team_name': team_name,\n","            \"set_name\": main_set_name,\n","            \"report_date\": date_string,\n","            \"fmeasure\": {\"low\": low, \"mean\": meanFmeasure, \"high\":high},\n","            \"precision\":  {\"low\": low_precision, \"mean\": mean_precision, \"high\":high_precision},\n","            \"recall\":  {\"low\": low_recall, \"mean\": mean_recall, \"high\":high_recall}\n","    }\n","    \n","    with open(os.path.join(save_path,\"Evaluation_report_\" + team_name + \"_\" + main_set_name + '_' + date_string + '.json'), 'w') as outfile:\n","        json.dump(report, outfile)\n","\n","    return\n","\n","def evaluate_bootstrapped(pred_file_path, ref_file_path, team_name, dataset, savepath, bootstraps = 1000):\n","\n","    #computes overall scores with 95% confidence intervals\n","    #generates report.\n","\n","    print(\"\\nEvaluation for:\", team_name, dataset)\n","    #read Gt file structure: get subsets and paths for ref csvs make an inverted dictionary with audiofilenames as keys and folder as value\n","    gt_file_structure = {}\n","    gt_file_structure[dataset] = {}\n","    inv_gt_file_structure = {}\n","    list_of_subsets = os.listdir(ref_file_path)\n","    for subset in list_of_subsets:\n","        gt_file_structure[dataset][subset] = [os.path.basename(fl)[0:-4]+'.wav' for fl in glob.glob(os.path.join(ref_file_path,subset,\"*.csv\"))]\n","        for audiofile in gt_file_structure[dataset][subset]:\n","            inv_gt_file_structure[audiofile] = subset\n","\n","\n","    #read prediction csv\n","    pred_csv = pd.read_csv(pred_file_path, dtype=str)\n","    #verify headers:\n","    if list(pred_csv.columns) !=  PRED_FILE_HEADER:\n","        print('Please correct the header of the prediction file. This should be', PRED_FILE_HEADER)\n","        exit(1)\n","    \n","    overall_fmeasures_bootstrapped = []\n","    overall_precision_bootstrapped = []\n","    overall_recall_bootstrapped = []\n","\n","    for bi in tqdm(range(bootstraps)):\n","        #  parse prediction csv\n","\n","        #remove predictions for bootstrapping \n","        prediction_indexes_to_remove = np.random.choice(range(len(pred_csv)), round(0.05*len(pred_csv)), replace=False)\n","\n","        pred_csv_new = pred_csv.drop(prediction_indexes_to_remove)\n","        #  split remaining predictions into lists of events for the same audiofile.\n","        pred_events_by_audiofile = dict(tuple(pred_csv_new.groupby('Audiofilename')))\n","\n","        counts_per_audiofile = {}\n","        for audiofilename in list(pred_events_by_audiofile.keys()):\n","        \n","                \n","            # for each audiofile, load correcponding GT File (audiofilename.csv)\n","            ref_events_this_audiofile_all = pd.read_csv(os.path.join(ref_file_path, inv_gt_file_structure[audiofilename], audiofilename[0:-4]+'.csv'), dtype={'Starttime':np.float64, 'Endtime': np.float64})\n","            # sort events by starttime:\n","            ref_events_this_audiofile_all = ref_events_this_audiofile_all.sort_values(by='Starttime', axis=0, ascending=True)\n","            \n","            #Remove the 5 shots from GT:\n","            ref_events_this_audiofile = remove_shots_from_ref(ref_events_this_audiofile_all, number_shots=N_SHOTS)\n","            \n","            # compare and get counts: TP, FP .. \n","            tp_count, fp_count, fn_count , total_n_events_in_audiofile = compute_tp_fp_fn(pred_events_by_audiofile[audiofilename], ref_events_this_audiofile )\n","\n","            counts_per_audiofile[audiofilename] = {\"TP\": tp_count, \"FP\": fp_count, \"FN\": fn_count, \"total_n_pos_events\": total_n_events_in_audiofile}\n","            # print(audiofilename, counts_per_audiofile[audiofilename])\n","\n","        dataset_metadata = copy.deepcopy(gt_file_structure)\n","\n","        # include audiofiles for which there were no predictions:\n","        list_all_audiofiles = []\n","        for miniset in dataset_metadata[dataset].keys():\n","            list_all_audiofiles.extend(dataset_metadata[dataset][miniset])\n","\n","        for audiofilename in list_all_audiofiles:\n","            if audiofilename not in counts_per_audiofile.keys():\n","                ref_events_this_audiofile = pd.read_csv(os.path.join(ref_file_path, inv_gt_file_structure[audiofilename], audiofilename[0:-4]+'.csv'), dtype=str)\n","                # sort ref_events by starttime\n","                ref_events_this_audiofile = ref_events_this_audiofile.sort_values(by='Starttime', axis=0, ascending=True)\n","                total_n_pos_events_in_audiofile =  len(select_events_with_value(ref_events_this_audiofile, value=POS_VALUE))\n","                counts_per_audiofile[audiofilename] = {\"TP\": 0, \"FP\": 0, \"FN\": total_n_pos_events_in_audiofile, \"total_n_pos_events\": total_n_pos_events_in_audiofile}\n","            \n","        # aggregate the counts per class or subset: \n","        list_sets_in_mainset = list(dataset_metadata[dataset].keys())\n","\n","        counts_per_class_per_set = {}\n","        scores_per_class_per_set = {}\n","        counts_per_set = {}\n","        scores_per_set = {}\n","        scores_per_audiofile = {}\n","        for data_set in list_sets_in_mainset:\n","            # print(data_set)\n","            \n","            list_audiofiles_in_set = dataset_metadata[dataset][data_set]\n","            tp = 0\n","            fn = 0\n","            fp = 0\n","            total_n_pos_events_this_set = 0\n","            for audiofile in  list_audiofiles_in_set:\n","\n","                scores_per_audiofile[audiofile] = compute_scores_from_counts(counts_per_audiofile[audiofile])\n","                tp = tp + counts_per_audiofile[audiofile][\"TP\"]\n","                fn = fn + counts_per_audiofile[audiofile][\"FN\"]\n","                fp = fp + counts_per_audiofile[audiofile][\"FP\"]\n","                total_n_pos_events_this_set = total_n_pos_events_this_set + counts_per_audiofile[audiofile][\"total_n_pos_events\"]\n","                counts_per_set[data_set] = {\"TP\": tp, \"FN\": fn, \"FP\": fp, \"total_n_pos_events_this_set\": total_n_pos_events_this_set}\n","            \n","                #  compute scores per subset\n","                scores_per_set[data_set] = compute_scores_from_counts(counts_per_set[data_set])\n","                        \n","        overall_scores = {\"precision\" : stats.hmean([scores_per_set[dt][\"precision\"] for dt in scores_per_set.keys()]), \n","                        \"recall\":  stats.hmean([scores_per_set[dt][\"recall\"] for dt in scores_per_set.keys()]) ,\n","                        \"fmeasure (percentage)\": np.round(stats.hmean([scores_per_set[dt][\"f-measure\"] for dt in scores_per_set.keys()])*100, 3)\n","                        }\n","        overall_fmeasures_bootstrapped.append(overall_scores[\"fmeasure (percentage)\"])\n","        overall_precision_bootstrapped.append(overall_scores[\"precision\"])\n","        overall_recall_bootstrapped.append(overall_scores[\"recall\"])\n","    \n","    overall_high_fmeasure = np.percentile(overall_fmeasures_bootstrapped,97.5)\n","    overall_low_fmeasure = np.percentile(overall_fmeasures_bootstrapped, 2.5)\n","    overall_mean_fmeasure = np.mean(overall_fmeasures_bootstrapped)\n","    overall_high_precision = np.percentile(overall_precision_bootstrapped,97.5)\n","    overall_low_precision = np.percentile(overall_precision_bootstrapped, 2.5)\n","    overall_mean_precision = np.mean(overall_precision_bootstrapped)\n","    overall_high_recall = np.percentile(overall_recall_bootstrapped,97.5)\n","    overall_low_recall = np.percentile(overall_recall_bootstrapped, 2.5)\n","    overall_mean_recall = np.mean(overall_recall_bootstrapped)\n","\n","    # print(overall_low_fmeasure, '<',overall_mean_fmeasure, '<', overall_high_fmeasure )\n","    # print('min:', np.min(overall_fmeasures_bootstrapped), '------ max:', np.max(overall_fmeasures_bootstrapped))\n","    build_mini_report_bootstrapped_results(overall_low_fmeasure, overall_high_fmeasure, overall_mean_fmeasure,\n","                                     overall_high_precision, overall_low_precision,  overall_mean_precision, overall_high_recall,\n","                                     overall_low_recall, overall_mean_recall, savepath, dataset, team_name)\n","       \n","    return\n","\n","\n","if __name__ == \"__main__\":\n","\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-pred_file', type=str, help='csv predictions file')\n","    parser.add_argument('-ref_files_path', type=str, help='path to the ground truth csvs folder')\n","    # parser.add_argument('-metadata', type=str, help=\"path for metadata json. Participants may ignore this option.\")\n","    parser.add_argument('-team_name', type=str, help='team identification')\n","    parser.add_argument('-dataset', type=str, help=\"which set to evaluate: EVAL or VAL\")\n","    parser.add_argument('-savepath', type=str, help=\"path where to save the report to\")\n","    args = parser.parse_args()\n","    \n","    \n","\n","    evaluate_bootstrapped(args.pred_file_path,args.ref_file_path, args.team_name, args.dataset,  args.savepath)"]},{"cell_type":"markdown","id":"2b68837f","metadata":{},"source":["## Utility function to improve localization"]},{"cell_type":"code","execution_count":123,"id":"013f5bc6","metadata":{},"outputs":[],"source":["def wrong_prediction_correction(predictions,mean_event_duration,mode=0):\n","    frame_mean_event_duration=int(mean_event_duration/0.01)\n","    if frame_mean_event_duration<4 or mode==\"Wav2Vec\" or mode==\"AST\":\n","        return predictions\n","\n","    high_percentage_error_percentage = math.ceil(frame_mean_event_duration / 5)\n","    high_percentage_error_percentage=4\n","   \n","    if frame_mean_event_duration<70:high_percentage_error_percentage=2\n","\n","\n","    for p in range(len(predictions)):\n","        if(p<high_percentage_error_percentage or p+high_percentage_error_percentage>len(predictions)):\n","            continue\n","        \n","        correction_needed= False\n","        for q in range (-high_percentage_error_percentage,high_percentage_error_percentage):\n","            if p + q < 0 or p + q >= len(predictions):\n","                continue\n","            if predictions[p] != predictions[p + q]:\n","                correction_needed = True\n","                break\n","\n","            \n","        if correction_needed==True:\n","            predictions[p]=1 -predictions[p]\n","            \n","    return predictions\n","\n"]},{"cell_type":"markdown","id":"7566ff69","metadata":{},"source":["## Event handling function"]},{"cell_type":"code","execution_count":124,"id":"3ef32f2b","metadata":{},"outputs":[],"source":["def event_finder(label_vector,mean_event,mode=0):\n","    print(int((mean_event)/0.01))\n","    y=int(mean_event/0.01)\n","\n","    \n","    if 1500<int(mean_event/0.01)<2000:min_event_length_frames=55\n","    elif int(mean_event/0.01)>2000:min_event_length_frames=65\n","    elif 1000<int(mean_event/0.01)<1500:min_event_length_frames=45\n","    elif int((mean_event)/0.01)<40:min_event_length_frames=int((mean_event)/0.01)*0.75\n","    elif int((mean_event)/0.01)<30:min_event_length_frames=25\n","    \n","    if mode==\"Wav2Vec\":\n","        if int(mean_event/0.01)>2000:min_event_length_frames=30\n","        #elif 1600<y<2000: min_event_length_frames=20  #20=0.333\n","        elif 1000<y<2000:min_event_length_frames=50\n","        else: min_event_length_frames=y*0.75\n","    if mode==\"AST\":\n","        min_event_length_frames=20\n","    \n","    print(f'min_event_frame:{min_event_length_frames}')\n","    events = []\n","    start = None\n","\n","    for number, label in enumerate(label_vector):\n","        if label == 1 and start is None:\n","            # Inizia un nuovo evento\n","            start = number\n","        elif label == 0 and start is not None:\n","            # Finisce l'evento corrente\n","            if number - start >= min_event_length_frames:\n","                events.append((start*0.01, (number-1)*0.01))\n","            start = None\n","            \n","    if start is not None and len(label_vector) - start >= min_event_length_frames:\n","        events.append((start * 0.01, (len(label_vector) - 1) * 0.01))\n","\n","\n","    return events\n"]},{"cell_type":"markdown","id":"35a22958","metadata":{},"source":["## Segmentation metrics"]},{"cell_type":"code","execution_count":92,"id":"1a33d586","metadata":{},"outputs":[],"source":["def iou(event1, event2):\n","    \"\"\"Calcola l'Intersection over Union (IoU) tra due eventi (start, end).\"\"\"\n","    start1, end1 = event1\n","    start2, end2 = event2\n","\n","    inter_start = max(start1, start2)\n","    inter_end = min(end1, end2)\n","    inter = max(0, inter_end - inter_start + 1)\n","\n","    union = max(end1, end2) - min(start1, start2) + 1\n","\n","    return inter / union"]},{"cell_type":"code","execution_count":93,"id":"4ef07b13","metadata":{},"outputs":[],"source":["def compute_tp_fp_fn(pred_events, true_events, min_iou_th=0.05):\n","    \"\"\"\n","    Calcola TP, FP, FN basato su IoU tra gli eventi predetti e quelli reali.\n","    \n","    Args:\n","    - pred_events (list of tuples): Lista di eventi predetti (start, end).\n","    - true_events (list of tuples): Lista di eventi reali (start, end).\n","    - min_iou_th (float): Soglia minima di IoU per considerare una predizione come vera positiva.\n","    \n","    Returns:\n","    - tp, fp, fn (int, int, int): True Positives, False Positives, False Negatives.\n","    \"\"\"\n","    tp = 0\n","    matched_pred_indexes = set()\n","\n","    for true_event in true_events:\n","        match_found = False\n","        for id, pred_event in enumerate(pred_events):\n","            if id in matched_pred_indexes:\n","                continue\n","\n","            # Calcolo dell'IoU tra evento reale e evento predetto\n","            if iou(true_event, pred_event) >= min_iou_th:\n","                tp += 1\n","                matched_pred_indexes.add(id)\n","                match_found = True\n","                break\n","    \n","    fp = len(pred_events) - tp\n","    fn = len(true_events) - tp\n","    print(tp,fp,fn)\n","    return tp, fp, fn"]},{"cell_type":"code","execution_count":94,"id":"7d5ec8c7","metadata":{},"outputs":[],"source":["def compute_precision_recall_f1(tp, fp, fn, min_eval_value=0.00001):\n","    \"\"\"\n","    Calcola precision, recall, e F1-score basato su TP, FP, FN.\n","    \n","    Args:\n","    - tp (int): Numero di True Positives.\n","    - fp (int): Numero di False Positives.\n","    - fn (int): Numero di False Negatives.\n","    - min_eval_value (float): Valore minimo per evitare divisioni per 0.\n","    \n","    Returns:\n","    - metrics (dict): Dizionario con precision, recall e F1-score.\n","    \"\"\"\n","    precision = tp / (tp + fp) if tp + fp != 0 else min_eval_value\n","    recall = tp / (tp + fn) if tp + fn != 0 else min_eval_value\n","    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else min_eval_value\n","\n","    return {\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1_score\n","    }"]},{"cell_type":"markdown","id":"3ba4ea9d","metadata":{},"source":["## Inference function"]},{"cell_type":"code","execution_count":104,"id":"eb94a7c3","metadata":{},"outputs":[],"source":["def inference_audio(model,audio_name,mode=\"classical_network\",model_checkpoint=0,batch_size=32,pcen=False,rolling_mean=False):\n","    model.eval()\n","\n","    pth_file=f'inference_audio{audio_name}.pth'\n","    print(i)\n","\n","    if mode==\"AST\": batch_size=512\n","    if mode==\"Wav2Vec\": batch_size=32\n","\n","    if(model_checkpoint!=0):\n","        model.load_state_dict(model_checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(model_checkpoint['optimizer_state_dict'])\n","\n","\n","    if( mode==\"Wav2Vec\"):\n","        file_path=os.path.join(directory_path_raw_audio_inference, pth_file)\n","    elif mode==\"AST\":\n","        file_path=os.path.join(directory_path_spectrograms_inference_AST, pth_file)\n","    else:\n","        if pcen==False:\n","            file_path=os.path.join(directory_path_spectrograms_inference, pth_file)\n","        else:\n","            file_path=os.path.join(directory_path_spectrograms_inference_pcen, pth_file)\n","\n","\n","    inference_audio=torch.load(file_path)\n","    query_predictions=[]\n","\n","\n","    support_samples = [x[0] for x in inference_audio[0]]\n","    support_labels = [x[1] for x in inference_audio[0]]\n","\n","    query_samples = [x[0] for x in inference_audio[1]]\n","    query_labels =[x[1] for x in inference_audio[1]]\n","\n","    \n","    if mode==\"AST\" or mode==\"Wav2Vec\":\n","        if mode==\"AST\":\n","            support_samples=torch.stack(support_samples).to(dtype=torch.float32).to(device)\n","            query_samples=torch.stack(query_samples).to(dtype=torch.float32).to(device)\n","            support_labels=torch.tensor(support_labels)\n","            print(support_samples.shape,support_labels.shape)\n","            mask_1 = support_labels == 1\n","            mask_0 = support_labels == 0    \n","\n","        \n","        \n","            support_samples = torch.cat((support_samples[mask_1], support_samples[mask_0]), dim=0)\n","            support_labels = torch.cat((support_labels[mask_1], support_labels[mask_0]), dim=0)\n","        elif mode==\"classical_network\":\n","           \n","            support_samples=torch.stack(support_samples)\n","            support_labels=torch.tensor(support_labels)\n","        else: \n","            support_samples = torch.tensor(support_samples,dtype=torch.float16).to(device)\n","            query_samples = torch.tensor(query_samples,dtype=torch.float16).to(device)\n","       \n","\n","\n","    \n","    del inference_audio\n","\n","    if(mode==\"classical_network\"):\n","        \n","        support_samples=torch.stack(support_samples).to(dtype=torch.float32).to(device)\n","        query_samples=torch.stack(query_samples).to(dtype=torch.float32).to(device)\n","        \n","        #support_samples = torch.tensor(support_samples,dtype=torch.float32).to(device)\n","        #query_samples = torch.tensor(query_samples,dtype=torch.float32).to(device)\n","    elif mode==\"Wav2Vec\":\n","        support_samples = torch.tensor(support_samples,dtype=torch.float16).to(device)\n","        query_samples = torch.tensor(query_samples,dtype=torch.float16).to(device)\n","\n","\n","\n","    if(mode==\"classical_network\" or mode==\"AST\"):\n","        support_samples=support_samples.view(-1, 1, 128, 2)\n","        query_samples=query_samples.view(-1, 1, 128, 2)\n","\n","\n","    if pcen==False or mode==\"Wav2Vec\":\n","        mean = support_samples.mean()\n","        std = support_samples.std()\n","        support_samples = (support_samples - mean) / std\n","        query_samples = (query_samples - mean) / std\n","\n","        min_val = support_samples.min()\n","        max_val = support_samples.max()\n","        support_samples = (support_samples - min_val) / (max_val - min_val)\n","        query_samples = (query_samples - min_val) / (max_val - min_val)\n","        \n","        \n","\n","    if mode==\"Wav2Vec\":processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","\n","    support_labels = torch.tensor(support_labels).to(device)\n","    query_labels=torch.tensor(query_labels).to(device)\n","\n","    \"\"\"\n","    print(f'Support Samples:{support_samples.shape}')\n","    print(f'Support Labels:{support_labels.shape}')\n","\n","    print(f'Query Samples:{query_samples.shape}')\n","    print(f'Query labels:{query_labels.shape}')\n","    \"\"\"\n","\n","\n","\n","\n","    support_dataset = TensorDataset(support_samples, support_labels)\n","    query_dataset = TensorDataset(query_samples,query_labels)\n","\n","    support_loader = DataLoader(support_dataset, batch_size=batch_size, shuffle=False)\n","    query_loader = DataLoader(query_dataset, batch_size=batch_size, shuffle=False)\n","\n","    query_samples_size=query_samples.shape[0]\n","\n","    del query_samples\n","    del support_samples\n","    del support_labels\n","\n","    aggregated_positive_prototype=None\n","    aggregated_negative_prototype=None\n","\n","    with torch.no_grad():\n","            if rolling_mean==False:\n","                negative_embeddings=[]\n","                positive_embeddings=[]\n","            \n","            for support_batch,support_labels_batch in support_loader:\n","\n","\n","                support_batch = support_batch.to(device)\n","                support_labels_batch=support_labels_batch.to(device)\n","                \n","                if rolling_mean==True:\n","                    negative_embeddings=[]\n","                    positive_embeddings=[]\n","               \n","                if mode==\"classical_network\":\n","                    with autocast():\n","                        for x in range(0,support_batch.shape[0]):\n","\n","                            frame=support_batch[x].unsqueeze(0)\n","                            support_embeddings, _, _= model(frame)\n","                            torch.cuda.empty_cache()\n","                            if (support_labels_batch[x]==0):\n","                                negative_embeddings.append(support_embeddings)\n","                            else:\n","                                positive_embeddings.append(support_embeddings)\n","                elif mode==\"AST\":\n","                    if support_batch.shape[0]<batch_size:\n","                        continue\n","                        \n","                    support_batch=support_batch.squeeze(dim=1)\n","                    support_batch = support_batch.permute(0, 2, 1)\n","                    support_batch = support_batch.reshape(-1, 1024)\n","                    support_batch = support_batch.unsqueeze(0)\n","                    with autocast():\n","                        most_frequent_label = np.bincount(support_labels_batch.cpu()).argmax()\n","                        batch_embedding,_,_=model(support_batch)\n","\n","                        if (most_frequent_label==0):\n","                            negative_embeddings.append(batch_embedding)\n","                        else:\n","                            positive_embeddings.append(batch_embedding)\n","                            #below the code for rolling mean\n","\n","                elif mode==\"Wav2Vec\":\n","                    if support_batch.shape[0]<batch_size:\n","                        break\n","                    support_batch = support_batch.view(-1)\n","                    #the output of processor is a dictionay, using .input_values we are taking the right output.\n","                    support_batch=processor(support_batch, return_tensors=\"pt\", sampling_rate=16000).input_values\n","                    support_batch=torch.tensor(support_batch,dtype=torch.half).to(device)\n","                    with autocast():\n","                        most_frequent_label = np.bincount(support_labels_batch.cpu()).argmax()\n","                        torch.cuda.empty_cache()\n","                        batch_embedding,_,_=model(support_batch)\n","                    \n","                        if (most_frequent_label==0):\n","                            negative_embeddings.append(batch_embedding)\n","                        else:\n","                            positive_embeddings.append(batch_embedding)\n","\n","\n","                if rolling_mean==True:\n","                    if len(positive_embeddings) > 0:\n","                        positive_embeddings=torch.stack(positive_embeddings)\n","                        prototype = positive_embeddings.mean(dim=0)\n","\n","                        if aggregated_positive_prototype is None:\n","                            aggregated_positive_prototype=prototype\n","                        else:\n","                            aggregated_positive_prototype = (torch.stack([aggregated_positive_prototype, prototype])).mean(dim=0)\n","\n","                    if len(negative_embeddings) > 0:\n","                        negative_embeddings=torch.stack(negative_embeddings)\n","                        prototype = negative_embeddings.mean(dim=0)\n","                        if aggregated_negative_prototype is None:\n","                            aggregated_negative_prototype=prototype\n","                        else:\n","                            aggregated_negative_prototype = (torch.stack([aggregated_negative_prototype, prototype])).mean(dim=0)\n","                    torch.cuda.empty_cache()\n","                    negative_embeddings=[]\n","                    positive_embeddings=[]\n","                \n","\n","\n","            if rolling_mean==False:\n","                if len(positive_embeddings) > 0:\n","\n","                    positive_embeddings=torch.stack(positive_embeddings)\n","                    aggregated_positive_prototype = positive_embeddings.mean(dim=0)\n","\n","                if len(negative_embeddings) > 0:\n","                    negative_embeddings=torch.stack(negative_embeddings)\n","                    aggregated_negative_prototype = negative_embeddings.mean(dim=0)\n","                \n","            \n","                torch.cuda.empty_cache()\n","                negative_embeddings=[]\n","                positive_embeddings=[]\n","            \n","\n","\n","\n","    #adding the negative and positive prototype for the entire episode.\n","    negative_prototype=aggregated_negative_prototype\n","    positive_prototype=aggregated_positive_prototype\n","\n","    \n","    progress_bar = tqdm(total=query_samples_size)\n","    \n","    with torch.no_grad():\n","        for query_batch,query_batch_labels in query_loader:\n","            query_batch=torch.tensor(query_batch)\n","            query_batch = query_batch.to(device)\n","            if mode==\"classical_network\":\n","                for y in range(0,query_batch.shape[0]):\n","                    with autocast():\n","                        frame=query_batch[y]\n","                        frame=frame.unsqueeze(0)\n","                        query_embeddings, binary_query_output, multiclass_query_output = model(frame)\n","                        distance_from_negative = F.pairwise_distance(query_embeddings, negative_prototype)\n","                        distance_from_positive=F.pairwise_distance(query_embeddings, positive_prototype)\n","                        if(distance_from_positive<distance_from_negative):\n","                            query_predictions.append(1)\n","                        else:\n","                            query_predictions.append(0)\n","                        progress_bar.update(1)\n","            \n","            elif(mode==\"Wav2Vec\"):\n","                if query_batch_labels.shape[0]<batch_size:\n","                    break\n","                most_frequent_label = np.bincount(query_batch_labels.cpu()).argmax()\n","                query_batch = query_batch.view(-1)\n","                query_batch=processor(query_batch, return_tensors=\"pt\", sampling_rate=16000).input_values\n","                query_batch=torch.tensor(query_batch,dtype=torch.half).to(device)\n","\n","                with autocast():\n","                    query_embeddings,_,multiclass_query_output = model(query_batch)\n","                distance_from_negative = F.pairwise_distance(query_embeddings,negative_prototype)\n","                distance_from_positive=F.pairwise_distance(query_embeddings, positive_prototype)\n","                for h in range (0,15):\n","                    if(distance_from_positive[0][h]<distance_from_negative[0][h]):\n","                            query_predictions.append(1)\n","                            query_predictions.append(1)\n","                    else:\n","                            query_predictions.append(0)\n","                            query_predictions.append(0)\n","                query_predictions.append(query_predictions[-1])\n","                query_predictions.append(query_predictions[-1])\n","            \n","                progress_bar.update(batch_size)\n","  \n","            elif mode==\"AST\":\n","                with autocast():\n","                    if query_batch.shape[0]<batch_size:\n","                        continue\n","                    query_batch=query_batch.squeeze(dim=1)\n","                    query_batch = query_batch.permute(0, 2, 1)\n","                    query_batch = query_batch.reshape(-1, 1024)\n","                    query_batch = query_batch.unsqueeze(0)\n","                    most_frequent_label = np.bincount(query_batch_labels.cpu()).argmax()\n","                    \n","                    query_embeddings,_,multiclass_query_output = model(query_batch)\n","                    most_frequent_label=torch.tensor(most_frequent_label)\n","                    distance_from_negative = F.pairwise_distance(query_embeddings,negative_prototype)\n","                    distance_from_positive=F.pairwise_distance(query_embeddings, positive_prototype)\n","                    if(distance_from_positive<distance_from_negative):\n","                        for _ in range(0,batch_size):\n","                            query_predictions.append(1)\n","                    else:\n","                        for _ in range(0,batch_size):\n","                            query_predictions.append(0)\n","\n","                    progress_bar.update(batch_size)\n","\n","\n","\n","    query_predictions_np = np.array(query_predictions)\n","    if len(query_predictions_np)<len(query_labels.cpu().detach().numpy()):\n","        query_labels=query_labels[:len(query_predictions_np)]\n","\n","    count_of_ones = np.sum(query_predictions_np)\n","    count_of_ones_2=np.sum(query_labels.cpu().detach().numpy())\n","    print(count_of_ones,count_of_ones_2)\n","    \n","    \n","\n","    \n","    if mode==\"AST\" or mode==\"Wav2Vec\":\n","        sr=16000\n","    else: \n","        sr=22050\n","    mean_duration=0\n","\n","    support_set=get_support_set_inference(audio)\n","    events=support_set[:5]\n","    for evento in events:\n","        start=evento['start']\n","        end=evento['end']\n","        if mean_duration==0: mean_duration=end-start\n","        else: mean_duration+=(end-start)\n","    mean_event_duration=mean_duration/5\n","    mean_event_duration=mean_duration*0.3\n","\n","    print(mean_event_duration)\n","\n","\n","    f1 = f1_score(query_labels.cpu().detach().numpy(), query_predictions_np, average='binary')\n","\n","    print(f'F1 score pre correzione: {f1}')\n","    \n","    query_predictions_np=wrong_prediction_correction(query_predictions_np,mean_event_duration,mode=mode)\n","\n","    count_of_ones = np.sum(query_predictions_np)\n","    print(count_of_ones,count_of_ones_2)\n","\n","    predicted_events=event_finder(query_predictions_np,mean_event_duration,mode=mode)\n","    real_events=event_finder(query_labels.cpu().detach().numpy(),mean_event_duration,mode=mode)\n","    print(predicted_events)\n","    print(real_events)\n","\n","    tp, fp, fn = compute_tp_fp_fn(predicted_events, real_events)\n","    metrics = compute_precision_recall_f1(tp, fp, fn)\n","\n","\n","    f1_segment_based=metrics['f1_score']\n","    precision_segment_based=metrics['precision']\n","    recall_segment_based=metrics['recall']\n","    \n","    \n","    f1 = f1_score(query_labels.cpu().detach().numpy(), query_predictions_np, average='binary')\n","    precision=precision_score(query_labels.cpu().detach().numpy(), query_predictions_np)\n","    recall=recall_score(query_labels.cpu().detach().numpy(), query_predictions_np)\n","    \n","\n","    print(f'F1 score event based: {f1_segment_based}')\n","    print(f'Precision event based: {precision_segment_based}')\n","    print(f'Recall event based: {recall_segment_based}')\n","\n","    print(f'F1 score: {f1}')\n","    print(f'Precision: {precision}')\n","    print(f'Recall: {recall}')\n","    return metrics,f1\n","\n","       \n"]},{"cell_type":"markdown","id":"042ba7dc","metadata":{},"source":["## Inference for an audio given the path to the file and the json file containing its events "]},{"cell_type":"code","execution_count":126,"id":"f7ae425e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","Doing inference for audio: ME1.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioME1.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([4350, 128, 2]) torch.Size([4350])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/47196 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n"," 98%|█████████▊| 46080/47196 [00:03<00:00, 13508.01it/s]C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","100%|█████████▉| 47104/47196 [00:03<00:00, 13195.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0 322\n","0.3650999999999996\n","F1 score pre correzione: 0.0\n","0 322\n","36\n","min_event_frame:10\n","36\n","min_event_frame:10\n","[]\n","[(9.46, 9.950000000000001), (12.32, 12.870000000000001), (31.18, 31.77), (32.58, 33.15), (35.88, 36.27), (38.160000000000004, 38.65)]\n","0 0 6\n","F1 score event based: 0.0\n","Precision event based: 1e-05\n","Recall event based: 0.0\n","F1 score: 0.0\n","Precision: 0.0\n","Recall: 0.0\n","Doing inference for audio: file_97_113.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audiofile_97_113.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([117846, 128, 2]) torch.Size([117846])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/67275 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n","100%|█████████▉| 67072/67275 [00:05<00:00, 12042.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["2048 27158\n","12.058792199999997\n","F1 score pre correzione: 0.010203382866534274\n","2048 27158\n","1205\n","min_event_frame:10\n","1205\n","min_event_frame:10\n","[(30.72, 35.83), (307.2, 312.31), (568.32, 573.4300000000001), (614.4, 619.51)]\n","[(6.0600000000000005, 31.27), (48.44, 68.35000000000001), (74.0, 112.77), (119.56, 134.75), (140.1, 147.15), (152.88, 161.79), (167.14000000000001, 183.87), (190.14000000000001, 197.51), (203.42000000000002, 259.77), (268.02, 306.59000000000003), (313.0, 316.31), (342.14, 343.77), (361.34000000000003, 363.27), (424.44, 424.81), (448.44, 450.79), (457.92, 463.11), (493.92, 494.57), (508.48, 509.01), (514.4, 517.55), (523.38, 524.51), (531.24, 533.17), (536.92, 538.83), (547.22, 549.45), (572.04, 572.95), (625.1800000000001, 627.29), (642.3000000000001, 644.71), (657.4, 658.23), (663.5, 667.87)]\n","2 2 26\n","F1 score event based: 0.125\n","Precision event based: 0.5\n","Recall event based: 0.07142857142857142\n","F1 score: 0.010203382866534274\n","Precision: 0.07275390625\n","Recall: 0.005486412843361072\n","Doing inference for audio: file_423_487.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audiofile_423_487.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([170686, 128, 2]) torch.Size([170686])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/56100 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n"," 99%|█████████▉| 55808/56100 [00:04<00:00, 13076.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["13312 40793\n","17.0251068\n","F1 score pre correzione: 0.40158950189446446\n","13312 40793\n","1702\n","min_event_frame:10\n","1702\n","min_event_frame:10\n","[(10.24, 15.35), (51.2, 56.31), (71.68, 76.79), (87.04, 92.15), (107.52, 112.63), (133.12, 138.23), (143.36, 153.59), (199.68, 204.79), (220.16, 225.27), (230.4, 235.51), (266.24, 271.35), (281.6, 286.71), (317.44, 322.55), (358.40000000000003, 363.51), (384.0, 389.11), (394.24, 399.35), (409.6, 414.71000000000004), (419.84000000000003, 424.95), (440.32, 450.55), (471.04, 476.15000000000003), (491.52, 501.75), (506.88, 511.99), (527.36, 532.47)]\n","[(8.58, 44.51), (51.54, 73.71000000000001), (79.06, 88.61), (94.5, 101.99000000000001), (107.96000000000001, 113.97), (120.42, 151.81), (163.16, 178.83), (187.1, 213.67000000000002), (220.3, 232.93), (239.14000000000001, 258.89), (265.76, 288.77), (295.86, 327.69), (334.54, 346.31), (354.7, 386.73), (394.12, 431.29), (438.72, 451.55), (457.88, 470.25), (477.34000000000003, 489.65000000000003), (496.64, 517.21), (524.26, 538.55), (545.92, 558.07)]\n","14 9 7\n","F1 score event based: 0.6363636363636365\n","Precision event based: 0.6086956521739131\n","Recall event based: 0.6666666666666666\n","F1 score: 0.40158950189446446\n","Precision: 0.8161057692307693\n","Recall: 0.2663202019954404\n","Doing inference for audio: R4_cleaned recording_13-10-17.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioR4_cleaned recording_13-10-17.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([270836, 128, 2]) torch.Size([270836])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/34372 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n","100%|█████████▉| 34304/34372 [00:02<00:00, 12935.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["4608 25889\n","27.267150899999994\n","F1 score pre correzione: 0.11981506377676493\n","4608 25889\n","2726\n","min_event_frame:10\n","2726\n","min_event_frame:10\n","[(10.24, 15.35), (56.32, 61.43), (66.56, 71.67), (76.8, 87.03), (138.24, 143.35), (194.56, 199.67000000000002), (204.8, 209.91), (317.44, 322.55)]\n","[(17.48, 56.77), (65.88, 75.87), (88.86, 128.83), (138.48, 199.61), (209.86, 260.45), (269.06, 303.69), (319.90000000000003, 343.03000000000003)]\n","3 5 4\n","F1 score event based: 0.39999999999999997\n","Precision event based: 0.375\n","Recall event based: 0.42857142857142855\n","F1 score: 0.11981506377676493\n","Precision: 0.396484375\n","Recall: 0.07057051257290742\n","Doing inference for audio: BUK5_20161101_002104a.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioBUK5_20161101_002104a.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([27676, 128, 2]) torch.Size([27676])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/105714 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n"," 99%|█████████▉| 104960/105714 [00:07<00:00, 13076.63it/s]C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","100%|█████████▉| 105472/105714 [00:08<00:00, 12996.60it/s]"]},{"name":"stdout","output_type":"stream","text":["0 1119\n","0.28889999999999816\n","F1 score pre correzione: 0.0\n","0 1119\n","28\n","min_event_frame:10\n","28\n","min_event_frame:10\n","[]\n","[(45.300000000000004, 45.75), (46.06, 46.49), (49.14, 49.45), (49.620000000000005, 50.01), (50.72, 51.19), (66.52, 66.79), (67.3, 67.47), (67.74, 68.01), (68.56, 69.07000000000001), (74.58, 74.69), (79.72, 80.11), (160.20000000000002, 160.73), (175.16, 175.77), (224.06, 224.35), (224.8, 225.07), (411.0, 411.25), (411.62, 411.81), (412.04, 412.33), (418.28000000000003, 418.37), (434.92, 435.17), (435.58, 435.77), (436.1, 436.37), (470.78000000000003, 470.95), (730.04, 730.4300000000001), (855.72, 856.09), (858.7, 859.0500000000001), (863.5, 863.83), (864.0600000000001, 864.39), (864.64, 864.95), (869.3000000000001, 869.83), (945.9200000000001, 946.33), (1019.16, 1019.45)]\n","0 0 32\n","F1 score event based: 0.0\n","Precision event based: 1e-05\n","Recall event based: 0.0\n","F1 score: 0.0\n","Precision: 0.0\n","Recall: 0.0\n","Doing inference for audio: BUK5_20180921_015906a.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioBUK5_20180921_015906a.wav.pth\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([17702, 128, 2]) torch.Size([17702])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/106540 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n"," 99%|█████████▉| 105472/106540 [00:07<00:00, 13457.36it/s]C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","100%|█████████▉| 106496/106540 [00:08<00:00, 13033.58it/s]"]},{"name":"stdout","output_type":"stream","text":["0 190\n","0.15364829999999968\n","F1 score pre correzione: 0.0\n","0 190\n","15\n","min_event_frame:10\n","15\n","min_event_frame:10\n","[]\n","[(636.72, 637.03), (640.54, 640.69), (645.44, 645.5500000000001), (661.04, 661.35), (695.12, 695.69), (750.96, 751.15)]\n","0 0 6\n","F1 score event based: 0.0\n","Precision event based: 1e-05\n","Recall event based: 0.0\n","F1 score: 0.0\n","Precision: 0.0\n","Recall: 0.0\n","Doing inference for audio: BUK1_20181013_023504.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioBUK1_20181013_023504.wav.pth\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([20150, 128, 2]) torch.Size([20150])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/132561 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n","100%|█████████▉| 132096/132561 [00:10<00:00, 9251.04it/s] C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","100%|█████████▉| 132096/132561 [00:10<00:00, 12463.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0 19\n","0.0531905999999843\n","F1 score pre correzione: 0.0\n","0 19\n","5\n","min_event_frame:10\n","5\n","min_event_frame:10\n","[]\n","[]\n","0 0 0\n","F1 score event based: 1e-05\n","Precision event based: 1e-05\n","Recall event based: 1e-05\n","F1 score: 0.0\n","Precision: 0.0\n","Recall: 0.0\n","Doing inference for audio: BUK1_20181011_001004.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioBUK1_20181011_001004.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([29098, 128, 2]) torch.Size([29098])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/106986 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n"," 99%|█████████▊| 105472/106986 [00:08<00:00, 13263.55it/s]C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","100%|█████████▉| 106496/106986 [00:08<00:00, 12364.81it/s]"]},{"name":"stdout","output_type":"stream","text":["0 488\n","0.22137119999997026\n","F1 score pre correzione: 0.0\n","0 488\n","22\n","min_event_frame:10\n","22\n","min_event_frame:10\n","[]\n","[(0.36, 0.5700000000000001), (3.04, 3.23), (6.88, 7.41), (9.48, 9.81), (10.1, 10.370000000000001), (46.2, 46.53), (49.04, 49.33), (596.14, 596.73), (606.26, 606.53), (608.38, 609.17), (633.14, 633.51), (968.88, 969.33)]\n","0 0 12\n","F1 score event based: 0.0\n","Precision event based: 1e-05\n","Recall event based: 0.0\n","F1 score: 0.0\n","Precision: 0.0\n","Recall: 0.0\n","Doing inference for audio: BUK4_20161011_000804.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioBUK4_20161011_000804.wav.pth\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([22894, 128, 2]) torch.Size([22894])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/130842 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n","100%|█████████▉| 130560/130842 [00:09<00:00, 13239.27it/s]C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","100%|█████████▉| 130560/130842 [00:09<00:00, 13114.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0 113\n","0.03521489999999687\n","F1 score pre correzione: 0.0\n","0 113\n","3\n","min_event_frame:10\n","3\n","min_event_frame:10\n","[]\n","[(859.26, 859.37), (1086.46, 1086.55)]\n","0 0 2\n","F1 score event based: 0.0\n","Precision event based: 1e-05\n","Recall event based: 0.0\n","F1 score: 0.0\n","Precision: 0.0\n","Recall: 0.0\n","Doing inference for audio: BUK4_20171022_004304a.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioBUK4_20171022_004304a.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([51682, 128, 2]) torch.Size([51682])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/80304 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n"," 99%|█████████▉| 79360/80304 [00:06<00:00, 13224.23it/s]C:\\Users\\aldob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"," 99%|█████████▉| 79872/80304 [00:06<00:00, 12951.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0 70\n","0.29211779999995763\n","F1 score pre correzione: 0.0\n","0 70\n","29\n","min_event_frame:10\n","29\n","min_event_frame:10\n","[]\n","[(0.26, 0.5700000000000001), (228.70000000000002, 229.03)]\n","0 0 2\n","F1 score event based: 0.0\n","Precision event based: 1e-05\n","Recall event based: 0.0\n","F1 score: 0.0\n","Precision: 0.0\n","Recall: 0.0\n","Doing inference for audio: R4_cleaned recording_TEL_20-10-17.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioR4_cleaned recording_TEL_20-10-17.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([153604, 128, 2]) torch.Size([153604])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/61586 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n","100%|█████████▉| 61440/61586 [00:04<00:00, 12966.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["59392 48255\n","15.308924999999999\n","F1 score pre correzione: 0.8657556643473575\n","59392 48255\n","1530\n","min_event_frame:10\n","1530\n","min_event_frame:10\n","[(0.0, 127.99000000000001), (133.12, 312.31), (317.44, 424.95), (430.08, 542.71), (547.84, 614.39)]\n","[(5.1000000000000005, 8.41), (12.68, 32.85), (41.7, 63.77), (68.78, 91.11), (96.08, 112.63), (117.42, 151.77), (158.68, 168.89000000000001), (173.98, 204.33), (209.6, 236.07), (240.82, 247.93), (252.94, 264.13), (269.24, 289.37), (295.2, 306.43), (310.84000000000003, 330.63), (334.56, 367.23), (372.68, 402.85), (409.2, 440.97), (447.78000000000003, 466.41), (471.76, 511.15000000000003), (516.3, 541.75), (546.64, 561.45), (565.22, 574.9300000000001), (579.64, 592.09), (596.72, 603.87), (609.82, 614.39)]\n","5 0 20\n","F1 score event based: 0.33333333333333337\n","Precision event based: 1.0\n","Recall event based: 0.2\n","F1 score: 0.8657556643473575\n","Precision: 0.7845837823275862\n","Recall: 0.9656615894725935\n","Doing inference for audio: R4_cleaned recording_17-10-17.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioR4_cleaned recording_17-10-17.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([110018, 128, 2]) torch.Size([110018])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/41949 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n"," 99%|█████████▉| 41472/41949 [00:03<00:00, 12853.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["40960 33980\n","10.9473285\n","F1 score pre correzione: 0.8931945556445157\n","40960 33980\n","1094\n","min_event_frame:10\n","1094\n","min_event_frame:10\n","[(0.0, 276.47), (281.6, 414.71000000000004)]\n","[(6.34, 44.67), (48.74, 61.19), (65.02, 100.09), (105.3, 139.51), (144.3, 160.77), (169.68, 173.33), (179.04, 189.45000000000002), (193.98000000000002, 233.05), (237.76, 258.53000000000003), (263.08, 326.51), (332.98, 337.15000000000003), (342.96, 356.11), (360.94, 377.67), (383.12, 414.71000000000004)]\n","2 0 12\n","F1 score event based: 0.25\n","Precision event based: 1.0\n","Recall event based: 0.14285714285714285\n","F1 score: 0.8931945556445157\n","Precision: 0.81708984375\n","Recall: 0.9849323131253679\n","Doing inference for audio: R4_cleaned recording_TEL_24-10-17.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioR4_cleaned recording_TEL_24-10-17.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([163468, 128, 2]) torch.Size([163468])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/78309 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n"," 99%|█████████▉| 77824/78309 [00:05<00:00, 13055.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["33280 62823\n","16.3056891\n","F1 score pre correzione: 0.5844354494656775\n","33280 62823\n","1630\n","min_event_frame:10\n","1630\n","min_event_frame:10\n","[(0.0, 5.11), (25.6, 35.83), (40.96, 51.19), (56.32, 66.55), (71.68, 76.79), (102.4, 112.63), (117.76, 122.87), (143.36, 148.47), (153.6, 168.95000000000002), (189.44, 199.67000000000002), (204.8, 209.91), (250.88, 255.99), (261.12, 266.23), (281.6, 286.71), (291.84000000000003, 302.07), (322.56, 327.67), (353.28000000000003, 363.51), (368.64, 383.99), (394.24, 404.47), (419.84000000000003, 424.95), (435.2, 455.67), (471.04, 476.15000000000003), (481.28000000000003, 491.51), (496.64, 501.75), (512.0, 517.11), (537.6, 542.71), (547.84, 552.95), (568.32, 573.4300000000001), (583.6800000000001, 588.79), (599.04, 604.15), (619.52, 645.11), (655.36, 660.47), (670.72, 675.83), (696.32, 747.51), (757.76, 767.99)]\n","[(4.12, 12.69), (17.0, 38.47), (47.26, 71.67), (75.8, 91.41), (95.18, 126.77), (131.16, 178.11), (182.9, 222.15), (226.68, 238.47), (242.32, 274.93), (278.8, 301.47), (305.7, 329.15000000000003), (334.48, 351.63), (355.78000000000003, 379.67), (383.56, 393.75), (399.90000000000003, 425.29), (429.7, 461.99), (467.02, 479.27), (483.94, 491.79), (495.82, 514.07), (519.5, 525.83), (530.0600000000001, 541.99), (550.22, 560.59), (565.12, 571.59), (574.64, 594.8100000000001), (600.22, 614.13), (619.7, 634.33), (642.5600000000001, 661.49), (666.0600000000001, 692.45), (697.36, 722.5500000000001), (726.24, 751.67), (756.02, 778.23)]\n","27 8 4\n","F1 score event based: 0.8181818181818182\n","Precision event based: 0.7714285714285715\n","Recall event based: 0.8709677419354839\n","F1 score: 0.5844354494656775\n","Precision: 0.8438401442307693\n","Recall: 0.4470178119478535\n","Doing inference for audio: R4_cleaned recording_TEL_25-10-17.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioR4_cleaned recording_TEL_25-10-17.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([161361, 128, 2]) torch.Size([161361])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/94367 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n","100%|█████████▉| 94208/94367 [00:07<00:00, 13041.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["93184 75271\n","16.361774099999998\n","F1 score pre correzione: 0.885280935561426\n","93184 75271\n","1636\n","min_event_frame:10\n","1636\n","min_event_frame:10\n","[(0.0, 112.63), (117.76, 127.99000000000001), (133.12, 942.07)]\n","[(4.6000000000000005, 38.69), (44.1, 72.49), (77.52, 85.23), (92.34, 114.57000000000001), (119.36, 148.41), (152.6, 165.39000000000001), (173.78, 187.45000000000002), (191.22, 198.43), (204.24, 234.25), (239.3, 251.47), (255.94, 284.27), (288.68, 301.17), (305.38, 337.17), (340.86, 385.53000000000003), (389.88, 414.83), (419.3, 427.39), (432.78000000000003, 453.61), (458.82, 472.17), (476.58, 488.33), (494.46000000000004, 502.33), (511.58, 533.65), (537.86, 550.75), (556.7, 607.37), (612.9, 640.51), (645.96, 675.39), (680.12, 692.73), (697.26, 724.85), (733.8000000000001, 740.03), (745.22, 775.63), (784.38, 821.59), (828.54, 839.63), (845.0600000000001, 864.79), (869.58, 874.51), (880.1, 924.19), (928.08, 942.07)]\n","3 0 32\n","F1 score event based: 0.15789473684210528\n","Precision event based: 1.0\n","Recall event based: 0.08571428571428572\n","F1 score: 0.885280935561426\n","Precision: 0.8001910199175825\n","Recall: 0.9906205577181119\n","Doing inference for audio: R4_cleaned recording_16-10-17.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioR4_cleaned recording_16-10-17.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([194901, 128, 2]) torch.Size([194901])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/45543 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n"," 99%|█████████▉| 45056/45543 [00:03<00:00, 12878.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["43008 31073\n","19.9852653\n","F1 score pre correzione: 0.7973704458633117\n","43008 31073\n","1998\n","min_event_frame:10\n","1998\n","min_event_frame:10\n","[(5.12, 158.71), (163.84, 322.55), (327.68, 409.59000000000003), (414.72, 450.55)]\n","[(49.32, 59.47), (69.76, 103.29), (115.38, 183.35), (197.70000000000002, 226.57), (240.1, 291.07), (308.14, 367.03000000000003), (390.36, 450.55)]\n","4 0 3\n","F1 score event based: 0.7272727272727273\n","Precision event based: 1.0\n","Recall event based: 0.5714285714285714\n","F1 score: 0.7973704458633117\n","Precision: 0.6867327008928571\n","Recall: 0.9505036526888295\n","Doing inference for audio: R4_cleaned recording_TEL_19-10-17.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioR4_cleaned recording_TEL_19-10-17.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([197212, 128, 2]) torch.Size([197212])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/57000 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n","100%|█████████▉| 56832/57000 [00:04<00:00, 12986.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["26112 46656\n","19.6003188\n","F1 score pre correzione: 0.5663478452066842\n","26112 46656\n","1960\n","min_event_frame:10\n","1960\n","min_event_frame:10\n","[(15.36, 35.83), (40.96, 46.07), (51.2, 56.31), (66.56, 71.67), (81.92, 97.27), (102.4, 112.63), (117.76, 127.99000000000001), (133.12, 138.23), (143.36, 148.47), (158.72, 163.83), (168.96, 174.07), (184.32, 189.43), (194.56, 199.67000000000002), (215.04, 225.27), (235.52, 240.63), (245.76, 250.87), (256.0, 261.11), (266.24, 271.35), (276.48, 281.59000000000003), (291.84000000000003, 302.07), (307.2, 312.31), (317.44, 327.67), (337.92, 343.03000000000003), (358.40000000000003, 363.51), (373.76, 378.87), (389.12, 399.35), (414.72, 419.83), (424.96000000000004, 435.19), (440.32, 445.43), (450.56, 455.67), (486.40000000000003, 491.51), (496.64, 506.87), (512.0, 517.11), (522.24, 537.59), (547.84, 552.95), (563.2, 568.3100000000001)]\n","[(4.3, 25.67), (31.62, 127.63000000000001), (137.44, 167.47), (173.6, 203.37), (213.5, 216.73000000000002), (227.54, 277.71), (282.2, 310.61), (315.22, 358.61), (364.94, 384.87), (391.1, 415.63), (419.66, 453.27), (458.36, 478.01), (484.14, 501.91), (507.38, 523.9300000000001), (531.42, 555.0500000000001), (560.14, 568.3100000000001)]\n","15 21 1\n","F1 score event based: 0.5769230769230769\n","Precision event based: 0.4166666666666667\n","Recall event based: 0.9375\n","F1 score: 0.5663478452066842\n","Precision: 0.7891390931372549\n","Recall: 0.44165809327846367\n","Doing inference for audio: R4_cleaned recording_TEL_23-10-17.wav\n","E:/Tesi/spectrograms_support_and_query_for_inference_AST_16KHz_pcen_pth\\inference_audioR4_cleaned recording_TEL_23-10-17.wav.pth\n","Audio già pronto per l'inferenza\n","E:/Tesi/Models_pth\\model_type_AST.pth\n","0\n","torch.Size([136332, 128, 2]) torch.Size([136332])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  support_labels = torch.tensor(support_labels).to(device)\n","  0%|          | 0/70560 [00:00<?, ?it/s]C:\\Users\\aldob\\AppData\\Local\\Temp\\ipykernel_11328\\1557730710.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  query_batch=torch.tensor(query_batch)\n"," 99%|█████████▉| 70144/70560 [00:05<00:00, 13022.56it/s]"]},{"name":"stdout","output_type":"stream","text":["68608 57683\n","13.860858300000004\n","F1 score pre correzione: 0.8898971423141792\n","68608 57683\n","1386\n","min_event_frame:10\n","1386\n","min_event_frame:10\n","[(0.0, 61.43), (66.56, 235.51), (240.64000000000001, 686.07), (691.2, 701.4300000000001)]\n","[(4.46, 21.73), (25.0, 52.31), (57.0, 68.99), (73.12, 105.31), (108.66, 124.91), (128.26, 157.01), (160.26, 192.05), (197.56, 212.45000000000002), (215.72, 227.03), (231.68, 253.35), (263.18, 276.33), (280.08, 294.81), (298.24, 333.55), (337.40000000000003, 353.95), (358.76, 389.89), (393.44, 404.15000000000003), (407.78000000000003, 421.33), (428.06, 455.75), (459.58, 478.73), (482.16, 511.65000000000003), (516.5, 535.73), (539.58, 551.53), (555.66, 576.99), (580.98, 600.79), (604.4, 617.91), (622.14, 632.73), (636.5, 665.71), (671.22, 690.73), (695.22, 701.4300000000001)]\n","4 0 25\n","F1 score event based: 0.2424242424242424\n","Precision event based: 1.0\n","Recall event based: 0.13793103448275862\n","F1 score: 0.8898971423141792\n","Precision: 0.8190444263059702\n","Recall: 0.9741691659587747\n","Mean F1 score for frame classification:0.35375823452593624\n","Mean F1 score:0.2510237394906435\n","Mean recall:0.24194561429911232\n","Mean precision:0.45128593472171485\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["audios=[\"ME1.wav\",\"file_97_113.wav\",\"file_423_487.wav\",\"R4_cleaned recording_13-10-17.wav\",\"ME2.wav\",\"BUK5_20161101_002104a.wav\",\"BUK5_20180921_015906a.wav\",\"BUK1_20181013_023504.wav\",\"BUK1_20181011_001004.wav\",\n","\"BUK4_20161011_000804.wav\",\"BUK4_20171022_004304a.wav\",\"R4_cleaned recording_TEL_20-10-17.wav\",\"R4_cleaned recording_17-10-17.wav\",\"R4_cleaned recording_TEL_24-10-17.wav\",\"R4_cleaned recording_TEL_25-10-17.wav\",\"R4_cleaned recording_16-10-17.wav\",\n","\"R4_cleaned recording_TEL_19-10-17.wav\",\"R4_cleaned recording_TEL_23-10-17.wav\"]\n","lista=[1,0]\n","lista=[0]\n","\n","for i in lista:\n","    \n","    f1_scores_frame_classification_dictionary={}\n","    f1_scores_dictionary={}\n","    precision_dictionary={}\n","    recall_dictionary={}\n","\n","\n","    #If i=0 AST if i=1 then Wav2Vec. If i=2 CNN with negative support undersampling, if i=3 CNN with full negative support, if i=4 CNN with event spectrogram creation and full negative support.\n","\n","    number_of_audio_tested=0\n","    rolling_mean=True\n","\n","    if i==0:\n","        audios=[\"ME1.wav\",\"file_97_113.wav\",\"file_423_487.wav\",\"R4_cleaned recording_13-10-17.wav\",\"BUK5_20161101_002104a.wav\",\"BUK5_20180921_015906a.wav\",\"BUK1_20181013_023504.wav\",\"BUK1_20181011_001004.wav\",\n","        \"BUK4_20161011_000804.wav\",\"BUK4_20171022_004304a.wav\",\"R4_cleaned recording_TEL_20-10-17.wav\",\"R4_cleaned recording_17-10-17.wav\",\"R4_cleaned recording_TEL_24-10-17.wav\",\"R4_cleaned recording_TEL_25-10-17.wav\",\"R4_cleaned recording_16-10-17.wav\",\n","        \"R4_cleaned recording_TEL_19-10-17.wav\",\"R4_cleaned recording_TEL_23-10-17.wav\"]\n","\n","       \n","    \n","    #audios=[\"R4_cleaned recording_16-10-17.wav\"]\n","    #audios= [\"R4_cleaned recording_13-10-17.wav\"]\n","    #note: Pcen can be used only if the encoder is not Wav2Vec.\n","    pcen=True\n","    \n","    if(i==0 or i==1):rolling_mean=False\n","\n","    if (i==1): pcen=False\n","    print(i)\n","\n","    mean_f1_score=0\n","    mean_precision=0\n","    mean_recall=0\n","    mean_f1_score_classification=0\n","\n","    for audio in audios:\n","        print(f'Doing inference for audio: {audio}')\n","\n","        file_name=f'inference_audio{audio}.pth'\n","    \n","        \n","\n","        augment_negative_support=False\n","\n","        if i==1:\n","            file_path=os.path.join(directory_path_raw_audio_inference, file_name)\n","        elif i==0:\n","            file_path=os.path.join(directory_path_spectrograms_inference_AST, file_name)\n","        else: \n","            if pcen==False:\n","                file_path=os.path.join(directory_path_spectrograms_inference, file_name)\n","            else: \n","                file_path=os.path.join(directory_path_spectrograms_inference_pcen, file_name)\n","        print(file_path)\n","\n","\n","        if os.path.isfile(file_path):\n","            support_and_query_set=torch.load(file_path)\n","            print(\"Audio già pronto per l'inferenza\")\n","        else:\n","            audio_support_events=get_support_set_inference(audio)\n","            audio_query_events=get_query_set_inference(audio)\n","            labels_audio_inference=label_audio(audio_support_events,audio_query_events,audio)\n","            if i==1:\n","                support_and_query_set=create_support_and_query_set_inference_audio_raw(labels_audio_inference,audio,audio_support_events)\n","            elif i==0:\n","                support_and_query_set=create_support_and_query_set_inference_audio_spectrogramsAST(labels_audio_inference,audio,audio_support_events,pcen=pcen,augment_negative_support=augment_negative_support)\n","            else:\n","                support_and_query_set=create_support_and_query_set_inference_audio_spectrograms(labels_audio_inference,audio,audio_support_events,pcen=pcen,augment_negative_support=augment_negative_support)\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","        # Dimension for the spectrogram in our cases.\n","        hidden_dim = 768\n","        output_dim = 22 \n","\n","\n","\n","        if i==1:\n","            mode=\"Wav2Vec\"\n","            model = CustomWav2Vec2Model(\n","                base_model_name=\"facebook/wav2vec2-large-960h-lv60-self\",\n","                num_labels_task_b=22,\n","            ).to(device)\n","        elif i==0:\n","            mode=\"AST\"\n","            model=AST_Model(output_dim).to(device)\n","        else:\n","            input_dim = (128, 2)\n","            \n","            mode=\"classical_network\"\n","            if rolling_mean== True:\n","                model=PrototypicalNetwork_with_binary_head(input_dim=input_dim,output_dim=output_dim,hidden_dim=hidden_dim).to(device)\n","            else: \n","                model=PrototypicalNetwork(input_dim=input_dim,output_dim=output_dim,hidden_dim=hidden_dim).to(device)\n","\n","        if i==0:\n","            optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","            mode=\"AST\"\n","        elif i==1:\n","            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","            mode=\"Wav2Vec\"\n","        else:\n","            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","            mode=\"classical_network\"   \n","\n","        if rolling_mean==True:\n","            rolling=\"_rolling\"\n","        else: \n","            rolling=\"\"\n","        checkpoint_file=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+ \"_complete_support_set.pth\")\n","    \n","\n","        if i==0 or i==1:\n","            model_info=0\n","            if os.path.isfile(os.path.join(checkpoint_path, \"model_type_\" + str(mode)+\".pth\")):\n","                checkpoint_file=os.path.join(checkpoint_path, \"model_type_\" + str(mode) + \".pth\")\n","                print(checkpoint_file)\n","                model_info=torch.load(checkpoint_file)\n","        elif i==2:\n","            model_info=0\n","            if os.path.isfile(os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+\"_balanced_support_set.pth\")):\n","                checkpoint_file=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+ \"_balanced_support_set.pth\")\n","                model_info=torch.load(checkpoint_file)\n","        elif i==3:\n","            model_info=0\n","            if os.path.isfile(os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+\"_complete_support_set.pth\")):\n","                checkpoint_file=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+ \"_complete_support_set.pth\")\n","                model_info=torch.load(checkpoint_file)\n","        elif i==4:\n","            model_info=0\n","            if os.path.isfile(os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+ \"_event_spectrogram.pth\")):\n","                checkpoint_file=os.path.join(checkpoint_path, \"model_type_\" + str(mode) +str(rolling)+ \"_event_spectrogram.pth\")\n","                model_info=torch.load(checkpoint_file)\n","        else: \n","            model_info=0\n","\n","\n","        metrics,f1_score_classification=inference_audio(model=model,audio_name=audio,model_checkpoint=model_info,mode=mode,pcen=pcen)\n","\n","        \n","        f1_scores_frame_classification_dictionary[audio]=f1_score_classification\n","        f1_scores_dictionary[audio]=metrics['f1_score']\n","        precision_dictionary[audio]=metrics['precision']\n","        recall_dictionary[audio]=metrics['recall']\n","\n","\n","        \n","        if(mean_f1_score==0): mean_f1_score=metrics['f1_score']\n","        else: mean_f1_score+=metrics['f1_score']\n","\n","\n","        if(mean_precision==0): mean_precision=metrics['precision']\n","        else: mean_precision+=metrics['precision']\n","\n","        if(mean_recall==0): mean_recall=metrics['recall']\n","        else: mean_recall+=metrics['recall']\n","\n","        if(mean_f1_score_classification==0):mean_f1_score_classification=f1_score_classification\n","        else: mean_f1_score_classification+=f1_score_classification\n","        number_of_audio_tested+=1\n","\n","    mean_f1_score=mean_f1_score/number_of_audio_tested\n","    mean_recall=mean_recall/number_of_audio_tested\n","    mean_precision=mean_precision/number_of_audio_tested\n","    mean_f1_score_classification=mean_f1_score_classification/number_of_audio_tested\n","    \n","\n","\n","    print(f'Mean F1 score for frame classification:{mean_f1_score_classification}')\n","    print(f'Mean F1 score:{mean_f1_score}')\n","    print(f'Mean recall:{mean_recall}')\n","    print(f'Mean precision:{mean_precision}')\n","\n","    if not os.path.exists(directory_path_inference_stats):\n","            os.makedirs(directory_path_inference_stats)\n","    torch.save({'f1_scores_frame_classification':f1_scores_frame_classification_dictionary,\n","                'mean_f1_score_classification':mean_f1_score_classification,\n","                'f1_scores':f1_scores_dictionary,\n","                'mean_f1_score':mean_f1_score,\n","                'precisions':precision_dictionary,\n","                'mean_precision':mean_precision,\n","                'recalls':recall_dictionary,\n","                'mean_recall':mean_recall\n","    },os.path.join(directory_path_inference_stats, \"inference_stats_\" + str(mode) +str(i)+ \".pth\"))\n","\n"]},{"cell_type":"code","execution_count":121,"id":"59080de0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'f1_scores_frame_classification': {'ME1.wav': 0.010654617465762977, 'file_97_113.wav': 0.4763845156510465, 'file_423_487.wav': 0.8496466737971825, 'R4_cleaned recording_13-10-17.wav': 0.4195501083626037, 'ME2.wav': 0.07405558329172908, 'BUK5_20161101_002104a.wav': 0.024970553592461718, 'BUK5_20180921_015906a.wav': 0.0030257186081694403, 'BUK1_20181013_023504.wav': 0.001815757369421511, 'BUK1_20181011_001004.wav': 0.011139900939488481, 'BUK4_20161011_000804.wav': 0.0012488477892420684, 'BUK4_20171022_004304a.wav': 0.007628196072891651, 'R4_cleaned recording_TEL_20-10-17.wav': 0.4021360712810527, 'R4_cleaned recording_17-10-17.wav': 0.9330605581996841, 'R4_cleaned recording_TEL_24-10-17.wav': 0.48888824584756063, 'R4_cleaned recording_TEL_25-10-17.wav': 0.5311763874493546, 'R4_cleaned recording_16-10-17.wav': 0.8152404655588903, 'R4_cleaned recording_TEL_19-10-17.wav': 0.7905480265548691, 'R4_cleaned recording_TEL_23-10-17.wav': 0.5355309291259704}, 'mean_f1_score_classification': 0.3542611753865212, 'f1_scores': {'ME1.wav': 1e-05, 'file_97_113.wav': 0.328042328042328, 'file_423_487.wav': 0.5673758865248226, 'R4_cleaned recording_13-10-17.wav': 0.358974358974359, 'ME2.wav': 0.0, 'BUK5_20161101_002104a.wav': 0.058355437665782495, 'BUK5_20180921_015906a.wav': 0.014204545454545454, 'BUK1_20181013_023504.wav': 0.002035364457448162, 'BUK1_20181011_001004.wav': 0.03617571059431524, 'BUK4_20161011_000804.wav': 0.0036924699986812604, 'BUK4_20171022_004304a.wav': 0.022813688212927757, 'R4_cleaned recording_TEL_20-10-17.wav': 0.3333333333333333, 'R4_cleaned recording_17-10-17.wav': 0.30973451327433627, 'R4_cleaned recording_TEL_24-10-17.wav': 0.5573770491803278, 'R4_cleaned recording_TEL_25-10-17.wav': 0.2926829268292683, 'R4_cleaned recording_16-10-17.wav': 0.4912280701754386, 'R4_cleaned recording_TEL_19-10-17.wav': 0.10714285714285715, 'R4_cleaned recording_TEL_23-10-17.wav': 0.3559322033898305}, 'mean_f1_score': 0.213283930180589, 'precisions': {'ME1.wav': 0.0, 'file_97_113.wav': 0.36470588235294116, 'file_423_487.wav': 0.47619047619047616, 'R4_cleaned recording_13-10-17.wav': 0.35, 'ME2.wav': 0.0, 'BUK5_20161101_002104a.wav': 0.031161473087818695, 'BUK5_20180921_015906a.wav': 0.00715307582260372, 'BUK1_20181013_023504.wav': 0.0010187189609066599, 'BUK1_20181011_001004.wav': 0.018867924528301886, 'BUK4_20161011_000804.wav': 0.0018496498876998282, 'BUK4_20171022_004304a.wav': 0.011583011583011582, 'R4_cleaned recording_TEL_20-10-17.wav': 0.47058823529411764, 'R4_cleaned recording_17-10-17.wav': 0.18421052631578946, 'R4_cleaned recording_TEL_24-10-17.wav': 0.4689655172413793, 'R4_cleaned recording_TEL_25-10-17.wav': 0.75, 'R4_cleaned recording_16-10-17.wav': 0.5185185185185185, 'R4_cleaned recording_TEL_19-10-17.wav': 1.0, 'R4_cleaned recording_TEL_23-10-17.wav': 0.6176470588235294}, 'mean_precision': 0.29291444825594964, 'recalls': {'ME1.wav': 0.0, 'file_97_113.wav': 0.2980769230769231, 'file_423_487.wav': 0.7017543859649122, 'R4_cleaned recording_13-10-17.wav': 0.3684210526315789, 'ME2.wav': 1e-05, 'BUK5_20161101_002104a.wav': 0.4583333333333333, 'BUK5_20180921_015906a.wav': 1.0, 'BUK1_20181013_023504.wav': 1.0, 'BUK1_20181011_001004.wav': 0.4375, 'BUK4_20161011_000804.wav': 1.0, 'BUK4_20171022_004304a.wav': 0.75, 'R4_cleaned recording_TEL_20-10-17.wav': 0.25806451612903225, 'R4_cleaned recording_17-10-17.wav': 0.9722222222222222, 'R4_cleaned recording_TEL_24-10-17.wav': 0.6868686868686869, 'R4_cleaned recording_TEL_25-10-17.wav': 0.18181818181818182, 'R4_cleaned recording_16-10-17.wav': 0.4666666666666667, 'R4_cleaned recording_TEL_19-10-17.wav': 0.05660377358490566, 'R4_cleaned recording_TEL_23-10-17.wav': 0.25}, 'mean_recall': 0.49368554123869124}\n"]}],"source":["stats=torch.load(os.path.join(directory_path_inference_stats, \"inference_stats_Wav2Vec1.pth\"))\n","print(stats)"]},{"cell_type":"code","execution_count":119,"id":"2dd29042","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'f1_scores_frame_classification': {'ME1.wav': 0.009254431699687175, 'file_97_113.wav': 0.4763275382110405, 'file_423_487.wav': 0.6456009307509557, 'R4_cleaned recording_13-10-17.wav': 0.6684264278419357, 'ME2.wav': 0.04065654183567588, 'BUK5_20161101_002104a.wav': 0.01728683246137397, 'BUK5_20180921_015906a.wav': 0.0025381464109719173, 'BUK1_20181013_023504.wav': 0.0014033075808788024, 'BUK1_20181011_001004.wav': 0.011122844068989793, 'BUK4_20161011_000804.wav': 0.0014972008853013931, 'BUK4_20171022_004304a.wav': 0.0013636594921894877, 'R4_cleaned recording_TEL_20-10-17.wav': 0.6803912782488861, 'R4_cleaned recording_17-10-17.wav': 0.6892615575173955, 'R4_cleaned recording_TEL_24-10-17.wav': 0.7008663486810085, 'R4_cleaned recording_TEL_25-10-17.wav': 0.7048649578900947, 'R4_cleaned recording_16-10-17.wav': 0.64754523239454, 'R4_cleaned recording_TEL_19-10-17.wav': 0.6733742870483114, 'R4_cleaned recording_TEL_23-10-17.wav': 0.7109011462944047}, 'mean_f1_score_classification': 0.3712601482952023, 'f1_scores': {'ME1.wav': 0.16, 'file_97_113.wav': 0.27272727272727276, 'file_423_487.wav': 0.4117647058823529, 'R4_cleaned recording_13-10-17.wav': 0.13333333333333333, 'ME2.wav': 0.1142857142857143, 'BUK5_20161101_002104a.wav': 1e-05, 'BUK5_20180921_015906a.wav': 1e-05, 'BUK1_20181013_023504.wav': 0.002862400327131466, 'BUK1_20181011_001004.wav': 1e-05, 'BUK4_20161011_000804.wav': 1e-05, 'BUK4_20171022_004304a.wav': 0.07407407407407407, 'R4_cleaned recording_TEL_20-10-17.wav': 0.41379310344827586, 'R4_cleaned recording_17-10-17.wav': 0.3529411764705882, 'R4_cleaned recording_TEL_24-10-17.wav': 0.5806451612903225, 'R4_cleaned recording_TEL_25-10-17.wav': 0.4946236559139785, 'R4_cleaned recording_16-10-17.wav': 0.08, 'R4_cleaned recording_TEL_19-10-17.wav': 0.19512195121951217, 'R4_cleaned recording_TEL_23-10-17.wav': 0.5294117647058824}, 'mean_f1_score': 0.21197912853769105, 'precisions': {'ME1.wav': 0.10526315789473684, 'file_97_113.wav': 0.21739130434782608, 'file_423_487.wav': 0.358974358974359, 'R4_cleaned recording_13-10-17.wav': 0.2, 'ME2.wav': 0.2857142857142857, 'BUK5_20161101_002104a.wav': 0.0, 'BUK5_20180921_015906a.wav': 0.0, 'BUK1_20181013_023504.wav': 0.0014332514332514332, 'BUK1_20181011_001004.wav': 0.0, 'BUK4_20161011_000804.wav': 0.0, 'BUK4_20171022_004304a.wav': 0.04, 'R4_cleaned recording_TEL_20-10-17.wav': 0.48, 'R4_cleaned recording_17-10-17.wav': 0.2903225806451613, 'R4_cleaned recording_TEL_24-10-17.wav': 0.5510204081632653, 'R4_cleaned recording_TEL_25-10-17.wav': 0.5, 'R4_cleaned recording_16-10-17.wav': 0.07692307692307693, 'R4_cleaned recording_TEL_19-10-17.wav': 0.25, 'R4_cleaned recording_TEL_23-10-17.wav': 0.42857142857142855}, 'mean_precision': 0.21031188070374396, 'recalls': {'ME1.wav': 0.3333333333333333, 'file_97_113.wav': 0.36585365853658536, 'file_423_487.wav': 0.4827586206896552, 'R4_cleaned recording_13-10-17.wav': 0.1, 'ME2.wav': 0.07142857142857142, 'BUK5_20161101_002104a.wav': 0.0, 'BUK5_20180921_015906a.wav': 0.0, 'BUK1_20181013_023504.wav': 1.0, 'BUK1_20181011_001004.wav': 0.0, 'BUK4_20161011_000804.wav': 0.0, 'BUK4_20171022_004304a.wav': 0.5, 'R4_cleaned recording_TEL_20-10-17.wav': 0.36363636363636365, 'R4_cleaned recording_17-10-17.wav': 0.45, 'R4_cleaned recording_TEL_24-10-17.wav': 0.6136363636363636, 'R4_cleaned recording_TEL_25-10-17.wav': 0.48936170212765956, 'R4_cleaned recording_16-10-17.wav': 0.08333333333333333, 'R4_cleaned recording_TEL_19-10-17.wav': 0.16, 'R4_cleaned recording_TEL_23-10-17.wav': 0.6923076923076923}, 'mean_recall': 0.3169805355016421}\n"]}],"source":["stats=torch.load(os.path.join(directory_path_inference_stats, \"inference_stats_classical_network3.pth\"))\n","#print(stats['f1_scores'])\n","print (stats)"]},{"cell_type":"code","execution_count":120,"id":"4afabf70","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'f1_scores_frame_classification': {'ME1.wav': 0.010837266212200665, 'file_97_113.wav': 0.47370578349797104, 'file_423_487.wav': 0.656851399842463, 'R4_cleaned recording_13-10-17.wav': 0.6847767200681062, 'ME2.wav': 0.03879939738092479, 'BUK5_20161101_002104a.wav': 0.016633299715502838, 'BUK5_20180921_015906a.wav': 0.0024706518719939184, 'BUK1_20181013_023504.wav': 0.0014030852789197752, 'BUK1_20181011_001004.wav': 0.011075902508366157, 'BUK4_20161011_000804.wav': 0.0016787708477792782, 'BUK4_20171022_004304a.wav': 0.0012442199483741574, 'R4_cleaned recording_TEL_20-10-17.wav': 0.6968995528121397, 'R4_cleaned recording_17-10-17.wav': 0.7122943463954532, 'R4_cleaned recording_TEL_24-10-17.wav': 0.7064389910592846, 'R4_cleaned recording_TEL_25-10-17.wav': 0.7131405302263109, 'R4_cleaned recording_16-10-17.wav': 0.6572886297376094, 'R4_cleaned recording_TEL_19-10-17.wav': 0.6601986687527194, 'R4_cleaned recording_TEL_23-10-17.wav': 0.7419991716139721}, 'mean_f1_score_classification': 0.37709646598722724, 'f1_scores': {'ME1.wav': 0.05, 'file_97_113.wav': 0.32352941176470584, 'file_423_487.wav': 0.49122807017543857, 'R4_cleaned recording_13-10-17.wav': 0.13333333333333333, 'ME2.wav': 0.13043478260869565, 'BUK5_20161101_002104a.wav': 1e-05, 'BUK5_20180921_015906a.wav': 0.2857142857142857, 'BUK1_20181013_023504.wav': 0.003130240357741755, 'BUK1_20181011_001004.wav': 1e-05, 'BUK4_20161011_000804.wav': 1e-05, 'BUK4_20171022_004304a.wav': 1e-05, 'R4_cleaned recording_TEL_20-10-17.wav': 0.4912280701754386, 'R4_cleaned recording_17-10-17.wav': 0.3673469387755102, 'R4_cleaned recording_TEL_24-10-17.wav': 0.5833333333333334, 'R4_cleaned recording_TEL_25-10-17.wav': 0.577319587628866, 'R4_cleaned recording_16-10-17.wav': 0.30769230769230765, 'R4_cleaned recording_TEL_19-10-17.wav': 0.44, 'R4_cleaned recording_TEL_23-10-17.wav': 0.4511278195488722}, 'mean_f1_score': 0.25752545450602937, 'precisions': {'ME1.wav': 0.029411764705882353, 'file_97_113.wav': 0.23157894736842105, 'file_423_487.wav': 0.5, 'R4_cleaned recording_13-10-17.wav': 0.2, 'ME2.wav': 0.16666666666666666, 'BUK5_20161101_002104a.wav': 0.0, 'BUK5_20180921_015906a.wav': 1.0, 'BUK1_20181013_023504.wav': 0.0015675736199753667, 'BUK1_20181011_001004.wav': 0.0, 'BUK4_20161011_000804.wav': 0.0, 'BUK4_20171022_004304a.wav': 0.0, 'R4_cleaned recording_TEL_20-10-17.wav': 0.5833333333333334, 'R4_cleaned recording_17-10-17.wav': 0.3103448275862069, 'R4_cleaned recording_TEL_24-10-17.wav': 0.5384615384615384, 'R4_cleaned recording_TEL_25-10-17.wav': 0.56, 'R4_cleaned recording_16-10-17.wav': 0.2857142857142857, 'R4_cleaned recording_TEL_19-10-17.wav': 0.44, 'R4_cleaned recording_TEL_23-10-17.wav': 0.3191489361702128}, 'mean_precision': 0.28701265964591793, 'recalls': {'ME1.wav': 0.16666666666666666, 'file_97_113.wav': 0.5365853658536586, 'file_423_487.wav': 0.4827586206896552, 'R4_cleaned recording_13-10-17.wav': 0.1, 'ME2.wav': 0.10714285714285714, 'BUK5_20161101_002104a.wav': 0.0, 'BUK5_20180921_015906a.wav': 0.16666666666666666, 'BUK1_20181013_023504.wav': 1.0, 'BUK1_20181011_001004.wav': 0.0, 'BUK4_20161011_000804.wav': 0.0, 'BUK4_20171022_004304a.wav': 0.0, 'R4_cleaned recording_TEL_20-10-17.wav': 0.42424242424242425, 'R4_cleaned recording_17-10-17.wav': 0.45, 'R4_cleaned recording_TEL_24-10-17.wav': 0.6363636363636364, 'R4_cleaned recording_TEL_25-10-17.wav': 0.5957446808510638, 'R4_cleaned recording_16-10-17.wav': 0.3333333333333333, 'R4_cleaned recording_TEL_19-10-17.wav': 0.44, 'R4_cleaned recording_TEL_23-10-17.wav': 0.7692307692307693}, 'mean_recall': 0.34492972339115174}\n"]}],"source":["stats=torch.load(os.path.join(directory_path_inference_stats, \"inference_stats_classical_network4.pth\"))\n","#print(stats['f1_scores'])\n","print (stats)"]},{"cell_type":"code","execution_count":117,"id":"efefc253","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'f1_scores_frame_classification': {'file_423_487.wav': 0.002643042435514659}, 'mean_f1_score_classification': 0.002643042435514659, 'f1_scores': {'file_423_487.wav': 0.0}, 'mean_f1_score': 0.0, 'precisions': {'file_423_487.wav': 1e-05}, 'mean_precision': 1e-05, 'recalls': {'file_423_487.wav': 0.0}, 'mean_recall': 0.0}\n"]}],"source":["stats=torch.load(os.path.join(directory_path_inference_stats, \"inference_stats_AST0.pth\"))\n","#print(stats['f1_scores'])\n","print (stats)"]},{"cell_type":"code","execution_count":null,"id":"e8f9848d","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"d4d3dfc2","metadata":{},"source":[" 'recalls': {'ME1.wav': 0.16666666666666666, 'file_97_113.wav': 0.5365853658536586, 'file_423_487.wav': 0.4827586206896552, 'R4_cleaned recording_13-10-17.wav': 0.1, 'ME2.wav': 0.10714285714285714, 'BUK5_20161101_002104a.wav': 0.0, 'BUK5_20180921_015906a.wav': 0.16666666666666666, 'BUK1_20181013_023504.wav': 1.0, 'BUK1_20181011_001004.wav': 0.0, 'BUK4_20161011_000804.wav': 0.0, 'BUK4_20171022_004304a.wav': 0.0, 'R4_cleaned recording_TEL_20-10-17.wav': 0.42424242424242425, 'R4_cleaned recording_17-10-17.wav': 0.45, 'R4_cleaned recording_TEL_24-10-17.wav': 0.6363636363636364, 'R4_cleaned recording_TEL_25-10-17.wav': 0.5957446808510638, 'R4_cleaned recording_16-10-17.wav': 0.3333333333333333, 'R4_cleaned recording_TEL_19-10-17.wav': 0.44, 'R4_cleaned recording_TEL_23-10-17.wav': 0.7692307692307693}, 'mean_recall': 0.34492972339115174}"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["c3c5dcf1-fd7f-498b-a627-f95b3d1f7a28","u_GsoAfvObd7","c09451c2","f78f2f77","803732a0","86d793b6","0d62e1e0","98ec77fe","d5fe007c","cf8a0446"],"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":5}
